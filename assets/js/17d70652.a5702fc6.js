"use strict";(self.webpackChunkcentrifugal_dev=self.webpackChunkcentrifugal_dev||[]).push([[2871],{28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var s=t(96540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}},52739:e=>{e.exports=JSON.parse('{"permalink":"/blog/2025/06/17/streaming-ai-gpt-responses-with-centrifugo","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2025-06-17-streaming-ai-gpt-responses-with-centrifugo.md","source":"@site/blog/2025-06-17-streaming-ai-gpt-responses-with-centrifugo.md","title":"Streaming AI responses with Centrifugo","description":"Centrifugo is an efficient and scalable transport for streaming AI responses. In this article, we will stream ChatGPT responses in real-time using Centrifugo temporary channels and Python. Simple and effective!","date":"2025-06-17T00:00:00.000Z","tags":[{"inline":true,"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"inline":true,"label":"ai","permalink":"/blog/tags/ai"},{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":7.845,"hasTruncateMarker":true,"authors":[{"name":"Alexander Emelin","title":"Founder of Centrifugal Labs","imageURL":"/img/alexander_emelin.jpeg","key":null,"page":null}],"frontMatter":{"title":"Streaming AI responses with Centrifugo","tags":["centrifugo","ai","python","tutorial"],"description":"Centrifugo is an efficient and scalable transport for streaming AI responses. In this article, we will stream ChatGPT responses in real-time using Centrifugo temporary channels and Python. Simple and effective!","author":"Alexander Emelin","authorTitle":"Founder of Centrifugal Labs","authorImageURL":"/img/alexander_emelin.jpeg","image":"/img/gpt_cover.jpg","hide_table_of_contents":false},"unlisted":false,"nextItem":{"title":"Building a real-time WebSocket leaderboard with Centrifugo and Redis","permalink":"/blog/2025/04/28/websocket-real-time-leaderboard"}}')},58728:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var s=t(52739),i=t(74848),r=t(28453);const o={title:"Streaming AI responses with Centrifugo",tags:["centrifugo","ai","python","tutorial"],description:"Centrifugo is an efficient and scalable transport for streaming AI responses. In this article, we will stream ChatGPT responses in real-time using Centrifugo temporary channels and Python. Simple and effective!",author:"Alexander Emelin",authorTitle:"Founder of Centrifugal Labs",authorImageURL:"/img/alexander_emelin.jpeg",image:"/img/gpt_cover.jpg",hide_table_of_contents:!1},a=void 0,l={authorsImageUrls:[void 0]},c=[{value:"\ud83e\uddf0 Tech Stack",id:"-tech-stack",level:2},{value:"Backend",id:"backend",level:2},{value:"Frontend",id:"frontend",level:2},{value:"Centrifugo",id:"centrifugo",level:2},{value:"Nginx",id:"nginx",level:2},{value:"Combining everything with Docker Compose",id:"combining-everything-with-docker-compose",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"Centrifugo may be used as an efficient and scalable transport for streaming AI responses. In this article, we will stream ChatGPT responses in real-time using Centrifugo temporary channels and Python. We will use OpenAI API to get the answers to user's prompts and stream them to the user using Centrifugo. The user will be able to see the response as it is being generated, similar to how ChatGPT works."}),"\n",(0,i.jsx)(n.p,{children:"Here is a video of the final result:"}),"\n",(0,i.jsx)("video",{width:"100%",loop:!0,autoPlay:"autoplay",muted:!0,controls:"",src:"/img/gpt.mp4"}),"\n",(0,i.jsxs)(n.p,{children:["And the source code is available on ",(0,i.jsx)(n.a,{href:"https://github.com/centrifugal/examples/tree/master/v6/gpt-stream",children:"GitHub"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"-tech-stack",children:"\ud83e\uddf0 Tech Stack"}),"\n",(0,i.jsx)(n.p,{children:"In this example, we will use the following technologies:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://fastapi.tiangolo.com/",children:"FastAPI"})," \u2013 async backend in Python which is good for streaming."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Centrifugo"})," \u2013 will be used as transport for streaming responses to web clients."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://openai.com/api/",children:"OpenAI API"})," \u2013 LLM responses (via GPT-3.5 Turbo is used in the example)."]}),"\n",(0,i.jsxs)(n.li,{children:["Some ",(0,i.jsx)(n.a,{href:"https://tailwindcss.com/",children:"Tailwind CSS"})," for styling."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://nginx.org/",children:"Nginx"})," as a reverse proxy to serve the frontend and route API requests to the backend."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://docs.docker.com/compose/",children:"Docker Compose"})," to run everything with a single command."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"backend",children:"Backend"}),"\n",(0,i.jsxs)(n.p,{children:["We will build the backend using ",(0,i.jsx)(n.a,{href:"https://fastapi.tiangolo.com/",children:"FastAPI"})," - which is a modern web framework for building APIs with Python. It is easy to use, and has great support for asynchronous programming, which is perfect for streaming responses."]}),"\n",(0,i.jsx)(n.p,{children:"The entire backend app is about 70 lines of code only:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nimport httpx\nimport os\n\napp = FastAPI()\n\nclient = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))\n\nCENTRIFUGO_HTTP_API_URL = "http://centrifugo:8000/api"\nCENTRIFUGO_HTTP_API_KEY = "secret"\n\nclass Command(BaseModel):\n    text: str\n    channel: str\n\n\n@app.post("/api/execute")\nasync def api_execute(cmd: Command):\n    await handle_command(cmd)\n    return {}\n\n\nclass StreamMessage(BaseModel):\n    text: str\n    done: bool\n\n\nasync def handle_command(cmd: Command):\n    text = cmd.text\n    channel = cmd.channel\n\n    try:\n        response = client.chat.completions.create(\n            model="gpt-3.5-turbo",\n            messages=[{"role": "user", "content": text}],\n            stream=True,\n        )\n        for chunk in response:\n            token = chunk.choices[0].delta.content or ""\n            if token:\n                await publish_message(\n                    channel,\n                    StreamMessage(text=token, done=False).model_dump()\n                )\n        await publish_message(\n            channel,\n            StreamMessage(text=token, done=True).model_dump()\n        )\n    except Exception as e:\n        await publish_message(\n            channel,\n            StreamMessage(text=f"\u26a0\ufe0f Error: {e}", done=True).model_dump()\n        )\n\n\nasync def publish_message(channel, stream_message):\n    payload = {\n        "channel": channel,\n        "data": stream_message\n    }\n\n    headers = {\n        "X-API-Key": f"{CENTRIFUGO_HTTP_API_KEY}",\n        "Content-Type": "application/json"\n    }\n\n    async with httpx.AsyncClient() as http_client:\n        await http_client.post(\n            f"{CENTRIFUGO_HTTP_API_URL}/publish", json=payload, headers=headers\n        )\n'})}),"\n",(0,i.jsx)(n.p,{children:"Let's go through the code step by step:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"FastAPI Setup"}),": We create a FastAPI application instance."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"OpenAI Client"}),": We initialize the OpenAI client with the API key from environment variables."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Command Model"}),": We define a Pydantic model ",(0,i.jsx)(n.code,{children:"Command"})," to validate incoming requests with ",(0,i.jsx)(n.code,{children:"text"})," and ",(0,i.jsx)(n.code,{children:"channel"})," fields."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"API Endpoint"}),": We create an endpoint ",(0,i.jsx)(n.code,{children:"/api/execute"})," that accepts POST requests with a ",(0,i.jsx)(n.code,{children:"Command"})," payload."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Command Handler"}),": The ",(0,i.jsx)(n.code,{children:"handle_command"})," function processes the command, sending the user's text to OpenAI's chat completion API and streaming the response."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Stream Message Model"}),": We define a ",(0,i.jsx)(n.code,{children:"StreamMessage"})," model to structure the messages sent to Centrifugo."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Publish Message"}),": The ",(0,i.jsx)(n.code,{children:"publish_message"})," function sends the streamed messages to the specified Centrifugo channel using its HTTP API."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error Handling"}),": If an error occurs during the OpenAI API call, we send an error message to the Centrifugo channel."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Asynchronous Execution"}),": The use of ",(0,i.jsx)(n.code,{children:"async"})," and ",(0,i.jsx)(n.code,{children:"await"})," allows the application to handle multiple requests concurrently, making it efficient for streaming responses."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"frontend",children:"Frontend"}),"\n",(0,i.jsxs)(n.p,{children:["The frontend in this example is a single ",(0,i.jsx)(n.code,{children:"index.html"})," file which draws a chat interface, handles user prompts and connects to Centrifugo to receive answer tokens in real-time."]}),"\n",(0,i.jsxs)(n.p,{children:["Here is the code for the frontend (",(0,i.jsx)(n.code,{children:"frontend/index.html"}),"):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-html",children:'<!DOCTYPE html>\n<html lang="en">\n<head>\n  <meta charset="UTF-8" />\n  <title>Chat with GPT Streaming</title>\n  <script src="https://unpkg.com/centrifuge@5.3.5/dist/centrifuge.js"><\/script>\n  <script src="https://cdn.tailwindcss.com"><\/script>\n  <meta name="viewport" content="width=device-width, initial-scale=1.0" />\n</head>\n<body class="bg-black text-gray-200 min-h-screen flex flex-col items-center justify-start py-6 px-4 text-base">\n  <div class="w-full max-w-2xl bg-black shadow-lg rounded-xl overflow-hidden border border-gray-700">\n    <div class="bg-gradient-to-r from-blue-700 to-indigo-700 text-white px-6 py-4 text-2xl font-bold">\n      \ud83e\udde0 Chat with GPT Streaming\n    </div>\n    <div id="chat" class="h-96 overflow-y-auto p-4 space-y-3 bg-black text-base"></div>\n    <div class="border-t border-gray-700 px-4 py-3 bg-black flex gap-3">\n      <input id="input" type="text" placeholder="Type your question..."\n        class="flex-1 border border-gray-600 bg-gray-900 text-white rounded-lg px-4 py-2 focus:outline-none focus:ring-2 focus:ring-blue-500"\n        onkeydown="if(event.key === \'Enter\') handleSend()" />\n      <button onclick="handleSend()"\n        class="bg-blue-600 hover:bg-blue-700 text-white font-medium px-4 py-2 rounded-lg transition text-base">Send</button>\n    </div>\n  </div>\n\n  <script>\n    const USER = "User_" + Math.floor(Math.random() * 1000);\n    const BACKEND_URL = "/api/execute";\n    const CENTRIFUGO_WS = "ws://" + location.host + "/connection/websocket";\n    const centrifuge = new Centrifuge(CENTRIFUGO_WS);\n    centrifuge.connect();\n\n    const chat = document.getElementById("chat");\n    const input = document.getElementById("input");\n\n    function appendMessage(text, id = null, type = "user") {\n      let el = id ? document.getElementById(id) : null;\n      if (!el) {\n        el = document.createElement("div");\n        el.className = `msg px-3 py-2 rounded-lg max-w-full break-words ${\n          type === "user" ? "bg-blue-500 text-white self-end ml-auto" : "bg-gray-700 text-gray-100"\n        }`;\n        el.id = id || "";\n        chat.appendChild(el);\n      }\n\n      el.innerHTML = text.replace(/\\n/g, \'<br>\');\n      chat.scrollTop = chat.scrollHeight;\n    }\n\n    async function handleStreamSubscription(channel, replyId) {\n      const sub = centrifuge.newSubscription(channel);\n      let reply = "";\n\n      sub.on("publication", ctx => {\n        const msg = ctx.data;\n        if (msg.text) {\n          const token = msg.text || "";\n          reply += token;\n          appendMessage(`GPTBot: ${reply}`, replyId, "bot");\n        }\n        if (msg.done) {\n          sub.unsubscribe();\n        }\n      });\n\n      sub.subscribe();\n      await sub.ready();\n    }\n\n    async function handleSend() {\n      const text = input.value.trim();\n      if (!text) return;\n      input.value = "";\n      const msgId = crypto.randomUUID();\n      const channel = `stream_${msgId}`;\n\n      appendMessage(`${USER}: ${text}`, null, "user");\n\n      const cmd = {\n        text: text,\n        channel: channel,\n      };\n\n      await handleStreamSubscription(channel, msgId);\n\n      await fetch(BACKEND_URL, {\n        method: "POST",\n        headers: { "Content-Type": "application/json" },\n        body: JSON.stringify(cmd)\n      });\n    }\n  <\/script>\n</body>\n</html>\n'})}),"\n",(0,i.jsx)(n.p,{children:"The key parts of the code are:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Centrifugo Connection"}),": The frontend connects to Centrifugo WebSocket endpoint using the ",(0,i.jsx)(n.a,{href:"https://github.com/centrifugal/centrifuge-js",children:"centrifuge-js"})," library."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Chat Interface"}),": The chat interface is built using Tailwind CSS for styling. It consists of a chat area and an input field for user prompts."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Message Handling"}),": The ",(0,i.jsx)(n.code,{children:"appendMessage"})," function appends messages to the chat area, distinguishing between user and bot messages."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Stream Subscription"}),": The ",(0,i.jsx)(n.code,{children:"handleStreamSubscription"})," function subscribes to a temporary channel for the user's prompt. It listens for incoming messages from Centrifugo and appends them to the chat interface in real-time."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sending User Prompts"}),": The ",(0,i.jsx)(n.code,{children:"handleSend"})," function sends the user's prompt to the backend API and initiates the stream subscription for the response."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"UUID Generation"}),": Each user prompt is assigned a unique ID using ",(0,i.jsx)(n.code,{children:"crypto.randomUUID()"}),", which is used to create a temporary channel for streaming the response."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time Updates"}),": The frontend updates the chat interface in real-time as tokens are received from the backend via Centrifugo. Once done signal is received, the subscription is unsubscribed."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"centrifugo",children:"Centrifugo"}),"\n",(0,i.jsx)(n.p,{children:"As we can see frontend connects to Centrifugo WebSocket endpoint and subscribes to a temporary channel for each user prompt. The backend publishes the response tokens to this channel, and the frontend appends them to the chat interface in real-time."}),"\n",(0,i.jsxs)(n.p,{children:["Here we run Centrifugo with a simple configuration. The ",(0,i.jsx)(n.code,{children:"config.json"})," file for Centrifugo will look like this:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "http_api": {\n    "key": "secret"\n  },\n  "client": {\n    "allowed_origins": ["*"],\n    "insecure": true\n  },\n  "log": {\n    "level": "debug"\n  }\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:'Note, we enabled insecure mode for the client, which allows us to not think about authentication in this example. In a real application, you should use secure connections and proper authentication mechanisms. We are also using a simple HTTP API key "secret" for the backend to publish messages to Centrifugo \u2013 you of course should use a more secure key in your app.'}),"\n",(0,i.jsx)(n.h2,{id:"nginx",children:"Nginx"}),"\n",(0,i.jsxs)(n.p,{children:["We will use Nginx as a reverse proxy to serve the frontend and route API requests to the backend. Nginx will also handle static files and provide a simple configuration for serving the application. Here is a Nginx server configuration we used (",(0,i.jsx)(n.code,{children:"nginx/default.conf"}),"):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-nginx",children:'server {\n  listen 80;\n\n  location / {\n    root /usr/share/nginx/html;\n    index index.html;\n    try_files $uri $uri/ =404;\n  }\n\n  location /api {\n    proxy_pass http://backend:5000;\n    proxy_set_header Host $host;\n    proxy_set_header X-Real-IP $remote_addr;\n  }\n\n  location /connection {\n    proxy_pass http://centrifugo:8000;\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection "upgrade";\n    proxy_set_header Host $host;\n  }\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"Basically it consists of three locations:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"/"})," \u2013 serves the static files from the frontend directory"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"/api"})," \u2013 proxies requests to the backend FastAPI application"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"/connection"})," \u2013 proxies requests to Centrifugo for establishing a connection properly proxying WebSocket Upgrade headers"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"combining-everything-with-docker-compose",children:"Combining everything with Docker Compose"}),"\n",(0,i.jsxs)(n.p,{children:["Finally, we will combine everything with Docker Compose. The ",(0,i.jsx)(n.code,{children:"docker-compose.yml"})," file will look like this:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'services:\n  centrifugo:\n    image: centrifugo/centrifugo:v6\n    container_name: centrifugo\n    ports:\n      - "8000:8000"\n    volumes:\n      - ./centrifugo:/centrifugo\n    command: centrifugo -c /centrifugo/config.json\n    env_file:\n      - .env\n\n  backend:\n    build: ./backend\n    container_name: backend\n    ports:\n      - "5000:5000"\n    volumes:\n      - ./backend:/app\n    env_file:\n      - .env\n    depends_on:\n      - centrifugo\n    command: uvicorn app:app --host 0.0.0.0 --port 5000 --reload\n\n  nginx:\n    image: nginx:latest\n    container_name: nginx\n    ports:\n      - "9000:80"\n    volumes:\n      - ./frontend:/usr/share/nginx/html:ro\n      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf:ro\n    depends_on:\n      - backend\n      - centrifugo\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Note, that to test the app with real OpenAI API you need to set your OpenAI API key in the ",(0,i.jsx)(n.code,{children:".env"})," file:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'OPENAI_API_KEY="<YOUR_OPEN_AI_TOKEN>"\n'})}),"\n",(0,i.jsxs)(n.p,{children:["We made Nginx available on port ",(0,i.jsx)(n.code,{children:"9000"}),", so once you start the application with:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"docker compose up\n"})}),"\n",(0,i.jsxs)(n.p,{children:["you can access the frontend at ",(0,i.jsx)(n.a,{href:"http://localhost:9000",children:"http://localhost:9000"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"In this article, we have shown how to stream ChatGPT responses in real-time using Centrifugo as a real-time transport. We used FastAPI for the backend and OpenAI API for generating responses, but it may be easily adapted to other LLMs or backend frameworks. The example is simple and effective, and it can be used as a starting point for building more complex applications that require real-time streaming of AI responses."}),"\n",(0,i.jsxs)(n.p,{children:["In real app don't forget to handle user authentication, including proper authentication of user in Centrifugo. For Centrifugo part see for example ",(0,i.jsx)(n.a,{href:"/docs/tutorial/centrifugo#adding-jwt-connection-authentication",children:"JWT auth example"})," in our Grand Chat tutorial."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);