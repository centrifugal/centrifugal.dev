"use strict";(self.webpackChunkcentrifugal_dev=self.webpackChunkcentrifugal_dev||[]).push([[3113],{5556:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2024/05/30/real-time-data-compression-experiments","metadata":{"permalink":"/blog/2024/05/30/real-time-data-compression-experiments","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2024-05-30-real-time-data-compression-experiments.md","source":"@site/blog/2024-05-30-real-time-data-compression-experiments.md","title":"Experimenting with real-time data compression by simulating a football match events","description":"This post shows the potential profit of enabling delta compression in channels and demonstrates the reduction of data transfer in various scenarios, including different Centrifugo protocol formats and using WebSocket permessage-deflate compression.","date":"2024-05-30T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"websocket","permalink":"/blog/tags/websocket"},{"label":"compression","permalink":"/blog/tags/compression"}],"readingTime":10.585,"hasTruncateMarker":false,"authors":[{"name":"Alexander Emelin","title":"Founder of Centrifugal Labs","imageURL":"/img/alexander_emelin.jpeg"}],"frontMatter":{"title":"Experimenting with real-time data compression by simulating a football match events","tags":["centrifugo","websocket","compression"],"description":"This post shows the potential profit of enabling delta compression in channels and demonstrates the reduction of data transfer in various scenarios, including different Centrifugo protocol formats and using WebSocket permessage-deflate compression.","author":"Alexander Emelin","authorTitle":"Founder of Centrifugal Labs","authorImageURL":"/img/alexander_emelin.jpeg","image":"/img/football_match_compression.png","hide_table_of_contents":false},"unlisted":false,"nextItem":{"title":"Stream logs from Loki to browser with Centrifugo Websocket-to-GRPC subscriptions","permalink":"/blog/2024/03/18/stream-loki-logs-to-browser-with-websocket-to-grpc-subscriptions"}},"content":"<img src=\\"/img/football_match_compression.png\\" />\\n\\nOptimizing data transfer over WebSocket connections can significantly reduce bandwidth costs. Compressing data usually leads to memory and CPU resource usage overhead \u2013 but in many cases it worth doing anyway since it positively impacts the final bill from the provider (bandwidth cost reduction overweights resource usage increase).\\n\\nCentrifugo v5.4.0 introduced [delta compression](/docs/server/delta_compression) feature. But before implementing it we wanted a playground which could demonstrate the potential benefit of using delta compression in Centrifugo channels.\\n\\nThis post outlines our approach to estimating the potential profit from implementing delta compression. It demonstrates the reduction in data transfer using once concrete use case across various configurations, including different Centrifugo protocol formats and the additional use of WebSocket permessage-deflate compression. Although these numbers can vary significantly depending on the data, we believe the results are valuable for providing a general understanding of Centrifugo compression options. This information can help Centrifugo users apply these insights to their use cases.\\n\\n## About delta compression\\n\\n![delta frames](/img/delta_abstract.png)\\n\\nFor a good overview of delta compression topic for the real-time messaging applications I suggest starting with a [blog post in Ably engineeiring blog](https://ably.com/blog/message-delta-compression).\\n\\nCentrifugo is very similar to Ably in many aspects (though self-hosted), so everything said in the linked post equally applies to Centrifugo use cases too. Though we have differences in the final implementation, one notable is that we are using [Fossil](https://fossil-scm.org/home/doc/tip/www/delta_format.wiki) delta algorithm in Centrifugo instead of VCDIFF. The reason over VCDIFF was mainly two factors:\\n\\n* availability of several Fossil delta implementations, specifically there are good libraries for Go (see [shadowspore/fossil-delta](https://github.com/shadowspore/fossil-delta)), and for Javascript - [fossil-delta-js](https://github.com/dchest/fossil-delta-js).\\n* the compactness of the algorithm implementation \u2013 under 500 lines of code in JavaScript\\n\\nThe compactness property is nice because there are no OSS Fossil implementations for Java, Dart and Swift \u2013 languages we have SDKs for \u2013 so we may have to implement this algorithm in the future ourselves.\\n\\nHaving said this all, let\'s proceed to the description of experiment we did to understand possible benefits of various compression techniques, and delta compression in particular. \\n\\n## Experiment Overview\\n\\nIn the experiment, we simulated a football match, sending the entire game state over a WebSocket connection upon every match event. Our compression playground looks like this:\\n\\n<video width=\\"100%\\" loop={true} autoPlay=\\"autoplay\\" muted controls=\\"\\" src=\\"/img/el_classico.mp4\\"></video>\\n\\nIt visualizes only the score, but under the hood there are other game changes happen \u2013 will be shown below.\\n\\nWe tested various configurations to evaluate the effectiveness of data compression if different cases. In each setup the same game data was sent over the wire. The data then was captured using [WireShark](https://www.wireshark.org/) with the filter:\\n\\n```\\ntcp.srcport == 8000 && websocket\\n```\\n\\nThis is how WebSocket packets look in Wireshark when applying a filter mentioned above:\\n\\n![wireshark](/img/compression_wireshark.png)\\n\\nBytes captured show the entire overhead from packets in the game channel going from server to client (including TCP/IP overhead).\\n\\nThe source code of the experiment may be found on Github as a [Centrifuge library example](https://github.com/centrifugal/centrifuge/tree/master/_examples/compression_playground). You can run it to inspect the exact WebSocket frames in each scenario.\\n\\nTo give reader a general idea about data, we sent 30 publications with the entire football game state, for example here is a first message in a match (2 teams, 11 players):\\n\\n<details>\\n<summary>Click to see the data</summary>\\n<p>\\n\\n```json\\n{\\n   \\"homeTeam\\":{\\n      \\"name\\":\\"Real Madrid\\",\\n      \\"players\\":[\\n         {\\n            \\"name\\":\\"John Doe\\"\\n         },\\n         {\\n            \\"name\\":\\"Jane Smith\\"\\n         },\\n         {\\n            \\"name\\":\\"Alex Johnson\\"\\n         },\\n         {\\n            \\"name\\":\\"Chris Lee\\"\\n         },\\n         {\\n            \\"name\\":\\"Pat Kim\\"\\n         },\\n         {\\n            \\"name\\":\\"Sam Morgan\\"\\n         },\\n         {\\n            \\"name\\":\\"Jamie Brown\\"\\n         },\\n         {\\n            \\"name\\":\\"Casey Davis\\"\\n         },\\n         {\\n            \\"name\\":\\"Morgan Garcia\\"\\n         },\\n         {\\n            \\"name\\":\\"Taylor White\\"\\n         },\\n         {\\n            \\"name\\":\\"Jordan Martinez\\"\\n         }\\n      ]\\n   },\\n   \\"awayTeam\\":{\\n      \\"name\\":\\"Barcelona\\",\\n      \\"players\\":[\\n         {\\n            \\"name\\":\\"Robin Wilson\\"\\n         },\\n         {\\n            \\"name\\":\\"Drew Taylor\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"RED_CARD\\"\\n               }\\n            ]\\n         },\\n         {\\n            \\"name\\":\\"Jessie Bailey\\"\\n         },\\n         {\\n            \\"name\\":\\"Casey Flores\\"\\n         },\\n         {\\n            \\"name\\":\\"Jordan Walker\\"\\n         },\\n         {\\n            \\"name\\":\\"Charlie Green\\"\\n         },\\n         {\\n            \\"name\\":\\"Alex Adams\\"\\n         },\\n         {\\n            \\"name\\":\\"Morgan Thompson\\"\\n         },\\n         {\\n            \\"name\\":\\"Taylor Clark\\"\\n         },\\n         {\\n            \\"name\\":\\"Jordan Hernandez\\"\\n         },\\n         {\\n            \\"name\\":\\"Jamie Lewis\\"\\n         }\\n      ]\\n   }\\n}\\n```\\n\\n</p>\\n</details>\\n\\nThen we send intermediary states \u2013 someone scores goal, gets yellow card, being subsctituted. And here is the end message in simulation (final scores, final events attached to corresponding players):\\n\\n<details>\\n<summary>Click to see the data</summary>\\n<p>\\n\\n```json\\n{\\n   \\"homeTeam\\":{\\n      \\"name\\":\\"Real Madrid\\",\\n      \\"score\\":3,\\n      \\"players\\":[\\n         {\\n            \\"name\\":\\"John Doe\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"YELLOW_CARD\\",\\n                  \\"minute\\":6\\n               },\\n               {\\n                  \\"type\\":\\"SUBSTITUTE\\",\\n                  \\"minute\\":39\\n               }\\n            ]\\n         },\\n         {\\n            \\"name\\":\\"Jane Smith\\"\\n         },\\n         {\\n            \\"name\\":\\"Alex Johnson\\"\\n         },\\n         {\\n            \\"name\\":\\"Chris Lee\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"GOAL\\",\\n                  \\"minute\\":84\\n               }\\n            ]\\n         },\\n         {\\n            \\"name\\":\\"Pat Kim\\"\\n         },\\n         {\\n            \\"name\\":\\"Sam Morgan\\"\\n         },\\n         {\\n            \\"name\\":\\"Jamie Brown\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"SUBSTITUTE\\",\\n                  \\"minute\\":9\\n               }\\n            ]\\n         },\\n         {\\n            \\"name\\":\\"Casey Davis\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"YELLOW_CARD\\",\\n                  \\"minute\\":81\\n               }\\n            ]\\n         },\\n         {\\n            \\"name\\":\\"Morgan Garcia\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"SUBSTITUTE\\",\\n                  \\"minute\\":15\\n               },\\n               {\\n                  \\"type\\":\\"GOAL\\",\\n                  \\"minute\\":30\\n               },\\n               {\\n                  \\"type\\":\\"YELLOW_CARD\\",\\n                  \\"minute\\":57\\n               },\\n               {\\n                  \\"type\\":\\"GOAL\\",\\n                  \\"minute\\":62\\n               },\\n               {\\n                  \\"type\\":\\"RED_CARD\\",\\n                  \\"minute\\":66\\n               }\\n            ]\\n         },\\n         {\\n            \\"name\\":\\"Taylor White\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"YELLOW_CARD\\",\\n                  \\"minute\\":18\\n               },\\n               {\\n                  \\"type\\":\\"SUBSTITUTE\\",\\n                  \\"minute\\":42\\n               },\\n               {\\n                  \\"type\\":\\"SUBSTITUTE\\",\\n                  \\"minute\\":45\\n               },\\n               {\\n                  \\"type\\":\\"YELLOW_CARD\\",\\n                  \\"minute\\":69\\n               },\\n               {\\n                  \\"type\\":\\"RED_CARD\\",\\n                  \\"minute\\":72\\n               }\\n            ]\\n         },\\n         {\\n            \\"name\\":\\"Jordan Martinez\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"SUBSTITUTE\\",\\n                  \\"minute\\":21\\n               },\\n               {\\n                  \\"type\\":\\"SUBSTITUTE\\",\\n                  \\"minute\\":24\\n               }\\n            ]\\n         }\\n      ]\\n   },\\n   \\"awayTeam\\":{\\n      \\"name\\":\\"Barcelona\\",\\n      \\"score\\":3,\\n      \\"players\\":[\\n         {\\n            \\"name\\":\\"Robin Wilson\\"\\n         },\\n         {\\n            \\"name\\":\\"Drew Taylor\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"RED_CARD\\"\\n               },\\n               {\\n                  \\"type\\":\\"GOAL\\",\\n                  \\"minute\\":12\\n               }\\n            ]\\n         },\\n         {\\n            \\"name\\":\\"Jessie Bailey\\"\\n         },\\n         {\\n            \\"name\\":\\"Casey Flores\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"YELLOW_CARD\\",\\n                  \\"minute\\":78\\n               }\\n            ]\\n         },\\n         {\\n            \\"name\\":\\"Jordan Walker\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"SUBSTITUTE\\",\\n                  \\"minute\\":33\\n               }\\n            ]\\n         },\\n         {\\n            \\"name\\":\\"Charlie Green\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"GOAL\\",\\n                  \\"minute\\":51\\n               },\\n               {\\n                  \\"type\\":\\"GOAL\\",\\n                  \\"minute\\":60\\n               },\\n               {\\n                  \\"type\\":\\"SUBSTITUTE\\",\\n                  \\"minute\\":75\\n               }\\n            ]\\n         },\\n         {\\n            \\"name\\":\\"Alex Adams\\"\\n         },\\n         {\\n            \\"name\\":\\"Morgan Thompson\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"YELLOW_CARD\\",\\n                  \\"minute\\":27\\n               },\\n               {\\n                  \\"type\\":\\"SUBSTITUTE\\",\\n                  \\"minute\\":48\\n               }\\n            ]\\n         },\\n         {\\n            \\"name\\":\\"Taylor Clark\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"SUBSTITUTE\\",\\n                  \\"minute\\":3\\n               },\\n               {\\n                  \\"type\\":\\"SUBSTITUTE\\",\\n                  \\"minute\\":87\\n               }\\n            ]\\n         },\\n         {\\n            \\"name\\":\\"Jordan Hernandez\\"\\n         },\\n         {\\n            \\"name\\":\\"Jamie Lewis\\",\\n            \\"events\\":[\\n               {\\n                  \\"type\\":\\"YELLOW_CARD\\",\\n                  \\"minute\\":36\\n               },\\n               {\\n                  \\"type\\":\\"SUBSTITUTE\\",\\n                  \\"minute\\":54\\n               }\\n            ]\\n         }\\n      ]\\n   }\\n}\\n```\\n\\n</p>\\n</details>\\n\\nWhen we used Protobuf encoding for game state we serialized the data according to this Protobuf schema:\\n\\n<details>\\n<summary>Click to see the Protobuf schema for the game state</summary>\\n<p>\\n\\n```protobuf\\nsyntax = \\"proto3\\";\\n\\npackage centrifugal.centrifuge.examples.compression_playground;\\n\\noption go_package = \\"./;apppb\\";\\n\\nenum EventType {\\n  UNKNOWN = 0; // Default value, should not be used\\n  GOAL = 1;\\n  YELLOW_CARD = 2;\\n  RED_CARD = 3;\\n  SUBSTITUTE = 4;\\n}\\n\\nmessage Event {\\n  EventType type = 1;\\n  int32 minute = 2;\\n}\\n\\nmessage Player {\\n  string name = 1;\\n  repeated Event events = 2;\\n}\\n\\nmessage Team {\\n  string name = 1;\\n  int32 score = 2;\\n  repeated Player players = 3;\\n}\\n\\nmessage Match {\\n  int32 id = 1;\\n  Team home_team = 2;\\n  Team away_team = 3;\\n}\\n```\\n\\n</p>\\n</details>\\n\\n## Results Breakdown\\n\\nBelow are the results of our experiment, comparing different protocols and compression settings:\\n\\n| Protocol                   | Compression | Delta      | Delay | Bytes sent | Percentage |\\n|----------------------------|-------------|------------|-------|------------|-----------|\\n| JSON over JSON             | No          | No         | 0     | 40251      | 100.0 (base)     |\\n| JSON over JSON             | Yes         | No         | 0     | 15669      | 38.93     |\\n| JSON over JSON             | No          | Yes        | 0     | 6043       | 15.01     |\\n| JSON over JSON             | Yes         | Yes        | 0     | 5360       | 13.32     |\\n| --             | --          | --         | --     | --      | --     |\\n| JSON over Protobuf         | No          | No         | 0     | 39180      | 97.34     |\\n| JSON over Protobuf         | Yes         | No         | 0     | 15542      | 38.61     |\\n| JSON over Protobuf         | No          | Yes        | 0     | 4287       | 10.65     |\\n| JSON over Protobuf         | Yes         | Yes        | 0     | 4126       | 10.25     |\\n| --             | --          | --         | --     | --      | --     |\\n| Protobuf over Protobuf     | No          | No         | 0     | 16562      | 41.15     |\\n| Protobuf over Protobuf     | Yes         | No         | 0     | 13115      | 32.58     |\\n| Protobuf over Protobuf     | No          | Yes        | 0     | 4382       | 10.89     |\\n| Protobuf over Protobuf     | Yes         | Yes        | 0     | 4473       | 11.11     |\\n\\n## Results analysis\\n\\nLet\'s now discuss the results we observed in detail.\\n\\n### JSON over JSON\\n\\nIn this case we are sending JSON data with football match game state over JSON Centrifugal protocol.\\n\\n1. JSON over JSON (No Compression, No Delta)\\nBytes Sent: 40251\\nPercentage: 100.0%\\nAnalysis: This is a baseline scenario, with no compression and no delta, results in the highest amount of data being sent. But very straightforward to implement.\\n\\n2. JSON over JSON (With Compression, No Delta)\\nBytes Sent: 15669\\nPercentage: 38.93%\\nAnalysis: Enabling compression reduces the data size significantly to 38.93% of the original, showcasing the effectiveness of deflate compression. See [how to configure compression](/docs/transports/websocket#websocket_compression) in Centrifugo, note that it comes with CPU and memory overhead which depends on your load profile.\\n\\n3. JSON over JSON (No Compression, With Delta)\\nBytes Sent: 6043\\nPercentage: 15.01%\\nAnalysis: Using delta compression without deflate compression reduces data size to 15.01% for this use case, only changes are being sent after the initial full payload. See how to enable [delta compression in channels](/docs/server/delta_compression) in Centrifugo. The nice thing about using delta compression instead of deflate compression is that deltas require less and more predictable resource overhead. \\n\\n4. JSON over JSON (With Compression and Delta)\\nBytes Sent: 5360\\nPercentage: 13.32%\\nAnalysis: Combining both compression and delta further reduces the data size to 13.32%, achieving the highest efficiency in this category. The benefit is not huge, because we already send small deltas here.\\n\\n### JSON over Protobuf\\n\\nIn this case we are sending JSON data with football match game state over Protobuf Centrifugal protocol.\\n\\n5. JSON over Protobuf (No Compression, No Delta)\\nBytes Sent: 39180\\nPercentage: 97.34%\\nAnalysis: Switching to Protobuf encoding of Centrifugo protocol but still sending JSON data slightly reduces the data size to 97.34% of the JSON over JSON baseline. The benefit here comes from the fact Centrifugo does not introduce a lot of its own protocol overhead \u2013 Protobuf is more compact. But we still send JSON data as Protobuf payloads \u2013 that\'s why it\'s generally comparable with a baseline.\\n\\n6. JSON over Protobuf (With Compression, No Delta)\\nBytes Sent: 15542\\nPercentage: 38.61%\\nAnalysis: Compression with Protobuf encoding brings similar benefits as with JSON, reducing the data size to 38.61%.\\n\\n7. JSON over Protobuf (No Compression, With Delta)\\nBytes Sent: 4287\\nPercentage: 10.65%\\nAnalysis: Delta compression with Protobuf is effective, reducing data to 10.65%. It\'s almost x10 reduction in bandwidth compared to the baseline!\\n\\nI guess at this point you may be curious how delta frames look like in case of JSON protocol. Here is a screenshot:\\n\\n![delta frames](/img/delta_frames.png)\\n\\n8. JSON over Protobuf (With Compression and Delta)\\nBytes Sent: 4126\\nPercentage: 10.25%\\nAnalysis: This combination provides the best results for JSON over Protobuf, reducing data size to 10.25% from the baseline.\\n\\n### Protobuf over Protobuf\\n\\nIn this case we are sending Protobuf binary data with football match game state over Protobuf Centrifugal protocol.\\n\\n9. Protobuf over Protobuf (No Compression, No Delta)\\nBytes Sent: 16562\\nPercentage: 41.15%\\nAnalysis: Using Protobuf for both encoding and transmission **without any compression or delta** reduces data size to 41.15%. So you may get the most efficient setup with nice bandwidth reduction. But the cost is more complex data encoding.\\n\\n10. Protobuf over Protobuf (With Compression, No Delta)\\nBytes Sent: 13115\\nPercentage: 32.58%\\nAnalysis: Compression reduces the data size to 32.58%. Note, that in this case it\'s not very different from JSON case.  \\n\\n11. Protobuf over Protobuf (No Compression, With Delta)\\nBytes Sent: 4382\\nPercentage: 10.89%\\nAnalysis: Delta compression is again very effective here, reducing the data size to 10.89%. Again - comparable to JSON case.\\n\\n12. Protobuf over Protobuf (With Compression and Delta)\\nBytes Sent: 4473\\nPercentage: 11.11%\\nAnalysis: Combining both methods results in a data size of 11.11%. Even more than in JSON case. That\'s bacause binary data is not compressed very well with deflate algorithm.\\n\\n## Conclusion\\n\\n* WebSocket permessage-deflate compression significantly reduces the amount of data transferred over WebSocket connections. While it incurs CPU and memory overhead, it may be still worth using from a total cost perspective.\\n\\n* Delta compression makes perfect sense for channels where data changes only slightly between publications. In our experiment, it resulted in a tenfold reduction in bandwidth usage.\\n\\n* Using binary data in combination with the Centrifugo Protobuf protocol provides substantial bandwidth reduction even without deflate or delta compression. However, this comes at the cost of increased data format complexity. An additional benefit of using the Centrifugo Protobuf protocol is its faster marshalling and unmarshalling on the server side compared to the JSON protocol.\\n\\nFor Centrifugo, these results highlighted the potential of implementing delta compression, so we proceeded with it. The benefit depends on the nature of the data being sent \u2013 you can achieve even greater savings if you have larger messages that are very similar to each other."},{"id":"/2024/03/18/stream-loki-logs-to-browser-with-websocket-to-grpc-subscriptions","metadata":{"permalink":"/blog/2024/03/18/stream-loki-logs-to-browser-with-websocket-to-grpc-subscriptions","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2024-03-18-stream-loki-logs-to-browser-with-websocket-to-grpc-subscriptions.md","source":"@site/blog/2024-03-18-stream-loki-logs-to-browser-with-websocket-to-grpc-subscriptions.md","title":"Stream logs from Loki to browser with Centrifugo Websocket-to-GRPC subscriptions","description":"Centrifugo has GRPC subscription streams feature, in this post we show how this feature may simplify a task of delivering data to application UI in real-time. We integrate with Loki, injest log entries and stream logs to the browser based on user-supplied query","date":"2024-03-18T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"loki","permalink":"/blog/tags/loki"},{"label":"grpc","permalink":"/blog/tags/grpc"}],"readingTime":7.955,"hasTruncateMarker":true,"authors":[{"name":"Alexander Emelin","title":"Founder of Centrifugal Labs","imageURL":"/img/alexander_emelin.jpeg"}],"frontMatter":{"title":"Stream logs from Loki to browser with Centrifugo Websocket-to-GRPC subscriptions","tags":["centrifugo","loki","grpc"],"description":"Centrifugo has GRPC subscription streams feature, in this post we show how this feature may simplify a task of delivering data to application UI in real-time. We integrate with Loki, injest log entries and stream logs to the browser based on user-supplied query","author":"Alexander Emelin","authorTitle":"Founder of Centrifugal Labs","authorImageURL":"/img/alexander_emelin.jpeg","image":"/img/centrifugo_loki.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Experimenting with real-time data compression by simulating a football match events","permalink":"/blog/2024/05/30/real-time-data-compression-experiments"},"nextItem":{"title":"Discovering Centrifugo PRO: push notifications API","permalink":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications"}},"content":"<img src=\\"/img/centrifugo_loki.jpg\\" />\\n\\nAs of version 5.1.0, Centrifugo introduces an experimental yet powerful extension that promises to simplify the data delivery process to the browser using GRPC streams. We believe it may help you to solve some practical tasks in minutes. Let\'s dive into how this feature works and how you can leverage it in your applications integrating with [Loki](https://grafana.com/oss/loki/) real-time log streaming capabilities.\\n\\n\x3c!--truncate--\x3e\\n\\n## What Are Proxy Subscription Streams?\\n\\n[Proxy Subscription Streams](/docs/server/proxy_streams) support pushing data directly to Centrifugo client channel subscriptions from your application backend over GRPC streams. This feature is designed to facilitate individual data streams to clients as soon as they subscribe to a channel, acting as a bridge between WebSocket connections from clients and GRPC streams to the backend. It supports both unidirectional (backend to client) and bidirectional (both ways) streams, thereby enhancing flexibility in data streaming.\\n\\n![](/img/on_demand_stream_connections.png)\\n\\nThe design is inspired by [Websocketd](http://websocketd.com/) server \u2013 but while Websocketd transforms data from programs running locally, Centrifugo provides a more generic network interface with GRPC. And all other features of Centrifugo like connection authentication, online presence come as a great bonus.\\n\\nIn the documentation for Proxy Subscription Streams we mentioned streaming logs from Loki as one of the possible use cases. Let\'s expand on the idea and implement the working solution in just 10 minutes.\\n\\n## Demo and source code\\n\\nHere is a demo of what we well get:\\n\\n<video width=\\"100%\\" loop={true} autoPlay=\\"autoplay\\" muted controls=\\"\\" src=\\"/img/loki.mp4\\"></video>\\n\\nTake a look at [full source code on Github](https://github.com/centrifugal/examples/tree/master/v5/subscription_streams_loki).\\n\\n## Setting Up Loki\\n\\n[Loki](https://grafana.com/oss/loki/) is a horizontally-scalable, highly-available, multi-tenant log aggregation system inspired by Prometheus. It is designed to be very cost-effective and easy to operate, making it a perfect candidate for our real-time log streaming example.\\n\\nWe will build the example using Docker Compose, all we have to do for the example is to include Loki image to `docker-compose.yml`: \\n\\n```yaml\\nservices:\\n  loki:\\n    image: grafana/loki:2.9.5\\n    ports:\\n      - \\"3100:3100\\"\\n```\\n\\nLoki can ingest logs via various methods, including Promtail, Grafana Agent, Fluentd, and more. For simplicity, we will send logs to Loki ourselves from the Go application.\\n\\nTo send logs to Loki, we can use the HTTP API that Loki provides. This is a straightforward way to push logs directly from an application. The example below demonstrates how to create a simple Go application that generates logs and sends them to Loki using HTTP POST requests.\\n\\nFor this post we will be using Go language to implement the backend part. But it could be any other programming language.\\n\\nFirst, let\'s some code to send a log entries to Loki:\\n\\n```go\\nconst (\\n\\tlokiPushEndpoint = \\"http://loki:3100/loki/api/v1/push\\"\\n)\\n\\ntype lokiPushMessage struct {\\n\\tStreams []lokiStream `json:\\"streams\\"`\\n}\\n\\ntype lokiStream struct {\\n\\tStream map[string]string `json:\\"stream\\"`\\n\\tValues [][]string        `json:\\"values\\"`\\n}\\n\\nfunc sendLogMessageToLoki(_ context.Context) error {\\n\\tsources := []string{\\"backend1\\", \\"backend2\\", \\"backend3\\"}\\n\\tsource := sources[rand.Intn(len(sources))]\\n\\tlogMessage := fmt.Sprintf(\\"log from %s source\\", source)\\n\\n\\tpayload := lokiPushMessage{\\n\\t\\tStreams: []lokiStream{\\n\\t\\t\\t{\\n\\t\\t\\t\\tStream: map[string]string{\\n\\t\\t\\t\\t\\t\\"source\\": source,\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\tValues: [][]string{\\n\\t\\t\\t\\t\\t{fmt.Sprintf(\\"%d\\", time.Now().UnixNano()), logMessage},\\n\\t\\t\\t\\t},\\n\\t\\t\\t},\\n\\t\\t},\\n\\t}\\n\\n\\tjsonData, err := json.Marshal(payload)\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tresp, err := http.Post(\\n\\t\\tlokiPushEndpoint, \\"application/json\\", bytes.NewBuffer(jsonData))\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tdefer resp.Body.Close()\\n\\n\\tif resp.StatusCode != http.StatusNoContent {\\n\\t\\treturn fmt.Errorf(\\"unexpected status code: %d\\", resp.StatusCode)\\n\\t}\\n\\treturn nil\\n}\\n\\nfunc sendLogsToLoki(ctx context.Context) {\\n\\tfor {\\n\\t\\tselect {\\n\\t\\tcase <-ctx.Done():\\n\\t\\t\\treturn\\n\\t\\tcase <-time.After(200 * time.Millisecond):\\n\\t\\t\\terr := sendLogMessageToLoki(ctx)\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\tlog.Println(\\"error sending log to Loki:\\", err)\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\\n\\nfunc main() {\\n\\tctx, cancel := signal.NotifyContext(context.Background(), syscall.SIGTERM, syscall.SIGINT)\\n\\tdefer cancel()\\n\\n\\tsendLogsToLoki(ctx)\\n}\\n```\\n\\nThis program defines a `sendLogsToLoki` function that constructs a log entry and sends it to Loki using its HTTP API. It continuously generates log messages every 200 milliseconds.\\n\\nThe `lokiPushMessage` struct is structured to match the JSON payload expected by Loki\'s [`/loki/api/v1/push`](https://grafana.com/docs/loki/latest/reference/api/#push-log-entries-to-loki) endpoint. Each log entry consists of a set of labels (in the Stream map) and log line values, where each value is a two-element array containing the timestamp and the log line. The timestamp is in nanoseconds to match Loki\'s expected format.\\n\\nNote, in the example we randomly set log entry `source` label choosing between `backend1`, `backend2` and `backend3` values.\\n\\nAt this point our program pushes some logs to Loki, now let\'s add Centrifugo to consume them from browser in real-time.\\n\\n## Configuring Centrifugo\\n\\nAdding Centrifugo is also rather straightforward:\\n\\n```yaml\\nservices:\\n  centrifugo:\\n    image: centrifugo/centrifugo:v5.3.0\\n    restart: unless-stopped\\n    volumes:\\n      - ./centrifugo/config.json:/centrifugo/config.json\\n    command: centrifugo -c config.json\\n    expose:\\n      - 8000\\n```\\n\\nWhere `config.json` is:\\n\\n```json\\n{\\n    \\"client_insecure\\": true,\\n    \\"allowed_origins\\": [\\"http://localhost:9000\\"],\\n    \\"proxy_subscribe_stream_endpoint\\": \\"grpc://backend:12000\\",\\n    \\"proxy_subscribe_stream_timeout\\": \\"3s\\",\\n    \\"namespaces\\": [\\n      {\\n          \\"name\\": \\"logs\\",\\n          \\"proxy_subscribe_stream\\": true\\n      }\\n    ]\\n}\\n```\\n\\nNote, we enabled `client_insecure` option here \u2013 this is to keep example short, but in real live you may benefit from Centrifugo authentication: [JWT-based](/docs/server/authentication) or [proxy-based](/docs/server/proxy#connect-proxy).\\n\\n## Writing frontend\\n\\n```html\\n<!DOCTYPE html>\\n<html lang=\\"en\\">\\n<head>\\n    <meta charset=\\"UTF-8\\">\\n    <title>Streaming logs with Centrifugo and Loki</title>\\n</head>\\n<body>\\n    <div id=\\"app\\">\\n        <form id=\\"input\\" onsubmit=\\"subscribeToLogs(event)\\">\\n            <input type=\\"text\\" id=\\"query\\" autocomplete=\\"off\\" placeholder=\\"Enter log query\\" />\\n            <button id=\\"submit\\" type=\\"submit\\">SUBSCRIBE</button>\\n        </form>\\n        <div id=\\"logs\\" style=\\"margin-top: 20px;\\">\\n            <ul id=\\"lines\\"></ul>\\n        </div>\\n    </div>\\n    <script src=\\"https://unpkg.com/centrifuge@^5/dist/centrifuge.js\\"><\/script>\\n    <script src=\\"app.js\\"><\/script>\\n</body>\\n</html>\\n```\\n\\nIn the final version we\'ve also included some CSS to this HTML to make it look a bit nicer.\\n\\nAnd our Javascript code in `app.js`:\\n\\n```javascript\\nconst logs = document.getElementById(\'logs\');\\nconst lines = document.getElementById(\'lines\');\\nconst queryInput = document.getElementById(\'query\');\\nconst button = document.getElementById(\'submit\');\\n\\nfunction subscribeToLogs(e) {\\n    e.preventDefault();\\n\\n    const query = queryInput.value;\\n    if (!query) {\\n        alert(\'Please enter a query.\');\\n        return;\\n    }\\n    queryInput.disabled = true;\\n    button.disabled = true;\\n\\n    const centrifuge = new Centrifuge(\'ws://localhost:9000/connection/websocket\');\\n\\n    const subscription = centrifuge.newSubscription(\'logs:stream\', {\\n        data: { query: query }\\n    });\\n\\n    subscription.on(\'publication\', function(ctx) {\\n        const logLine = ctx.data.line;\\n        const logItem = document.createElement(\'li\');\\n        logItem.textContent = logLine;\\n        lines.appendChild(logItem);\\n        logs.scrollTop = logs.scrollHeight;\\n    });\\n\\n    subscription.subscribe();\\n    centrifuge.connect();\\n}\\n```\\n\\nIn the final example we\'ve also added Nginx container to serve static files and proxy WebSocket connections to Centrifugo. Check it out in the source code.\\n\\nWhen user enters Loki query to input, subscription goes to Centrifugo and Centrifugo then realizes it\'s a proxy stream subscription (since channel belongs to `logs` channel namespace). Centrifugo then calls the backend GRPC endpoint (`backend:12000`) and expect it to implement unidirectional GRPC stream contract. Our last part here - to implement it.\\n\\n## Handle subscription stream on the Go side\\n\\nOn your backend, we\'ll implement a GRPC service that interacts with Loki to tail logs and then re-send them to Centrifugo subscription stream. Let\'s implement such service.\\n\\nWe first need to take Centrifugo [proxy.proto](https://github.com/centrifugal/centrifugo/blob/master/internal/proxyproto/proxy.proto) definitions. And we will implement `SubscribeUnidirectional` method from it.\\n\\nYou need to install [`protoc`](https://grpc.io/docs/protoc-installation/), also install plugins for Go and GRPC:\\n\\n```bash\\ngo install google.golang.org/protobuf/cmd/protoc-gen-go@latest\\ngo install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest\\n```\\n\\nAnd then:\\n\\n```bash\\nprotoc -I ./ proxy.proto --go_out=./ --go-grpc_out=./\\n```\\n\\nThis will generate Protobuf messages and GRPC code required for writing GRPC service. We can use generated definitions now:\\n\\n```go\\nimport (\\n\\t\\"log\\"\\n\\t\\"fmt\\"\\n\\n\\tpb \\"backend/internal/proxyproto\\"\\n\\t\\"github.com/grafana/loki/pkg/logproto\\"\\n\\t\\"google.golang.org/grpc\\"\\n\\t\\"google.golang.org/grpc/credentials/insecure\\"\\n)\\n\\nconst (\\n\\tlokiGRPCAddress  = \\"loki:9095\\"\\n)\\n\\ntype streamerServer struct {\\n\\tpb.UnimplementedCentrifugoProxyServer\\n\\tlokiQuerierClient logproto.QuerierClient\\n}\\n\\ntype clientData struct {\\n\\tQuery string `json:\\"query\\"`\\n}\\n\\nfunc (s *streamerServer) SubscribeUnidirectional(\\n\\treq *pb.SubscribeRequest,\\n\\tstream pb.CentrifugoProxy_SubscribeUnidirectionalServer,\\n) error {\\n\\tvar cd clientData\\n\\terr := json.Unmarshal(req.Data, &cd)\\n\\tif err != nil {\\n\\t\\treturn fmt.Errorf(\\"error unmarshaling data: %w\\", err)\\n\\t}\\n\\tquery := &logproto.TailRequest{\\n\\t\\tQuery: cd.Query,\\n\\t}\\n\\tctx, cancel := context.WithCancel(stream.Context())\\n\\tdefer cancel()\\n\\n\\tlogStream, err := s.lokiQuerierClient.Tail(ctx, query)\\n\\tif err != nil {\\n\\t\\treturn fmt.Errorf(\\"error querying Loki: %w\\", err)\\n\\t}\\n\\n\\tstarted := time.Now()\\n\\tlog.Println(\\"unidirectional subscribe called with request\\", req)\\n\\tdefer func() {\\n\\t\\tlog.Println(\\"unidirectional subscribe finished, elapsed\\", time.Since(started))\\n\\t}()\\n\\terr = stream.Send(&pb.StreamSubscribeResponse{\\n\\t\\tSubscribeResponse: &pb.SubscribeResponse{},\\n\\t})\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\n\\tfor {\\n\\t\\tselect {\\n\\t\\tcase <-stream.Context().Done():\\n\\t\\t\\treturn stream.Context().Err()\\n\\t\\tdefault:\\n\\t\\t\\tresp, err := logStream.Recv()\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\treturn fmt.Errorf(\\"error receiving from Loki stream: %v\\", err)\\n\\t\\t\\t}\\n\\t\\t\\tfor _, entry := range resp.Stream.Entries {\\n\\t\\t\\t\\tline := fmt.Sprintf(\\"%s: %s\\", entry.Timestamp.Format(\\"2006-01-02T15:04:05.000Z07:00\\"), entry.Line)\\n\\t\\t\\t\\terr = stream.Send(&pb.StreamSubscribeResponse{\\n\\t\\t\\t\\t\\tPublication: &pb.Publication{Data: []byte(`{\\"line\\":\\"` + line + `\\"}`)},\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\tif err != nil {\\n\\t\\t\\t\\t\\treturn err\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\\n\\nfunc main() {\\n\\tquerierConn, err := grpc.Dial(lokiGRPCAddress, grpc.WithTransportCredentials(insecure.NewCredentials()))\\n\\tif err != nil {\\n\\t\\tlog.Fatalf(\\"failed to dial Loki: %v\\", err)\\n\\t}\\n\\tquerierClient := logproto.NewQuerierClient(querierConn)\\n\\n\\taddr := \\":12000\\"\\n\\tlis, err := net.Listen(\\"tcp\\", addr)\\n\\tif err != nil {\\n\\t\\tlog.Fatalf(\\"failed to listen: %v\\", err)\\n\\t}\\n\\n\\ts := grpc.NewServer(grpc.MaxConcurrentStreams(math.MaxUint32))\\n\\tpb.RegisterCentrifugoProxyServer(s, &streamerServer{\\n\\t\\tlokiQuerierClient: querierClient,\\n\\t})\\n\\n\\tlog.Println(\\"Server listening on\\", addr)\\n\\tif err := s.Serve(lis); err != nil {\\n\\t\\tlog.Fatalf(\\"failed to serve: %v\\", err)\\n\\t}\\n}\\n```\\n\\nThings to note:\\n\\n* Loki also supports GRPC interface to tail logs, so we use it here. We could also use Loki WebSocket endpoint [`/loki/api/v1/tail`](https://grafana.com/docs/loki/latest/reference/api/#stream-log-messages) but this would mean establishing new connection for every tail operation - with GRPC we can use many concurrent tail requests all multiplexed over a single network connection.\\n* When subscription stream initialized from Centrifugo side we start tailing logs from Loki and resend them to Centrifugo\\n* Centrifugo then packs data to WebSocket connection and delivers to browser.\\n\\n:::caution\\n\\nNote, we bypass some security considerations in this example. In practice you must be more careful with query supplied by user in the form - validate and sanitize it before passing to Loki. Proxy subscription GRPC contract allows you to communicate custom errors with the client-side.\\n\\n:::\\n\\n## Conclusion\\n\\nSubscription streams may be a very powerful generic feature in your arsenal. Here we\'ve shown how simple it could be to make a proof of concept of the real-time application which consumes individual data from third-party streaming provider.\\n\\nCentrifugo provides WebSocket SDKs for popular languages used to build UI layer, provides authentication and proper management of real-time connections. And with subscription streams feature Centrifugo gives you an answer on how to quickly translate real-time data based on individual query to user."},{"id":"/2023/10/29/discovering-centrifugo-pro-push-notifications","metadata":{"permalink":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2023-10-29-discovering-centrifugo-pro-push-notifications.md","source":"@site/blog/2023-10-29-discovering-centrifugo-pro-push-notifications.md","title":"Discovering Centrifugo PRO: push notifications API","description":"We start talking more about recently launched Centrifugo PRO. In this post, we share details about Centrifugo PRO push notification API implementation - how it works and what makes it special and practical.","date":"2023-10-29T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"pro","permalink":"/blog/tags/pro"},{"label":"push notifications","permalink":"/blog/tags/push-notifications"}],"readingTime":13.125,"hasTruncateMarker":true,"authors":[{"name":"Alexander Emelin","title":"Founder of Centrifugal Labs","imageURL":"/img/alexander_emelin.jpeg"}],"frontMatter":{"title":"Discovering Centrifugo PRO: push notifications API","tags":["centrifugo","pro","push notifications"],"description":"We start talking more about recently launched Centrifugo PRO. In this post, we share details about Centrifugo PRO push notification API implementation - how it works and what makes it special and practical.","author":"Alexander Emelin","authorTitle":"Founder of Centrifugal Labs","authorImageURL":"/img/alexander_emelin.jpeg","image":"/img/blog_push_notifications_cover_thumb.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Stream logs from Loki to browser with Centrifugo Websocket-to-GRPC subscriptions","permalink":"/blog/2024/03/18/stream-loki-logs-to-browser-with-websocket-to-grpc-subscriptions"},"nextItem":{"title":"Using Centrifugo in RabbitX","permalink":"/blog/2023/08/29/using-centrifugo-in-rabbitx"}},"content":"<img src=\\"/img/blog_push_notifications_cover.jpg\\" />\\n\\nIn our [v5 release post](/blog/2023/06/29/centrifugo-v5-released), we announced the upcoming launch of Centrifugo PRO. We are happy to say that it was released soon after that, and at this point, we already have several customers of the PRO version.\\n\\nI think it\'s time to look at the current state of the PRO version and finally start talking more about its benefits. In this post, we will talk more about one of the coolest PRO features we have at this point: the push notifications API.\\n\\n\x3c!--truncate--\x3e\\n\\n## Centrifugo PRO goals\\n\\nWhen Centrifugo was originally created, its main goal was to help introduce real-time messaging features to existing systems, written in traditional frameworks which work on top of the worker/thread model. Serving many concurrent connections is a non-trivial task in general, and without native efficient concurrency support, it becomes mostly impossible without a shift in the technology stack. Integrating with Centrifugo makes it simple to introduce an efficient real-time layer, while keeping the existing application architecture.\\n\\nAs time went on, Centrifugo got some unique features which now justify its usage even in conjunction with languages/frameworks with good concurrency support. Simply using Centrifugo for at-most-once PUB/SUB may already save a lot of development time. The task, which seems trivial at first glance, has a lot of challenges in practice: client SDKs with reconnect and channel multiplexing, scalability to many nodes, WebSocket fallbacks, etc.\\n\\nThe combination of useful possibilities has made Centrifugo an attractive component for building enterprise-level applications. Let\'s be honest here - for pet projects, developers often prefer writing WebSocket communications themselves, and Centrifugo may be too heavy and an extra dependency. But in a corporate environment, the decision on which technology to use should take into account a lot of factors, like those we just mentioned above. Using a mature technology is often preferred to building from scratch and making all the mistakes along the way.\\n\\nWith the PRO version, our goal is to provide even more value for established businesses when switching to Centrifugo. We want to solve tricky cases and simplify them for our customers; we want to step into related areas where we see we can provide sufficient value.\\n\\nOne rule we try to follow for PRO features that extend Centrifugo\u2019s scope is this: we are not trying to replicate something that already exists in other systems, but rather, we strive to improve upon it. We focus on solving practical issues that we observe, providing a unique value proposition for our customers. This post describes one such example \u2014 we will demonstrate our approach to push notifications, which is [one the features](/docs/pro/overview#features) of Centrifugo PRO.\\n\\n## Why providing push notifications API\\n\\n<img src=\\"/img/push_characters.jpg\\" />\\n\\nWhy provide a push notifications API at all? Well, actually, real-time messages and push notifications are so close that many developers hardly see the difference before starting to work with both more closely.\\n\\nI\u2019ve heard several stories where chat functionality on mobile devices was implemented using only native push notifications \u2014 without using a separate real-time transport like WebSocket while the app is in the foreground. While this is not a recommended approach due to the delivery properties of push notifications, it proves that real-time messages and push notifications are closely related concepts and sometimes may interchange with each other.\\n\\nWhen developers introduce WebSocket communication in an application, they often ask the question\u2014what should I do next to deliver some important messages to a user who is currently not actively using the application? WebSockets are great when the app is in the foreground, but when the app goes to the background, the recommended approach is to close the WebSocket connection. This is important to save battery, and operating systems force the closing of connections after some time anyway.\\n\\nThe delivery of important app data is then possible over push notifications. See a [good overview of them on web.dev](https://web.dev/articles/push-notifications-overview).\\n\\nPreviously, Centrifugo positioned itself solely as a transport layer for real-time messages. In our FAQ, we emphasized this fact and suggested using separate software products to send push notifications.\\n\\nNow, with Centrifugo PRO, we provide this functionality to our customers. We have extended our server API with methods to manage and send push notifications. I promised to tell you why we believe our implementation is super cool. Let\u2019s dive into the details.\\n\\n## Push notifications API like no one provides\\n\\nPush notifications are super handy, but there\u2019s a bit to do to get them working right. Let\'s break it down!\\n\\n#### On the user\'s side (frontend)\\n\\n* Request permission from the user to receive push notifications.\\n* Integrate with the platform-specific notification service (e.g., Apple Push Notification Service for iOS, Firebase Cloud Messaging for Android) to obtain the device token.\\n* Send the device token to the server for storage and future use.\\n* Integrate with the platform-specific notification handler to listen for incoming push notifications\\n* Handle incoming push notifications: display the notification content to the user, either as a banner, alert, or in-app message, depending on the user\'s preferences and the type of notification. Handle user actions on the notification, such as opening the app, dismissing the notification, or taking a specific action related to the notification content.\\n\\n#### On the server (backend)\\n\\n* Store device tokens in a database when received from the client side\\n* Regularly clean up the database to remove stale or invalid device tokens. and handle scenarios where a device token becomes invalid or is revoked by the user, ensuring that no further notifications are sent to that device.\\n* Integrate with platform-specific notification services (e.g., APNS, FCM) to send notifications to devices. Handle errors or failures in sending notifications and implement retry mechanisms if necessary.\\n* Track the delivery status of each push notification sent out. Monitor the open rates, click-through rates, and other relevant metrics for the notifications.\\n* Use analytics to understand user behavior in response to notifications and refine the notification strategy based on insights gained.\\n\\nWe believe that we were able to achieve a unique combination of design decisions which allows us to provide push notification support like no one else provides. Let\u2019s dive into what makes our approach special!\\n\\n## Frontend decisions\\n\\nWhen providing the push notification feature, other solutions like Pusher or Ably also offer their own SDKs for managing notifications on the client side.\\n\\nWhat we\'ve learned, though, during the Centrifugo life cycle, is that creating and maintaining client SDKs for various environments (iOS, Android, Web, Flutter) is one of the hardest parts of the Centrifugo project.\\n\\nSo the decision here was simple and natural: Centrifugo PRO does not introduce any client SDKs for push notifications on the client side.\\n\\nWhen integrating with Centrifugo, you can simply use the native SDKs provided by each platform. We bypass the complexities of SDK development and concentrate on server-side improvements. With this decision, we are not introducing any limitations to the client side.\\n\\nYou get:\\n\\n* Wealthy documentation and community support. Platforms like APNs provide comprehensive documentation, tutorials, and best practices, making the integration process smoother.\\n* Stability and reliability: native SDKs are rigorously tested and frequently updated by the platform providers. This ensures that they are stable, reliable, and free from critical bugs.\\n* Access to the latest features. As platform providers roll out new features or enhancements, native SDKs are usually the first to get updated. This ensures that your application can leverage the latest functionalities without waiting for SDKs to catch up.\\n\\nThis approach was not possible with our real-time SDKs, as WebSocket communication is very low-level, and Centrifugo\u2019s main goal was to provide some high-level features on top of it. However, with push notifications, proceeding without a custom SDK seems like a choice beneficial for everyone.\\n\\n## Server implementation\\n\\nThe main work we did was on the server side. Let\'s go through the entire workflow of push notification delivery and describe what Centrifugo PRO provides for each step.\\n\\n### How we keep tokens\\n\\nLet\'s suppose you got the permission from the user and received the device push token. At this point you must save it to database for sending notifications later using this token. Centrifugo PRO provides API called [device_register](/docs/pro/push_notifications#device_register) to do exactly this.\\n\\nAt this point, we use PostgreSQL for storing tokens \u2013 which is a very popular SQL database. Probably we will add more storage backend options in the future.\\n\\nWhen calling Centrifugo `device_register` API you can provide user ID, list of topics to subscribe, platform from which the user came from (ios, android, web), also push notifications provider. To deliver push notifications to devices Centrifugo PRO integrates with the following push notification providers:\\n\\n* fcm - [Firebase Cloud Messaging (FCM)](https://firebase.google.com/docs/cloud-messaging) <i className=\\"bi bi-android2\\" style={{\'color\': \'yellowgreen\'}}></i> <i className=\\"bi bi-apple\\" style={{\'color\': \'cornflowerblue\'}}></i> <i className=\\"bi bi-globe\\" style={{color: \'orange\'}}></i>\\n* hms - [Huawei Messaging Service (HMS) Push Kit](https://developer.huawei.com/consumer/en/hms/huawei-pushkit/) <i className=\\"bi bi-android2\\" style={{\'color\': \'yellowgreen\'}}></i> <i className=\\"bi bi-apple\\" style={{\'color\': \'cornflowerblue\'}}></i> <i className=\\"bi bi-globe\\" style={{color: \'orange\'}}></i>\\n* apns - [Apple Push Notification service (APNs) ](https://developer.apple.com/documentation/usernotifications) <i className=\\"bi bi-apple\\" style={{\'color\': \'cornflowerblue\'}}></i>\\n\\n![Push](/img/push_notifications.png)\\n\\nSo we basically cover all the most popular platforms out of the box.\\n\\nAfter registering the device token, Centrifugo PRO returns a `device_id` to you. This device ID must be stored on the client device. As long as the frontend has this `device_id`, it can update the device\'s push token information from time to time to keep it current (by just calling `device_register` again, but with `device_id` attached).\\n\\nAfter saving the token, your backend can start sending push notifications to devices.\\n\\n### How we send notifications\\n\\nTo send push notifications we provide another API called [send_push_notification](/docs/pro/push_notifications#send_push_notification). You need to provide some filter in the API request to tell Centrifugo who you want to send notification. You also need to provide push notification payload. For example, using Centrifugo HTTP API:\\n\\n```bash\\ncurl -X POST http://localhost:8000/api/send_push_notification \\\\\\n-H \\"Authorization: apikey <KEY>\\" \\\\\\n-d @- <<\'EOF\'\\n\\n{\\n    \\"recipient\\": {\\n        \\"filter\\": {\\n            \\"topics\\": [\\"test\\"]\\n        }\\n    },\\n    \\"notification\\": {\\n        \\"fcm\\": {\\n            \\"message\\": {\\n                \\"notification\\": {\\"title\\": \\"Hello\\", \\"body\\": \\"How are you?\\"}\\n            }\\n        }\\n    }\\n}\\nEOF\\n```\\n\\n\\nHere is another important decision we made: Centrifugo PRO allows you to specify raw JSON objects for each provider we support. In other words, we do not wrap the push notifications API for FCM, APNS, HMS - we give you a way to construct the entire push notification message.\\n\\nThis means the Centrifugo push API supports all the fields of push notification payloads out-of-the-box, for all push providers. You can simply use the documentation of FCM, APNs, and send the constructed requests to Centrifugo. There is no need for us to update Centrifugo PRO in any way to support new fields added by providers to push APIs.\\n\\nWhen you send a push notification with a filter and push payload for each provider you want, it\'s queued by Centrifugo. We use Redis Streams for queuing and optionally a queue based on PostgreSQL (less efficient, but still robust enough).\\n\\nThe fact that the notification is being queued means a very fast response time \u2013 so you can integrate with Centrifugo from within the hot paths of your application backend. You may additionally provide a push expiration time and a unique push identifier. If you have not provided a unique identifier, Centrifugo generates one for you and returns it in the response. The unique identifier may later be used to track push status in Centrifugo PRO\'s push notification analytics.\\n\\nWe then have efficient workers which process the queue with minimal latency and send push notifications using batch requests for each provider - i.e., we do this in the most effective way possible. We conducted a benchmark of our worker system with FCM \u2013 and we can easily send **several million pushes per minute**.\\n\\nAnother decision we made - Centrifugo PRO supports sending push notifications to a raw list of tokens. This makes it possible for our customers to use their own token storage. For example, such storage could already exist before you started using Centrifugo, or you might need a different storage/schema. In such cases, you can use Centrifugo just as an effective push sender server.\\n\\nFinally, Centrifugo PRO supports sending delayed push notification - to queue push for a later delivery, so for example you can send notification based on user time zone and let Centrifugo PRO send it when needed. Or you may send slightly delayed push notification together with real-time message and if client provided an ack to real-time message - [cancel push notification](/docs/pro/push_notifications#cancel_push).\\n\\n### Secure unified topics\\n\\nFCM and HMS have a built-in way of sending notification to large groups of devices over topics mechanism (the same for HMS). One problem with native FCM or HMS topics though is that device can subscribe to any topic from the frontend side without any permission check. In today\'s world this is usually not desired. So Centrifugo PRO re-implements FCM and HMS topics by introducing an additional API to manage device subscriptions to topics.\\n\\nCentrifugo PRO device topic subscriptions also **add a way to introduce the missing topic semantics for APNs**.\\n\\nCentrifugo PRO additionally provides an API to create persistent bindings of user to notification topics. See [user_topic_list](/docs/pro/push_notifications#user_topic_list) and [user_topic_update](/docs/pro/push_notifications#user_topic_update). As soon as user registers a device \u2013 it will be automatically subscribed to its own topics pre-created over the API. As soon as user logs out from the app and you update user ID of the device - user topics binded to the device automatically removed/switched.\\n\\nThis design solves one of the issues with push notifications (with FCM in particular) \u2013 if two different users use the same device it\'s becoming problematic to unsubscribe the device from large number of topics upon logout. Also, as soon as user to topic binding added (using `user_topic_update` API) \u2013 it will be synchronized across all user active devices. You can still manage such persistent subscriptions on the application backend side if you prefer and provide the full list inside `device_register` call - Centrifugo PRO API gives you freedom here.\\n\\n### Push analytics\\n\\nCentrifugo PRO offers the ability to inspect sent push notifications using [ClickHouse analytics](/docs/pro/analytics). Push providers may also offer their own analytics, such as FCM, which provides insight into push notification delivery. Centrifugo PRO also offers a way to analyze push notification delivery and interaction using the [update_push_status](/docs/pro/push_notifications#update_push_status) API. This API allows updating ClickHouse table and add status for each push sent:\\n\\n* `delivered`\\n* or `interacted`\\n\\nIt\'s then possible to make queries to ClickHouse and build various analytical reports. Or use ClickHouse for real-time graphs - for example, from Grafana.\\n\\n### Push notifications UI\\n\\nFinally, Centrifugo PRO provides a simple web UI for inspecting registered devices. It can simplify development, provide a way to look at live data, and send simple push notification alerts to users or topics.\\n\\n![](/img/push_ui.png)\\n\\n## Conclusion\\n\\nWe really believe in our push notifications and will be working hard to make them even better. The API we already have serves well to cover common push notification delivery use cases, but we won\'t stop here. Some areas for improvements are: functionality of built-in push notifications web UI, extending push analytics by providing user friendly UI for the insights about push delivery and engagement. The good thing is that we already have a ground for making this.\\n\\nTake a look at the documentation of [Centrifugo PRO push notification API](/docs/pro/push_notifications) for more formal details and some things not mentioned here. Probably at the time you are reading this we already added something great to the API.\\n\\nEven though Centrifugo PRO is pretty new, it already has a lot of helpful features, and we have plans to add even more. You can see what\u2019s coming up next on our [Centrifugo PRO planned features board](https://github.com/orgs/centrifugal/projects/3/views/1). We\'re excited to share more blog posts like this one in the future."},{"id":"/2023/08/29/using-centrifugo-in-rabbitx","metadata":{"permalink":"/blog/2023/08/29/using-centrifugo-in-rabbitx","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2023-08-29-using-centrifugo-in-rabbitx.md","source":"@site/blog/2023-08-29-using-centrifugo-in-rabbitx.md","title":"Using Centrifugo in RabbitX","description":"In this post, the engineering team of RabbitX platform shares details about the usage of Centrifugo in their product.","date":"2023-08-29T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"interview","permalink":"/blog/tags/interview"},{"label":"usecase","permalink":"/blog/tags/usecase"}],"readingTime":3.655,"hasTruncateMarker":true,"authors":[{"name":"Centrifugal + RabbitX","title":"The interview with RabbitX engineering team","imageURL":"https://d1muf25xaso8hp.cloudfront.net/https%3A%2F%2F3918ead037b1d3dc3ed05287664aeaed.cdn.bubble.io%2Ff1655453377613x154516784582627620%2FLogo%2520big.png?w=128&h=&auto=compress&dpr=1&fit=max"}],"frontMatter":{"title":"Using Centrifugo in RabbitX","tags":["centrifugo","interview","usecase"],"description":"In this post, the engineering team of RabbitX platform shares details about the usage of Centrifugo in their product.","author":"Centrifugal + RabbitX","authorTitle":"The interview with RabbitX engineering team","authorImageURL":"https://d1muf25xaso8hp.cloudfront.net/https%3A%2F%2F3918ead037b1d3dc3ed05287664aeaed.cdn.bubble.io%2Ff1655453377613x154516784582627620%2FLogo%2520big.png?w=128&h=&auto=compress&dpr=1&fit=max","image":"/img/rabbitx_thumb.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Discovering Centrifugo PRO: push notifications API","permalink":"/blog/2023/10/29/discovering-centrifugo-pro-push-notifications"},"nextItem":{"title":"Asynchronous message streaming to Centrifugo with Benthos","permalink":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos"}},"content":"<img src=\\"/img/rabbitx.png\\" />\\n\\nThis post introduces a new format in Centrifugal blog \u2013 interview with a Centrifugo user! Let\'s dive into an exciting chat with the engineering team of [RabbitX platform](https://landing.rabbitx.io/), a global permissionless perpetuals exchange powered on Starknet. We will discover how Centrifugo helped RabbitX to build a broker platform with current trading volume of 25 million USD daily! \ud83d\ude80\ud83c\udf89\\n\\n\x3c!--truncate--\x3e\\n\\n#### [Q] Hey team - thanks for your desire to share your Centrifugo use case. First of all, could you provide some information about RabbitX - what is it?\\n\\nRabbitX is a global permissionless perpetuals exchange built on Starknet. RabbitX is building the most secure and liquid global derivatives network, giving you 24/7 access to global markets anywhere in the world, with 20x leverage. In its core there is an orderbook - where traders match against market makers, which require to support high throughput and low latency tech stack.\\n\\nThe technologies that we are using:\\n\\n* Tarantool as in-memory database and business logic server\\n* Centrifugo as our major websocket server\\n* Different stark tech to support decentralized settlement\\n\\n#### [Q] Great! What is the goal of Centrifugo in your project? Which real-time features you have?\\n\\nAlmost all the information users see in our terminal is streamed over Centrifugo. We use it for financial order books, candlestick chart updates, and stat number updates. We can also send real-time personal user notifications via Centrifugo. Instead of all the words, here is a short recording of our terminal trading BTC:\\n\\n<video width=\\"100%\\" loop={true} autoPlay=\\"autoplay\\" muted controls=\\"\\" src=\\"/img/rabbitx.mp4?v=1\\"></video>\\n\\n#### [Q] We know that you are using Centrifugo Tarantool engine - could you explain why and how it works in your case?\\n\\nWell, that\'s an interesting thing. We heavily use Tarantool in our system. It grants us immense flexibility, performance, and the power to craft whatever we envision. It ensures the atomicity essential for trading match-making.\\n\\nWhen we were in search of a WebSocket real-time bus for messages, we were pleasantly surprised to discover that Centrifugo integrates with Tarantool. In our scenario, this allowed us to bypass additional network round-trips, as we can stream data directly from Tarantool to Centrifugo channels. Reducing latency is paramount for financial instruments.\\n\\nFurthermore, I can mention that over our nine months in production, we didn\'t encounter any issues with Centrifugo \u2013 it performed flawlessly!\\n\\nRegarding authentication, we employ Centrifugo\'s JWT authentication and subscribe proxy. Thus, subscriptions are authorized on our specialized service written in Go. We\'re also actively using Centrifugo possibility to send initial channel data in the subscribe proxy response.\\n\\nOne challenge we overcame was bridging the gap between the subscription\'s initial request and the continuous message stream in the order book component. To address this, we employed our own sequence numbers in events, coupled with Centrifugo\'s channel history \u2013 this allowed us to deal with missed events when needed. Actually the gaps in event stream are rare in practice and our workaround not needed most of the time, but now we\'re confident our users never experience this issue.\\n\\n#### [Q] Looking at RabbitX terminal app we see quite modern UI - could you share more details about it too?\\n\\nOur frontend is built on top of React in combination with [TradingView Supercharts](https://www.tradingview.com/chart/). And of course we are using `centrifuge-js` SDK for establishing connections with Centrifugo.\\n\\n#### [Q] So you are nine months in production at this point. Can you share some real world numbers related to your Centrifugo setup?\\n\\nAt this point we can have up to a thousand active concurrent traders and send more than 60 messages per second towards one client in peak hours. All the load is served with a single Centrifugo instance (and we have one standby instance).\\n\\n#### [Q] Anything else you want to share with readers of Centrifugal blog?\\n\\nWhen we designed the system the main goal was to have a homogeneous tech zoo, with a small amount of different technologies, to keep the number of failure points as small as possible. Tarantool is a sort of technology that really allows us to achieve this, we were able to add different decentralized mechanics to our system because of that. It\u2019s not only an in-memory database, but in reality the app server as well.\\n\\nIn our case, the fact Centrifugo supports Tarantool broker was a big discovery \u2013 the integration went smoothly, and everything has been working great since then."},{"id":"/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos","metadata":{"permalink":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2023-08-19-asynchronous-message-streaming-to-centrifugo-with-benthos.md","source":"@site/blog/2023-08-19-asynchronous-message-streaming-to-centrifugo-with-benthos.md","title":"Asynchronous message streaming to Centrifugo with Benthos","description":"In this post, we\'ll demonstrate how to asynchronously stream messages into Centrifugo channels from external data providers using Benthos tool. We also highlight some pitfalls which become more important in asynchronous publishing scenario.","date":"2023-08-19T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"benthos","permalink":"/blog/tags/benthos"},{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":7.08,"hasTruncateMarker":true,"authors":[{"name":"Alexander Emelin","title":"Author of Centrifugo","imageURL":"https://github.com/FZambia.png"}],"frontMatter":{"title":"Asynchronous message streaming to Centrifugo with Benthos","tags":["centrifugo","benthos","tutorial"],"description":"In this post, we\'ll demonstrate how to asynchronously stream messages into Centrifugo channels from external data providers using Benthos tool. We also highlight some pitfalls which become more important in asynchronous publishing scenario.","author":"Alexander Emelin","authorTitle":"Author of Centrifugo","authorImageURL":"https://github.com/FZambia.png","image":"/img/benthos_thumb.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Using Centrifugo in RabbitX","permalink":"/blog/2023/08/29/using-centrifugo-in-rabbitx"},"nextItem":{"title":"Centrifugo v5 released","permalink":"/blog/2023/06/29/centrifugo-v5-released"}},"content":"<img src=\\"/img/benthos.png\\" />\\n\\nCentrifugo provides HTTP and GRPC APIs for publishing messages into channels. Publish server API is very straighforward to use - it\'s a simple request with a channel and data to be delivered to active WebSocket connections subscribed to a channel.\\n\\nSometimes though Centrifugo users want to avoid synchronous calls to Centrifugo API delegating this work to asynchronous tasks. Many companies have convenient infrastructure for messaging processing tasks - like Kafka, Nats Jetstream, Redis, RabbitMQ, etc. Some using transactional outbox pattern to reliably deliver events upon database changes and have a ready infrastructure to push such events to some queue. From which want to re-publish events to Centrifugo.\\n\\nIn this post we get familiar with a tool called [Benthos](https://www.benthos.dev/) and show how it may simplify integrating your asynchronous message flow with Centrifugo. And we discuss some non-obvious pitfalls of asynchronous publishing approach in regards to real-time applications.\\n\\n\x3c!--truncate--\x3e\\n\\n## Start Centrifugo\\n\\nFirst start Centrifugo (with debug logging to see incoming API requests in logs):\\n\\n```bash\\ncentrifugo genconfig\\ncentrifugo -c config.json --log_level debug\\n```\\n\\nHope this step is already simple for you, if not - check out [Quickstart tutorial](/docs/getting-started/quickstart).\\n\\n## Install and run Benthos\\n\\nBenthos is an awesome tool which allows consuming data from various inputs, process data, then output it into configured outputs. See more detailed description [on Benthos\' website](https://www.benthos.dev/docs/about).\\n\\nThe number of inputs supported by Benthos is huge: [check it out here](https://www.benthos.dev/docs/components/inputs/about#categories). Most of the major systems widely used these days are supported. Benthos also supports [many outputs](https://www.benthos.dev/docs/components/outputs/about#categories) \u2013 but here we only interested in message transfer to Centrifugo. There is no built-in Centrifugo output in Benthos but it provides a generic `http_client` output which may be used to send requests to any HTTP server. Benthos may also help with retries, provides tools for additional data processing and transformations.\\n\\n![](/img/benthos.svg)\\n\\nJust like Centrifugo Benthos written in Go language \u2013 so its installation is very straighforward and similar to Centrifugo. See [official instructions](https://www.benthos.dev/docs/guides/getting_started).\\n\\nLet\'s assume you\'ve installed Benthos and have `benthos` executable available in your system. Let\'s create Benthos configuration file:\\n\\n```bash\\nbenthos create > config.yaml\\n```\\n\\nTake a look at generated `config.yaml` - it contains various options for Benthos instance, the most important (for the context of this post) are `input` and `output` sections.\\n\\nAnd after that you can start Benthos instance with:\\n\\n```bash\\nbenthos -c config.yaml\\n```\\n\\nNow we need to tell Benthos from where to get data and how to send it to Centrifugo.\\n\\n## Configure Benthos input and output\\n\\nFor our example here we will user Redis List as an input, won\'t add any additional data processing and will output messages consumed from Redis List into Centrifugo publish server HTTP API.\\n\\nTo achieve this add the following as input in Benthos `config.yaml`:\\n\\n```yaml\\ninput:\\n  label: \\"centrifugo_redis_consumer\\"\\n  redis_list:\\n    url: \\"redis://127.0.0.1:6379\\"\\n    key: \\"centrifugo.publish\\"\\n```\\n\\nAnd configure the output like this:\\n\\n```yaml\\noutput:\\n  label: \\"centrifugo_http_publisher\\"\\n  http_client:\\n    url: \\"http://localhost:8000/api/publish\\"\\n    verb: POST\\n    headers:\\n      X-API-Key: \\"<CENTRIFUGO_API_KEY>\\"\\n    timeout: 5s\\n    max_in_flight: 20\\n```\\n\\nThe output points to Centrifugo [publish HTTP API](/docs/server/server_api#publish). Replace `<CENTRIFUGO_API_KEY>` with your Centrifugo `api_key` (found in Centrifugo configuration file).\\n\\n## Push messages to Redis queue\\n\\nStart Benthos instance:\\n\\n```bash\\nbenthos -c config.yaml\\n```\\n\\nYou will see errors while Benthos tries to connect to input Redis source. So start Redis server:\\n\\n```bash\\ndocker run --rm -it --name redis redis:7\\n```\\n\\nNow connect to Redis (using `redis-cli`):\\n\\n```bash\\ndocker exec -it redis redis-cli\\n```\\n\\nAnd push command to Redis list:\\n\\n```\\n127.0.0.1:6379> rpush centrifugo.publish \'{\\"channel\\": \\"chat\\", \\"data\\": {\\"input\\": \\"test\\"}}\'\\n(integer) 1\\n```\\n\\nThis message will be consumed from Redis list by Benthos and published to Centrifugo HTTP API. If you have active subscribers to channel `chat` \u2013 you will see messages delivered to them. That\'s it!\\n\\n:::tip\\n\\nWhen using Redis List input you can scale out Benthos instances to run several of them if needed.\\n\\n:::\\n\\n## Demo\\n\\nHere is a quick demonstration of the described integration. See how we push messages into Redis List and those are delivered to WebSocket clients:\\n\\n<video width=\\"100%\\" controls>\\n  <source src=\\"/img/benthos.mp4\\" type=\\"video/mp4\\" />\\n  Sorry, your browser doesn\'t support embedded video.\\n</video>\\n\\n## Pitfalls of async publishing\\n\\nThis all seems simple. But publishing messages asynchronously may highlight some pitfalls not visible or not applicable for synchronous publishing to Centrifugo API.\\n\\n### Late delivery\\n\\nMost of the time it will work just fine. But one day you can come across intermediate queue growth and increased delivery lag. This may happen due to temporary Centrifugo or worker process unavailability. As soon as system comes back to normal queued messages will be delivered.\\n\\nDepending on the real-time feature implemented this may be a concern to think about and decide whether this is desired or not. Your application should be designed accordingly to deal with late delivery.\\n\\nBTW late delivery may be a case even with synchronous publishing \u2013 it just almost never strikes. But theoretically client can reload browser page and load initial app state while message flying from the backend to client over Centrifugo. It\'s not Centrifugo specific actually - it\'s just a nature of networks and involved latencies.\\n\\nIn general solution to prevent late delivery UX issues completely is using object versioning. Object version should be updated in the database on every change from which the real-time event is generated. Attach object version information to every real-time message. Also get object version on initial state load. This way you can compare versions and drop non-actual real-time messages on client side.\\n\\nPossible strategy may be using synchronous API for real-time features where at most once delivery is acceptable and use asynchronous delivery where you need to deliver messages with at least once guarantees. In a latter case you most probably designed proper idempotency behaviour on client side anyway. \\n\\n### Ordering concerns\\n\\nAnother thing to consider is message ordering. Centrifugo itself [may provide message ordering in channels](/docs/getting-started/design#message-order-guarantees). If you published one message to Centrifugo API, then another one \u2013 you can expect that messages will be delivered to a client in the same order. But as soon as you have an intermediary layer like Benthos or any other asynchronous worker process \u2013 then you must be careful about ordering. In case of Benthos and example here you can set `max_in_flight` parameter to `1` instead of `20` and keep only one instance of Benthos running to preserve ordering.\\n\\nIn case of streaming from Kafka you can rely on Kafka message partitioning feature to preserve message ordering.\\n\\n### Throughput when ordering preserved\\n\\nIf you preserved ordering in your asynchronous workers then the next thing to consider is throughput limitations.\\n\\nYou have a limited number of workers, these workers send requests to Centrifugo one by one. In this case throughput is limited by the number of workers and RTT (round-trip time) between worker process and Centrifugo.\\n\\nIf we talk about using Redis List structure as a queue - you can possibly shard data amongst different Redis List queues by some key to improve throughput. In this case you need to push messages where order should be preserved into a specific queue. In this case your get a setup similar to Kafka partitioning.\\n\\nIn case of using manually partitioned queues or using Kafka you can have parallelism equal to the number of partitions.\\n\\nLet\'s say you have 20 workers which can publish messages in parallel and 5ms RTT time between worker and Centrifugo. In this case you can publish 20*(1000/5) = 4000 messages per second max.\\n\\nTo improve throughput futher consider increasing worker number or batching publish requests to Centrifugo (using Centrifugo\'s batch API).\\n\\n### Error handling\\n\\nWhen publishing asynchronously you should also don\'t forget about error handling. Benthos will handle network errors automatically for you. But there could be internal errors from Centrifugo returned as part of response. It\'s not very convenient to handle with Benthos out of the box \u2013 so we think about [adding transport-level error mode](https://github.com/centrifugal/centrifugo/pull/690) to Centrifugo.\\n\\n## Conclusion\\n\\nSometimes you want to publish to Centrifugo asynchronously using messaging systems convenient for your company. Usually you can write worker process to re-publish messages to Centrifugo. Sometimes it may be simplified using helpful tools like Benthos.\\n\\nHere we\'ve shown how Benthos may be used to transfer messages from Redis List queue to Centrifugo API. With some modifications you can achieve the same for other input sources - such as Kafka, RabbitMQ, Nats Jetstream, etc.\\n\\nBut publishing messages asynchronously highlights several pifalls - like late delivery, ordering issues,  throughput considerations and error handling \u2013 which should be carefully addressed. Different real-time features may require different strategies."},{"id":"/2023/06/29/centrifugo-v5-released","metadata":{"permalink":"/blog/2023/06/29/centrifugo-v5-released","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2023-06-29-centrifugo-v5-released.md","source":"@site/blog/2023-06-29-centrifugo-v5-released.md","title":"Centrifugo v5 released","description":"We are excited to announce a new version of Centrifugo. It\'s an evolutionary step which makes Centrifugo cleaner and more intuitive to use.","date":"2023-06-29T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"release","permalink":"/blog/tags/release"}],"readingTime":12.635,"hasTruncateMarker":true,"authors":[{"name":"Centrifugal team","title":"\ud83d\udcbb\u2728\ud83d\udd2e\u2728\ud83d\udcbb","imageURL":"/img/logo_animated.svg"}],"frontMatter":{"title":"Centrifugo v5 released","tags":["centrifugo","release"],"description":"We are excited to announce a new version of Centrifugo. It\'s an evolutionary step which makes Centrifugo cleaner and more intuitive to use.","author":"Centrifugal team","authorTitle":"\ud83d\udcbb\u2728\ud83d\udd2e\u2728\ud83d\udcbb","authorImageURL":"/img/logo_animated.svg","image":"/img/v5_thumb.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Asynchronous message streaming to Centrifugo with Benthos","permalink":"/blog/2023/08/19/asynchronous-message-streaming-to-centrifugo-with-benthos"},"nextItem":{"title":"Setting up Keycloak SSO authentication flow and connecting to Centrifugo WebSocket","permalink":"/blog/2023/03/31/keycloak-sso-centrifugo"}},"content":"In Centrifugo v5 we\'re phasing out old client protocol support, introducing a more intuitive HTTP API, adjusting token management behaviour in SDKs, improving configuration process, and refactoring the history meta ttl option. As the result you get a cleaner, more user-friendly, and optimized Centrifugo experience. And we have important news about the project - check it out in the end of this post.\\n\\n\x3c!--truncate--\x3e\\n\\n<img src=\\"/img/v5.jpg\\" />\\n\\n:::info What is Centrifugo?\\n\\nCentrifugo is an open-source scalable real-time messaging server. Centrifugo can instantly deliver messages to application online users connected over supported transports (WebSocket, HTTP-streaming, SSE/EventSource, GRPC, SockJS, WebTransport). Centrifugo has the concept of a channel \u2013 so it\'s a user-facing PUB/SUB server.\\n\\nCentrifugo is language-agnostic and can be used to build chat apps, live comments, multiplayer games, real-time data visualizations, collaborative tools, etc. in combination with any backend. It is well suited for modern architectures and allows decoupling the business logic from the real-time transport layer.\\n\\nSeveral official client SDKs for browser and mobile development wrap the bidirectional protocol. In addition, Centrifugo supports a unidirectional approach for simple use cases with no SDK dependency.\\n\\n:::\\n\\nLet\'s proceed and take a look at most notable changes of Centrifugo v5.\\n\\n## Dropping old client protocol\\n\\nWith the introduction of Centrifugo v4, our previous major release, [we rolled out](/blog/2022/07/19/centrifugo-v4-released#unified-client-sdk-api) a new version of the client protocol along with a set of client SDKs designed to work in conjunction with it. Nevertheless, we maintained support for the old client protocol in Centrifugo v4 to facilitate a seamless migration of applications.\\n\\nIn Centrifugo v5 we are discontinuing support for the old protocol. If you have been using Centrifugo v4 with the latest SDKs, this change should have no impact on you. From our perspective, removing support for the old protocol allows us to eliminate a considerable amount of non-obvious branching in the source code and cleanup Protobuf schema definitions.\\n\\n## Token behaviour adjustments in SDKs\\n\\nIn Centrifugo v5 we are adjusting [client SDK specification](/docs/transports/client_api) in the aspect of connection token management. Previously, returning an empty token string from `getToken` callback resulted in client disconnection with `unauthorized` reason.\\n\\nThere was some problem with it though. We did not take into account the fact that empty token may be a valid scenario actually. Centrifugo supports options to avoid using token at all for anonymous access. So the lack of possibility to switch between `token`/`no token` scenarios did not allow users to easily implement login/logout workflow. The only way was re-initializing SDK.\\n\\nNow returning an empty string from `getToken` is a valid scenario which won\'t result into disconnect on the client side. It\'s still possible to disconnect client by returning a special error from `getToken` function.\\n\\nAnd we are putting back `setToken` method to our SDKs \u2013 so it\'s now possible to reset the token to be empty upon user logout.\\n\\nAn abstract example in Javascript which demonstrates how login/logout flow may be now implemented with our SDK:\\n\\n```javascript\\nconst centrifuge = new Centrifuge(\'ws://localhost:8000/connection/websocket\', {\\n    // Provide function which returns empty string for anonymous users,\\n    // and proper JWT for authenticated users.\\n    getToken: getTokenImplementation\\n});\\ncentrifuge.connect();\\n\\nloginButton.addEventListener(\'click\', function() {\\n    centrifuge.disconnect();\\n    // Do actual login here.\\n    centrifuge.connect();\\n});\\n\\nlogoutButton.addEventListener(\'click\', function() {\\n    centrifuge.disconnect();\\n    // Reset token - so that getToken will be called on next connect attempt.\\n    centrifuge.setToken(\\"\\");\\n    // Do actual logout here.\\n    centrifuge.connect();\\n});\\n```\\n\\nWe updated all our SDKs to inherit described behaviour - check out v5 [migration guide](/docs/getting-started/migration_v5) for more details.\\n\\n## history_meta_ttl refactoring\\n\\nOne of Centrifugo\'s key advantages for real-time messaging tasks is its ephemeral channels and per-channel history. In version 5, we\'ve improved one aspect of handling history by offering users the ability to tune the history meta TTL option at the channel namespace level.\\n\\nThe history meta TTL is the duration Centrifugo retains meta information about each channel stream, such as the latest offset and current stream epoch. This data allows users to successfully restore connections upon reconnection, particularly useful when subscribed to mostly inactive channels where publications are infrequent. Although the history meta TTL can usually be reasonably large (significantly larger than history TTL), there are certain scenarios where it\'s beneficial to minimize it as much as possible.\\n\\nOne such use case is illustrated in this [example](https://github.com/centrifugal/examples/tree/master/v4/go_async_processing). Using Centrifugo SDK and channels with history, it\'s possible to reliably stream results of asynchronous tasks to clients.\\n\\nAs another example, consider a ChatGPT use case where clients ask questions, subscribe to a channel with the answer, and then the response is streamed towards the client token by token. This all may be done over a secure, separate channel protected with a token. With the ability to use a relatively small history meta TTL in the channel namespace, implementing such things is now simpler.\\n\\nHence, `history_meta_ttl` is now an option at the channel namespace level (instead of per-engine). However, setting it is optional as we have a global default value for it - see [details in the doc](/docs/server/channels#history_meta_ttl).\\n\\n## Node communication protocol update\\n\\nWhen running in cluster Centrifugo nodes can communicate between each other using broker\'s PUB/SUB. Nodes exchange some information - like statistics, emulation requests, etc.\\n\\nIn Centrifugo v5 we are simplifying and making inter-node communication protocol slightly faster by removing extra encoding layers from it\'s format. Something similar to what we did for our client protocol in Centrifugo v4.\\n\\nThis change, however, leads to an incompatibility between Centrifugo v4 and v5 nodes in terms of their communication protocols. Consequently, Centrifugo v5 cannot be part of a cluster with Centrifugo v4 nodes.\\n\\n## New HTTP API format\\n\\nFrom the beginning Centrifugo HTTP API exposed one `/api` endpoint to make requests with all command types.\\n\\nTo work properly HTTP API had to add one additional layer to request JSON payload to be able to distinguish between different API methods:\\n\\n```bash\\ncurl --header \\"Content-Type: application/json\\" \\\\\\n  --header \\"Authorization: apikey API_KEY\\" \\\\\\n  --request POST \\\\\\n  --data \'{\\"method\\": \\"publish\\", \\"params\\": {\\"channel\\": \\"test\\", \\"data\\": {\\"x\\": 1}}}\' \\\\\\n  http://localhost:8000/api\\n```\\n\\nAnd it worked fine. It additionally supported request batching where users could send many commands to Centrifugo in one request using line-delimited JSON.\\n\\nHowever, the fact that we were accommodating various commands via a single API endpoint resulted in nested serialized payloads for each command. The top-level method would determine the structure of the params. We addressed this issue in the client protocol in Centrifugo v4, and now we\'re addressing a similar issue in the inter-node communication protocol in Centrifugo v5.\\n\\nAt some point we introduced GRPC API in Centrifugo. In GRPC case we don\'t have a way to send batches of commands without defining a separate method to do so.\\n\\nThese developments highlighted the need for us to align the HTTP API format more closely with the GRPC API. Specifically, we need to separate the command method from the actual method payload, moving towards a structure like this:\\n\\n```bash\\ncurl --header \\"Content-Type: application/json\\" \\\\\\n  --header \\"X-API-Key: API_KEY\\" \\\\\\n  --request POST \\\\\\n  --data \'{\\"channel\\": \\"test\\", \\"data\\": {\\"x\\": 1}}\' \\\\\\n  http://localhost:8000/api/publish\\n```\\n\\nNote:\\n\\n* `/api/publish` instead of `/api` in path\\n* payload does not include `method` and `params` keys anymore\\n* we also support `X-API-Key` header for setting API key to be closer to OpenAPI specification (see more about OpenAPI below)\\n\\nIn v5 we implemented the approach above. Many thanks to [@StringNick](https://github.com/StringNick) for the help with the implementation and discussions.\\n\\nOur HTTP and GRPC API are very similar now. We\'ve also introduced a new batch method to send multiple commands in both HTTP and GRPC APIs, a feature that was previously unavailable in GRPC.\\n\\nDocumentation for v5 was updated to reflect this change. But it worth noting - old API format id still supported. It will be supported for some time while we are migrating our HTTP API libraries to use modern API version. Hopefully users won\'t be affected by this migration a lot, just switching to a new version of library at some point.\\n\\n## OpenAPI spec and Swagger UI\\n\\nOne additional benefit of moving to the new HTTP format is the possibility to define a clear OpenAPI schema for each API method Centrifugo has. It was previously quite tricky due to the fact we had one endpoint capable to work with all kinds of commands.\\n\\nThis change paves the way for generating HTTP clients based on our OpenAPI specification.\\n\\nWe now have Swagger UI built-in. To access it, launch Centrifugo with the `\\"swagger\\": true` option and navigate to `http://localhost:8000/swagger`.\\n\\nThe Swagger UI operates on the internal port, so if you\'re running Centrifugo using our Kubernetes Helm chart, it won\'t be exposed to the same ingress as client connection endpoints. This is similar to how our Prometheus, admin, API, and debug endpoints currently work.\\n\\n## OpenTelemetry for server API\\n\\nAnother good addition is an OpenTelemetry tracing support for HTTP and GRPC server API requests. If you are using OpenTelemetry in your services you can now now enable tracing export in Centrifugo and find Centrifugo API request exported traces in your tracing collector UI.\\n\\nDescription and simple example with Jaeger may be found [in observability chapter](/docs/server/observability#opentelemetry). We only support OTLP HTTP export format and trace format defined in W3C spec: https://www.w3.org/TR/trace-context/.\\n\\n## Separate config for subscription token\\n\\nWith the introduction of JWKS support in Centrifugo v4 (a way to validate JWT tokens using a remote endpoint which manages keys and key rotation - see [JWK spec](https://datatracker.ietf.org/doc/html/rfc7517)) Centrifugo users can rely on JWKS provider (like Keycloak, AWS Cognito) for making authentication.\\n\\nBut at the same time developers may want to work with channels using subscription tokens managed in a custom way \u2013 without using the same JWKS configuration used for connection tokens.\\n\\nCentrifugo v5 allows doing by introducing the `separate_subscription_token_config` option.\\n\\nWhen `separate_subscription_token_config` is `true` Centrifugo does not look at general token options at all when verifying subscription tokens and uses config options starting from `subscription_token_` prefix instead. \\n\\nHere is an example how to use JWKS for connection tokens, but have HMAC-based verification for subscription tokens:\\n\\n```json title=\\"config.json\\"\\n{\\n  \\"token_jwks_public_endpoint\\": \\"https://example.com/openid-connect/certs\\",\\n  \\"separate_subscription_token_config\\": true,\\n  \\"subscription_token_hmac_secret_key\\": \\"separate_secret_which_must_be_strong\\"\\n}\\n```\\n\\n## Unknown config keys warnings\\n\\nWith every release, Centrifugo offers more and more options. One thing we\'ve noticed is that some options from previous Centrifugo options, which were already removed, still persist in the user\'s configuration file.\\n\\nAnother issue is that a single typo in the configuration key can cost hours of debugging especially for Centrifugo new users. What is worse, the typo might result in unexpected behavior if the feature isn\'t properly tested before being run in production.\\n\\nIn Centrifugo v5, we are addressing these problems. Now, Centrifugo logs on WARN level about unknown keys found in the configuration upon server start-up. Not only in the configuration file but also verifying the validity of environment variables (looking at those starting with `CENTRIFUGO_` prefix). This should help clean up the configuration to align with the latest Centrifugo release and catch typos at an early stage.\\n\\nIt looks like this:\\n\\n```\\n08:25:33 [WRN] unknown key found in the namespace object key=watch namespace_name=xx\\n08:25:33 [WRN] unknown key found in the proxy object key=type proxy_name=connect\\n08:25:33 [WRN] unknown key found in the configuration file key=granulr_proxy_mode\\n08:25:33 [WRN] unknown key found in the environment key=CENTRIFUGO_ADDRES\\n```\\n\\nThese warnings do not prevent server to start so you can gradually clean up the configuration.\\n\\n## Simplifying protocol debug with Postman\\n\\nCentrifugo v5 supports a special url parameter for bidirectional websocket which turns on using native WebSocket frame ping-pong mechanism instead of server-to-client application level pings Centrifugo uses by default. This simplifies debugging Centrifugo protocol with tools like Postman, wscat, websocat, etc. \\n\\nPreviously it was inconvenient due to the fact Centrifugo sends periodic ping message to the client (`{}` in JSON protocol scenario) and expects pong response back within some time period. Otherwise Centrifugo closes connection. This results in problems with mentioned tools because you had to manually send `{}` pong message upon ping message. So typical session in `wscat` could look like this:\\n\\n```bash\\n\u276f wscat --connect ws://localhost:8000/connection/websocket\\nConnected (press CTRL+C to quit)\\n> {\\"id\\": 1, \\"connect\\": {}}\\n< {\\"id\\":1,\\"connect\\":{\\"client\\":\\"9ac9de4e-5289-4ad6-9aa7-8447f007083e\\",\\"version\\":\\"0.0.0\\",\\"ping\\":25,\\"pong\\":true}}\\n< {}\\nDisconnected (code: 3012, reason: \\"no pong\\")\\n```\\n\\nThe parameter is called `cf_ws_frame_ping_pong`, to use it connect to Centrifugo bidirectional WebSocket endpoint like `ws://localhost:8000/connection/websocket?cf_ws_frame_ping_pong=true`. Here is an example which demonstrates working with Postman WebSocket where we connect to local Centrifugo and subscribe to two channels `test1` and `test2`:\\n\\n<video width=\\"100%\\" loop={true} autoPlay=\\"autoplay\\" muted controls=\\"\\" src=\\"/img/postman.mp4\\"></video>\\n\\nYou can then proceed to Centrifugo [admin web UI](/docs/server/admin_web), publish something to these channels and see publications in Postman.\\n\\nNote, how we sent several JSON commands in one WebSocket frame to Centrifugo from Postman in the example above - this is possible since Centrifugo protocol supports batches of commands in line-delimited format.\\n\\nWe consider this feature to be used only for debugging, **in production prefer using our SDKs without using `cf_ws_frame_ping_pong` parameter** \u2013 because app-level ping-pong is more efficient and our SDKs detect broken connections due to it.\\n\\n## The future of SockJS\\n\\nAs you know SockJS is deprecated in Centrifugal ecosystem since Centrifugo v4. In this release we are not removing support for it \u2013 but we may do this in the next release.\\n\\nUnfortunately, SockJS client repo is poorly maintained these days. And some of its iframe-based transports are becoming archaic. If you depend on SockJS and you really need fallback for WebSocket \u2013 consider switching to Centrifugo own bidirectional emulation for the browser which works over HTTP-streaming (using modern fetch API with Readable streams) or SSE. It should be more performant and work without sticky sessions requirement (sticky sessions is an optimization in our implementation). More details may be found in [Centrifugo v4 release post](/blog/2022/07/19/centrifugo-v4-released#modern-websocket-emulation-in-javascript).\\n\\nIf you think SockJS is still required for your use case - reach us out so we could think about the future steps together.\\n\\n## Introducing Centrifugal Labs LTD\\n\\nFinally, some important news we mentioned in the beginning!\\n\\nCentrifugo is now backed by the company called **Centrifugal Labs LTD** - a Cyprus-registered technology company. This should help us to finally launch [Centrifugo PRO](/docs/pro/overview) offering \u2013 the product we have been working on for a couple of years now and which has some unique and powerful features like [real-time analytics](/docs/pro/analytics) or [push notification API](/docs/pro/push_notifications).\\n\\nAs a Centrifugo user you will start noticing mentions of Centrifugal Labs LTD in our licenses, Github organization, throughout this web site. And that\'s mostly it - no radical changes at this point. We will still be working on improving Centrifugo trying to find a balance between OSS and PRO versions. Which is difficult TBH \u2013 but we will try.\\n\\nAn ideal plan for us \u2013 make Centrifugo development sustainable enough to have the possibility for features from the PRO version flow to the OSS version eventually. The reality may be harder than this of course.\\n\\n## Conclusion\\n\\nThat\'s all about most notable things happened in Centrifugo v5. We updated documentation to reflect the changes in v5, also some documentation chapters were rewritten. For example, take a look at the refreshed [Design overview](/docs/getting-started/design). Several more changes and details outlined in the [migration guide for Centifugo v5](/docs/getting-started/migration_v5). Please feel free to contact in the community rooms if you have questions about the release. And as usual, let the Centrifugal force be with you!"},{"id":"/2023/03/31/keycloak-sso-centrifugo","metadata":{"permalink":"/blog/2023/03/31/keycloak-sso-centrifugo","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2023-03-31-keycloak-sso-centrifugo.md","source":"@site/blog/2023-03-31-keycloak-sso-centrifugo.md","title":"Setting up Keycloak SSO authentication flow and connecting to Centrifugo WebSocket","description":"This tutorial shows how to connect to Centrifugo when using Keycloak SSO flow for user authentication. Here we build a simple demo app using React and Vite.","date":"2023-03-31T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"keycloak","permalink":"/blog/tags/keycloak"},{"label":"sso","permalink":"/blog/tags/sso"},{"label":"authentication","permalink":"/blog/tags/authentication"}],"readingTime":4.98,"hasTruncateMarker":true,"authors":[{"name":"Alexander Emelin","title":"Author of Centrifugo","imageURL":"https://github.com/FZambia.png"}],"frontMatter":{"title":"Setting up Keycloak SSO authentication flow and connecting to Centrifugo WebSocket","tags":["centrifugo","keycloak","sso","authentication"],"description":"This tutorial shows how to connect to Centrifugo when using Keycloak SSO flow for user authentication. Here we build a simple demo app using React and Vite.","author":"Alexander Emelin","authorTitle":"Author of Centrifugo","authorImageURL":"https://github.com/FZambia.png","image":"/img/keycloak_sso_cover.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Centrifugo v5 released","permalink":"/blog/2023/06/29/centrifugo-v5-released"},"nextItem":{"title":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","permalink":"/blog/2022/12/20/improving-redis-engine-performance"}},"content":"![](/img/keycloak_sso_cover.jpg)\\n\\nSecuring user authentication and management can often be a challenging task when developing a modern application. As a result, many developers choose to delegate this responsibility to third-party identity providers, such as Okta, Auth0, or Keycloak.\\n\\nIn this blog post, we\'ll go through the process of setting up Single Sign-On (SSO) authentication using Keycloak - popular and powerful identity provider. After setting up SSO we will create React application and connect to Centrifugo using access token generated by Keycloak for our test user:\\n\\n<video width=\\"100%\\" loop={true} autoPlay=\\"autoplay\\" muted controls=\\"\\" src=\\"/img/keycloak.mp4\\"></video>\\n\\n\x3c!--truncate--\x3e\\n\\n## TLDR\\n\\nThe integraion is possible since Centrifugo works with [standard JWT for authentication](/docs/server/authentication) and additionally [supports JSON Web Key](https://centrifugal.dev/docs/server/authentication#json-web-key-support) specification. \\n\\nHere is a final [source code](https://github.com/centrifugal/examples/tree/master/v4/keycloak_sso_auth).\\n\\n## Keycloak\\n\\nFirst, run Keycloak using the following Docker command:\\n\\n```bash\\ndocker run --rm -it -p 8080:8080 \\\\\\n    -e KEYCLOAK_ADMIN=admin \\\\\\n    -e KEYCLOAK_ADMIN_PASSWORD=admin \\\\\\n    quay.io/keycloak/keycloak:21.0.1 start-dev\\n```\\n\\nAfter starting Keycloak, go to [http://localhost:8080/admin](http://localhost:8080/admin) and login. Then perform the following tasks:\\n\\n1. Create a new realm named `myrealm`.\\n1. Create a new client named `myclient`. Set valid redirect URIs to `http://localhost:5173/*`, and web origins as `http://localhost:5173`.\\n1. Create a user named `myuser` and set a password for it (in Credentials tab).\\n\\nSee [this guide](https://www.keycloak.org/getting-started/getting-started-docker) for additional details and illustrations of the process.\\n\\nMake sure your created client is `public` (this is default) since we will request token directly from the web application.\\n\\n## Centrifugo\\n\\nNext, run Centrifugo using the following Docker command:\\n\\n```bash\\ndocker run --rm -it -p 8000:8000 \\\\\\n    -e CENTRIFUGO_ALLOWED_ORIGINS=\\"http://localhost:5173\\" \\\\\\n    -e CENTRIFUGO_TOKEN_JWKS_PUBLIC_ENDPOINT=\\"http://host.docker.internal:8080/realms/myrealm/protocol/openid-connect/certs\\" \\\\\\n    -e CENTRIFUGO_ALLOW_USER_LIMITED_CHANNELS=true \\\\\\n    -e CENTRIFUGO_ADMIN=true \\\\\\n    -e CENTRIFUGO_ADMIN_SECRET=secret \\\\\\n    -e CENTRIFUGO_ADMIN_PASSWORD=admin \\\\\\n    centrifugo/centrifugo:v4.1.2 centrifugo\\n```\\n\\nSome comments about environment variables used here:\\n\\n* CENTRIFUGO_TOKEN_JWKS_PUBLIC_ENDPOINT allows tell Centrifugo to use JSON Web Key spec when validating tokens, we point to Keycloak\'s JWKS endpoint\\n* CENTRIFUGO_ALLOWED_ORIGINS is required since we will build Vite + React based app running on http://localhost:5173\\n* CENTRIFUGO_ALLOW_USER_LIMITED_CHANNELS - not required to connect, but you will see in the source code that we additionally subscribe to a user personal channel\\n* CENTRIFUGO_ADMIN, CENTRIFUGO_ADMIN_SECRET, CENTRIFUGO_ADMIN_PASSWORD - to enable Centrifugo admin web UI\\n\\nAlso note we are using `host.docker.internal` to access host port from inside the Docker network.\\n\\n## React app with Vite\\n\\nNow, let\'s create a new React app using very popular [Vite](https://vitejs.dev/) tool:\\n\\n```bash\\nnpm create vite@latest keycloak_sso_auth -- --template react\\ncd keycloak_sso_auth\\nnpm install\\n```\\n\\nAlso, install the necessary additional packages for the React app:\\n\\n```bash\\nnpm install --save @react-keycloak/web centrifuge keycloak-js\\n```\\n\\nAnd start the development server:\\n\\n```bash\\nnpm run dev\\n```\\n\\nNavigate to [http://localhost:5173/](http://localhost:5173/). You should see default Vite template working, we are going to modify it a bit.\\n\\n:::caution\\n\\nUse `localhost`, not `127.0.0.1` - since we used `localhost` for Keyloak and Centrifugo configurations above.\\n\\n:::\\n\\nAdd the following into `main.jsx`:\\n\\n```javascript\\nimport React from \'react\'\\nimport ReactDOM from \'react-dom/client\'\\nimport { ReactKeycloakProvider } from \'@react-keycloak/web\'\\nimport App from \'./App\'\\nimport \'./index.css\'\\n\\nimport Keycloak from \\"keycloak-js\\";\\n\\nconst keycloakClient = new Keycloak({\\n  url: \\"http://localhost:8080\\",\\n  realm: \\"myrealm\\",\\n  clientId: \\"myclient\\"\\n})\\n\\nReactDOM.createRoot(document.getElementById(\'root\')).render(\\n  <ReactKeycloakProvider authClient={keycloakClient}>\\n    <React.StrictMode>\\n      <App />\\n    </React.StrictMode>\\n  </ReactKeycloakProvider>,\\n)\\n```\\n\\nNote that we configured `Keycloak` instance pointing it to our Keycloak server. We also use `@react-keycloak/web` package to wrap React app into `ReactKeycloakProvider` component. It simplifies working with Keycloak by providing some useful hooks - we are using this hook below.\\n\\nOur `App` component inside `App.jsx` may look like this:\\n\\n```javascript\\nimport React, { useState, useEffect } from \'react\';\\nimport logo from \'./assets/centrifugo.svg\'\\nimport { Centrifuge } from \\"centrifuge\\";\\nimport { useKeycloak } from \'@react-keycloak/web\'\\nimport \'./App.css\'\\n\\nfunction App() {\\n  const { keycloak, initialized } = useKeycloak()\\n\\n  if (!initialized) {\\n    return null;\\n  }\\n\\n  return (\\n    <div>\\n      <header>\\n        <p>\\n          SSO with Keycloak and Centrifugo\\n        </p>\\n        {keycloak.authenticated ? (\\n          <div>\\n            <p>Logged in as {keycloak.tokenParsed?.preferred_username}</p>\\n            <button type=\\"button\\" onClick={() => keycloak.logout()}>\\n              Logout\\n            </button>\\n          </div>\\n        ) : (\\n          <button type=\\"button\\" onClick={() => keycloak.login()}>\\n            Login\\n          </button>\\n        )}\\n      </header>\\n    </div >\\n  );\\n}\\n\\nexport default App\\n```\\n\\nThis is actually enough for SSO flow to start working! You can click on login button and make sure that it\'s possible to use `myuser` credentials to log into the application. And log out after that.\\n\\nThe only missing part is Centrifugo. We can initialize connection inside `useEffect` hook of `App` component:\\n\\n```javascript\\nuseEffect(() => {\\n  if (!initialized || !keycloak.authenticated) {\\n    return;\\n  }\\n  const centrifuge = new Centrifuge(\\"ws://localhost:8000/connection/websocket\\", {\\n    token: keycloak.token,\\n    getToken: function () {\\n      return new Promise((resolve, reject) => {\\n        keycloak.updateToken(5).then(function () {\\n          resolve(keycloak.token);\\n        }).catch(function (err) {\\n          reject(err);\\n          keycloak.logout();\\n        });\\n      })\\n    }\\n  });\\n\\n  centrifuge.connect();\\n\\n  return () => {\\n    centrifuge.disconnect();\\n  };\\n}, [keycloak, initialized]);\\n```\\n\\nThe important thing here is how we configure tokens: we are using Keycloak client methods to set initial token and refresh the token when required.\\n\\nI also added some extra elements to the code to make it look a bit nicer. For example, we can listen to Centriffuge client state changes and show connection indicator on the page:\\n\\n```javascript\\nfunction App() {\\n  const [connectionState, setConnectionState] = useState(\\"disconnected\\");\\n  const stateToEmoji = {\\n    \\"disconnected\\": \\"\ud83d\udd34\\",\\n    \\"connecting\\": \\"\ud83d\udfe0\\",\\n    \\"connected\\": \\"\ud83d\udfe2\\"\\n  }\\n  ...\\n\\n  useEffect(() => {\\n    ...\\n    centrifuge.on(\'state\', function (ctx) {\\n      setConnectionState(ctx.newState);\\n    })\\n    ...\\n\\n  return (\\n    ...\\n    <span className={\\"connectionState \\" + connectionState}>\\n      {stateToEmoji[connectionState]}\\n    </span>\\n```\\n\\nYou can find more details about Centrifugo client SDK API and states in [client SDK spec](/docs/transports/client_api).\\n\\nIf you look at source code on Github - you will also find an example of channel subscription to a user personal channel:\\n\\n```javascript\\nfunction App() {\\n  ...\\n  const [publishedData, setPublishedData] = useState(\\"\\");\\n  ...\\n\\n  useEffect(() => {\\n    ...\\n    const userChannel = \\"#\\" + keycloak.tokenParsed?.sub;\\n    const sub = centrifuge.newSubscription(userChannel);\\n    sub.on(\\"publication\\", function (ctx) {\\n      setPublishedData(JSON.stringify(ctx.data));\\n    }).subscribe();\\n    ...\\n```\\n\\nYou can now:\\n\\n* test the SSO setup by logging into application\\n* making sure connection is successful\\n* try publishing a message into a user channel via the [Centrifugo Web UI](http://localhost:8000/#/actions). The published message will appear on application screen in real-time.\\n\\n<video width=\\"100%\\" loop={true} autoPlay=\\"autoplay\\" muted controls=\\"\\" src=\\"/img/keycloak_publish.mp4\\"></video>\\n\\nThat\'s it! We have successfully set up Keycloak SSO authentication with Centrifugo and a React application. Again, [source code](https://github.com/centrifugal/examples/tree/master/v4/keycloak_sso_auth) is on Github."},{"id":"/2022/12/20/improving-redis-engine-performance","metadata":{"permalink":"/blog/2022/12/20/improving-redis-engine-performance","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2022-12-20-improving-redis-engine-performance.md","source":"@site/blog/2022-12-20-improving-redis-engine-performance.md","title":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","description":"In this post we share some details about Centrifugo Redis Engine implementation and its recent performance improvements with the help of Rueidis Go library","date":"2022-12-20T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"redis","permalink":"/blog/tags/redis"},{"label":"go","permalink":"/blog/tags/go"}],"readingTime":28.995,"hasTruncateMarker":true,"authors":[{"name":"Alexander Emelin","title":"Author of Centrifugo","imageURL":"https://github.com/FZambia.png"}],"frontMatter":{"title":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","tags":["centrifugo","redis","go"],"description":"In this post we share some details about Centrifugo Redis Engine implementation and its recent performance improvements with the help of Rueidis Go library","author":"Alexander Emelin","authorTitle":"Author of Centrifugo","authorImageURL":"https://github.com/FZambia.png","image":"/img/redis_cover.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Setting up Keycloak SSO authentication flow and connecting to Centrifugo WebSocket","permalink":"/blog/2023/03/31/keycloak-sso-centrifugo"},"nextItem":{"title":"101 ways to subscribe user on a personal channel in Centrifugo","permalink":"/blog/2022/07/29/101-way-to-subscribe"}},"content":"![Centrifugo_Redis_Engine_Improvements](/img/redis_cover.png)\\n\\nThe main objective of Centrifugo is to manage persistent client connections established over various real-time transports (including WebSocket, HTTP-Streaming, SSE, WebTransport, etc \u2013 see [here](https://centrifugal.dev/docs/transports/overview)) and offer an API for publishing data towards established connections. Clients subscribe to channels, hence Centrifugo implements PUB/SUB mechanics to transmit published data to all online channel subscribers.\\n\\nCentrifugo employs [Redis](https://redis.com/) as its primary scalability option \u2013 so that it\'s possible to distribute client connections amongst numerous Centrifugo nodes without worrying about channel subscribers connected to separate nodes. Redis is incredibly mature, simple, and fast in-memory storage. Due to various built-in data structures and PUB/SUB support Redis is a perfect fit to be both Centrifugo `Broker` and `PresenceManager` (we will describe what\'s this shortly).\\n\\nIn Centrifugo v4.1.0 we introduced an updated implementation of our Redis Engine (`Engine` in Centrifugo == `Broker` + `PresenceManager`) which provides sufficient performance improvements to our users. This post discusses the factors that prompted us to update Redis Engine implementation and provides some insight into the results we managed to achieve. We\'ll examine a few well-known Go libraries for Redis communication and contrast them against Centrifugo tasks.\\n\\n\x3c!--truncate--\x3e\\n\\n## Broker and PresenceManager\\n\\nBefore we get started, let\'s define what Centrifugo\'s `Broker` and `PresenceManager` terms mean.\\n\\n[Broker](https://github.com/centrifugal/centrifuge/blob/f6e948a15fd49000627377df2a7c94cadda1daf8/broker.go#L97) is an interface responsible for maintaining subscriptions from different Centrifugo nodes (initiated by client connections). That helps to scale client connections over many Centrifugo instances and not worry about the same channel subscribers being connected to different nodes \u2013 since all Centrifugo nodes connected with PUB/SUB. Messages published to one node are delivered to a channel subscriber connected to another node.\\n\\nAnother major part of `Broker` is keeping an expiring publication history for channels (streams). So that Centrifugo may provide a fast cache for messages missed by clients upon going offline for a short period and compensate at most once delivery of Redis PUB/SUB using [Publication](https://github.com/centrifugal/centrifuge/blob/f6e948a15fd49000627377df2a7c94cadda1daf8/broker.go#L9) incremental offsets. Centrifugo uses STREAM and HASH data structures in Redis to store channel history and stream meta information.\\n\\nIn general Centrifugo architecture may be perfectly illustrated by this picture (Gophers are Centrifugo nodes all connected to `Broker`, and sockets are WebSockets):\\n\\n![gopher-broker](https://i.imgur.com/QOJ1M9a.png)\\n\\n[PresenceManager](https://github.com/centrifugal/centrifuge/blob/f6e948a15fd49000627377df2a7c94cadda1daf8/presence.go#L12) is an interface responsible for managing online presence information - list of currently active channel subscribers. While the connection is alive we periodically update presence entries for channels connection subscribed to (for channels where presence is enabled). Presence data should expire if not updated by a client connection for some time. Centrifugo uses two Redis data structures for managing presence in channels - HASH and ZSET.\\n\\n## Redigo\\n\\nFor a long time, the [gomodule/redigo](https://github.com/gomodule/redigo) package served as the foundation for the Redis Engine implementation in Centrifugo. Huge props go to [Mr Gary Burd](https://github.com/garyburd) for creating it.\\n\\nRedigo offers a connection [Pool](https://pkg.go.dev/github.com/gomodule/redigo/redis#Pool) to Redis. A simple usage of it involves getting the connection from the pool, issuing request to Redis over that connection, and then putting the connection back to the pool after receiving the result from Redis.\\n\\nLet\'s write a simple benchmark which demonstrates simple usage of Redigo and measures SET operation performance:\\n\\n```go\\nfunc BenchmarkRedigo(b *testing.B) {\\n\\tpool := redigo.Pool{\\n\\t\\tMaxIdle:   128,\\n\\t\\tMaxActive: 128,\\n\\t\\tWait:      true,\\n\\t\\tDial: func() (redigo.Conn, error) {\\n\\t\\t\\treturn redigo.Dial(\\"tcp\\", \\":6379\\")\\n\\t\\t},\\n\\t}\\n\\tdefer pool.Close()\\n\\n\\tb.ResetTimer()\\n\\tb.SetParallelism(128)\\n\\tb.ReportAllocs()\\n\\tb.RunParallel(func(pb *testing.PB) {\\n\\t\\tfor pb.Next() {\\n\\t\\t\\tc := pool.Get()\\n\\t\\t\\t_, err := c.Do(\\"SET\\", \\"redigo\\", \\"test\\")\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\tb.Fatal(err)\\n\\t\\t\\t}\\n\\t\\t\\tc.Close()\\n\\t\\t}\\n\\t})\\n}\\n```\\n\\nLet\'s run it:\\n\\n```\\nBenchmarkRedigo-8        228804        4648 ns/op        62 B/op         2 allocs/op\\n```\\n\\nSeems pretty fast, but we can improve it further.\\n\\n## Redigo with pipelining\\n\\nTo increase a throughput in Centrifugo, instead of using Redigo\'s `Pool` for each operation, we acquired a dedicated connection from the `Pool` and utilized [Redis pipelining](https://redis.io/docs/manual/pipelining/) to send multiple commands where possible.\\n\\nRedis pipelining improves performance by executing multiple commands using a single client-server-client round trip. Instead of executing many commands one by one, you can queue the commands in a pipeline and then execute the queued commands as if it is a single command. Redis processes commands in order and sends individual response for each command. Given a single CPU nature of Redis, reducing the number of active connections when using pipelining has a positive impact on throughput \u2013 therefore pipelining is beneficial from this angle as well.\\n\\n![Redis pipeline](/img/redis_pipeline.png)\\n\\nYou can quickly estimate the benefits of pipelining by running Redis locally and running `redis-benchmark` which comes with Redis distribution over it:\\n\\n```bash\\n> redis-benchmark -n 100000 set key value\\n\\nSummary:\\n  throughput summary: 84674.01 requests per second\\n```\\n\\nAnd with pipelining:\\n\\n```bash\\n> redis-benchmark -n 100000 -P 64 set key value\\n\\nSummary:\\n  throughput summary: 666880.00 requests per second\\n```\\n\\nIn Centrifugo we are using smart batching technique for collecting pipeline (also described in [one of the previous posts](/blog/2020/11/12/scaling-websocket) in this blog).\\n\\nTo demonstrate benefits from using pipelining let\'s look at the following benchmark:\\n\\n```go\\nconst (\\n\\tmaxCommandsInPipeline = 512\\n\\tnumPipelineWorkers    = 1\\n)\\n\\ntype command struct {\\n\\terrCh chan error\\n}\\n\\ntype sender struct {\\n\\tcmdCh chan command\\n\\tpool  redigo.Pool\\n}\\n\\nfunc newSender(pool redigo.Pool) *sender {\\n\\tp := &sender{\\n\\t\\tcmdCh: make(chan command),\\n\\t\\tpool:  pool,\\n\\t}\\n\\tgo func() {\\n\\t\\tfor {\\n\\t\\t\\tfor i := 0; i < numPipelineWorkers; i++ {\\n\\t\\t\\t\\tp.runPipelineRoutine()\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}()\\n\\treturn p\\n}\\n\\nfunc (s *sender) send() error {\\n\\terrCh := make(chan error, 1)\\n\\tcmd := command{\\n\\t\\terrCh: errCh,\\n\\t}\\n\\t// Submit command to be executed by runPipelineRoutine.\\n\\ts.cmdCh <- cmd\\n\\treturn <-errCh\\n}\\n\\nfunc (s *sender) runPipelineRoutine() {\\n\\tconn := p.pool.Get()\\n\\tdefer conn.Close()\\n\\tfor {\\n\\t\\tselect {\\n\\t\\tcase cmd := <-s.cmdCh:\\n\\t\\t\\tcommands := []command{cmd}\\n\\t\\t\\tconn.Send(\\"set\\", \\"redigo\\", \\"test\\")\\n\\t\\tloop:\\n\\t\\t\\t// Collect batch of commands to send to Redis in one RTT.\\n\\t\\t\\tfor i := 0; i < maxCommandsInPipeline; i++ {\\n\\t\\t\\t\\tselect {\\n\\t\\t\\t\\tcase cmd := <-s.cmdCh:\\n\\t\\t\\t\\t\\tcommands = append(commands, cmd)\\n\\t\\t\\t\\t\\tconn.Send(\\"set\\", \\"redigo\\", \\"test\\")\\n\\t\\t\\t\\tdefault:\\n\\t\\t\\t\\t\\tbreak loop\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\t// Flush all collected commands to the network.\\n\\t\\t\\terr := conn.Flush()\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\tfor i := 0; i < len(commands); i++ {\\n\\t\\t\\t\\t\\tcommands[i].errCh <- err\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\t}\\n\\t\\t\\t// Read responses to commands, they come in order.\\n\\t\\t\\tfor i := 0; i < len(commands); i++ {\\n\\t\\t\\t\\t_, err := conn.Receive()\\n\\t\\t\\t\\tcommands[i].errCh <- err\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\\n\\nfunc BenchmarkRedigoPipelininig(b *testing.B) {\\n\\tpool := redigo.Pool{\\n\\t\\tWait: true,\\n\\t\\tDial: func() (redigo.Conn, error) {\\n\\t\\t\\treturn redigo.Dial(\\"tcp\\", \\":6379\\")\\n\\t\\t},\\n\\t}\\n\\tdefer pool.Close()\\n\\n\\tsender := newSender(pool)\\n\\n\\tb.ResetTimer()\\n\\tb.SetParallelism(128)\\n\\tb.ReportAllocs()\\n\\tb.RunParallel(func(pb *testing.PB) {\\n\\t\\tfor pb.Next() {\\n\\t\\t\\terr := sender.send()\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\tb.Fatal(err)\\n\\t\\t\\t}\\n\\t\\t}\\n\\t})\\n}\\n```\\n\\nThis is a strategy that we employed in Centrifugo for a long time. As you can see code with automatic pipelining gets more complex, and in real life it\'s even more complicated to support different types of commands, channel send timeouts, and server shutdowns.\\n\\nWhat about the performance of this approach?\\n\\n```\\nBenchmarkRedigo-8               228804      4648 ns/op       62 B/op     2 allocs/op\\nBenchmarkRedigoPipelininig-8   1840758     604.7 ns/op      176 B/op     4 allocs/op\\n```\\n\\nOperation latency reduced from 4648 ns/op to 604.7 ns/op \u2013 not bad right?\\n\\nIt\'s worth mentioning that upon increased RTT between application and Redis the approach with pipelining will provide worse throughput. But it still can be better than in pool-based approach. Let\'s say we have latency 5ms between app and Redis. This means that with pool size of 128 you will be able to issue up to `128 * (1000 / 5) = 25600` requests per second over 128 connections. With the pipelining approach above the theoretical limit is `512 * (1000 / 5) = 102400` requests per second over a single connection (though in case of using code for pipelining shown above we need to have larger parallelism, say 512 instead of 128). And it can scale further if you increase `numPipelineWorkers` to work over several connections in paralell. Though increasing `numPipelineWorkers` has negative effect on CPU \u2013 we will discuss this later in this post.\\n\\nRedigo is an awesome battle-tested library that served us great for a long time.\\n\\n## Motivation to migrate\\n\\nThere are three modes in which Centrifugo can work with Redis these days:\\n\\n1. Connecting to a standalone single Redis instance\\n2. Connecting to Redis in master-replica configuration, where Redis Sentinel controls the failover process\\n3. Connecting to Redis Cluster\\n\\nAll modes additionally can be used with client-side consistent sharding. So it\'s possible to scale Redis even without a Redis Cluster setup.\\n\\nUnfortunately, with pure Redigo library, it\'s only possible to implement [ 1 ] \u2013 i.e. connecting to a single standalone Redis instance.\\n\\nTo support the scheme with Sentinel you whether need to have a proxy between the application and Redis which proxies the connection to Redis master. For example, with Haproxy it\'s possible in this way:\\n\\n```\\nlisten redis\\n    server redis-01 127.0.0.1:6380 check port 6380 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 on-marked-down shutdown-sessions on-marked-up shutdown-backup-sessions\\n    server redis-02 127.0.0.1:6381 check port 6381 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 backup\\n    bind *:6379\\n    mode tcp\\n    option tcpka\\n    option tcplog\\n    option tcp-check\\n    tcp-check send PING\\\\r\\\\n\\n    tcp-check expect string +PONG\\n    tcp-check send info\\\\ replication\\\\r\\\\n\\n    tcp-check expect string role:master\\n    tcp-check send QUIT\\\\r\\\\n\\n    tcp-check expect string +OK\\n    balance roundrobin\\n```\\n\\nOr, you need to additionally import [FZambia/sentinel](https://github.com/FZambia/sentinel) library - which provides a communication layer with Redis Sentinel on top of Redigo\'s connection Pool.\\n\\nFor communicating with Redis Cluster one more library may be used \u2013 [mna/redisc](https://github.com/mna/redisc) which is also a layer on top of `redigo` basic functionality.\\n\\nCombining `redigo` + `FZambia/sentinel` + `mna/redisc` we managed to implement all three connection modes. This worked, though resulted in rather tricky Redis setup. Also, it was difficult to re-use existing pipelining code we had for a standalone Redis with Redis Cluster. As a result, Centrifugo only used pipelining in a standalone or Sentinel Redis cases. When using Redis Cluster, however, Centrifugo merely used the connection pool to issue requests thus not benefiting from request pipelining. Due to this we had some code duplication to send the same requests in various Redis configurations.\\n\\nAnother thing is that Redigo uses `interface{}` for command construction. To send command to Redis Redigo has `Do` method which accepts name of the command and variadic `interface{}` arguments to construct command arguments:\\n\\n```go\\nDo(commandName string, args ...interface{}) (reply interface{}, err error)\\n```\\n\\nWhile this works well and you can issue any command to Redis, you need to be very accurate when constructing a command. This also adds some allocation overhead. As we know more memory allocations lead to the increased CPU utilization because the allocation process itself requires more processing power and the GC is under more strain.\\n\\nAt some point we felt that eliminating additional dependencies (even though I am the author of one of them) and reducing allocations in Redis communication layer is a nice step forward for Centrifugo. So we started looking around for `redigo` alternatives.\\n\\nTo summarize, here is what we wanted from Redis library:\\n\\n* Possibility to work with all three Redis setup options we support: standalone, master-replica(s) with Sentinel, Redis Cluster, so we can depend on one library instead of three\\n* Less memory allocations (and more type-safety API is a plus)\\n* Support working with RESP2-only Redis servers as we need that for backwards compatibility. And some vendors like Redis Enterprise still support RESP2 protocol only\\n* The library should be actively maintained\\n\\n## Go-redis/redis\\n\\nThe most obvious alternative to Redigo is [go-redis/redis](https://github.com/go-redis/redis) package. It\'s popular, regularly gets updates, used by a huge amount of Go projects (Grafana, Thanos, etc.). And maintained by \\n[Vladimir Mihailenco](https://github.com/vmihailenco) who created several more awesome Go libraries, like [msgpack](https://github.com/vmihailenco/msgpack) for example. I personally successfully used `go-redis/redis` in several other projects I worked on.\\n\\nTo avoid setup boilerplate for various Redis installation variations `go-redis/redis` has [UniversalClient](https://pkg.go.dev/github.com/go-redis/redis/v9#UniversalClient). From docs:\\n\\n> UniversalClient is a wrapper client which, based on the provided options, represents either a ClusterClient, a FailoverClient, or a single-node Client. This can be useful for testing cluster-specific applications locally or having different clients in different environments.\\n\\nIn terms of implementation `go-redis/redis` also has internal pool of connections to Redis, similar to `redigo`. It\'s also possible to use [Client.Pipeline](https://pkg.go.dev/github.com/go-redis/redis/v9#Client.Pipeline) method to allocate a [Pipeliner](https://pkg.go.dev/github.com/go-redis/redis/v9#Pipeliner) interface and use it for pipelining. So `UniversalClient` reduces setup boilerplate for different Redis installation types and number of dependencies we had, and it provide very similar way to pipeline requests so we could easily re-implement things we had with Redigo.\\n\\nGo-redis also provides more type-safety when constructing commands compared to Redigo, almost every command in Redis is implemented as a separate method of `Client`, for example `Publish` [defined](https://pkg.go.dev/github.com/go-redis/redis/v9#Client.Publish) as:\\n\\n```go\\nfunc (c Client) Publish(ctx context.Context, channel string, message interface{}) *IntCmd\\n```\\n\\nYou can see though that we still have `interface{}` here for `message` argument type. I suppose this was implemented in such way for convenience \u2013 to pass both `string` or `[]byte`. But it still produces some extra allocations.\\n\\nWithout pipelining the simplest program with `go-redis/redis` may look like this:\\n\\n```go\\nfunc BenchmarkGoredis(b *testing.B) {\\n\\tclient := redis.NewUniversalClient(&redis.UniversalOptions{\\n\\t\\tAddrs:    []string{\\":6379\\"},\\n\\t\\tPoolSize: 128,\\n\\t})\\n\\tdefer client.Close()\\n\\n\\tb.ResetTimer()\\n\\tb.SetParallelism(128)\\n\\tb.ReportAllocs()\\n\\tb.RunParallel(func(pb *testing.PB) {\\n\\t\\tfor pb.Next() {\\n\\t\\t\\tresp := client.Set(context.Background(), \\"goredis\\", \\"test\\", 0)\\n\\t\\t\\tif resp.Err() != nil {\\n\\t\\t\\t\\tb.Fatal(resp.Err())\\n\\t\\t\\t}\\n\\t\\t}\\n\\t})\\n}\\n```\\n\\nLet\'s run it:\\n\\n```\\nBenchmarkRedigo-8        228804        4648 ns/op        62 B/op         2 allocs/op\\nBenchmarkGoredis-8       268444        4561 ns/op       244 B/op         8 allocs/op\\n```\\n\\nResult is pretty comparable to Redigo, though Go-redis allocates more (btw most of allocations come from the connection liveness check upon getting from the pool which can not be turned off).\\n\\n![](/img/goredis_allocs.png)\\n\\nIt\'s interesting \u2013 if we dive deeper into what is it we can discover that this is the only way in Go to check connection was closed without reading data from it. The approach was originally introduced [by go-sql-driver/mysql](https://github.com/go-sql-driver/mysql/blob/41dd159e6ec9afad00d2b90144bbc083ea860db1/conncheck.go#L23), it\'s not cross-platform, and [related issue](https://github.com/golang/go/issues/15735) may be found in Go issue tracker.\\n\\nBut as I said in Centrifugo we already used pipelining over the dedicated connection for all operations so we avoid frequently getting connections from the pool. And early experiments proved that `go-redis` may provide some performance benefits for our use case.\\n\\nAt some point [@j178](https://github.com/j178) sent [a pull request](https://github.com/centrifugal/centrifuge/pull/235) to Centrifuge library with `Broker` and `PresenceManager` implementations based on `go-redis/redis`. The amount of code to cover all the various Redis setups was reduced, we got only one dependency instead of three \ud83d\udd25\\n\\nBut what about performance? Here we will show results for several operations which are typical for Centrifugo:\\n\\n1. Publish a message to a channel without saving it to the history - this is just a Redis PUBLISH command going through Redis PUB/SUB system (`RedisPublish`)\\n2. Publish message to a channel with saving it to history - this involves executing the LUA script on Redis side where we add a publication to STREAM data structure, update meta information HASH, and finally PUBLISH to PUB/SUB (`RedisPublish_History`)\\n3. Subscribe to a channel - that\'s a SUBSCRIBE Redis command, this is important to have it fast as Centrifugo should be able to re-subscribe to all the channels in the system upon [mass client reconnect scenario](/blog/2020/11/12/scaling-websocket#massive-reconnect) (`RedisSubscribe`)\\n4. Recovering missed publication state from channel STREAM, this is again may be called lots of times when all clients reconnect at once (`RedisRecover`).\\n5. Updating connection presence information - many connections may periodically update their channel online presence information in Redis (`RedisAddPresence`)\\n\\nHere are the benchmark results we got when comparing `redigo` (v1.8.9) implementation (old) and `go-redis/redis` (v9.0.0-rc.2) implementation (new) with Redis v6.2.7 on Mac with M1 processor and benchmark paralellism 128:\\n\\n```\\n\u276f benchstat redigo_p128.txt goredis_p128.txt\\nname                      old time/op    new time/op    delta\\nRedisPublish-8            1.45\xb5s \xb110%    1.88\xb5s \xb1 4%  +29.32%  (p=0.000 n=10+10)\\nRedisPublish_History-8    12.5\xb5s \xb1 6%     9.7\xb5s \xb1 3%  -22.77%  (p=0.000 n=10+10)\\nRedisSubscribe-8          1.47\xb5s \xb124%    1.47\xb5s \xb110%     ~     (p=0.469 n=10+10)\\nRedisRecover-8            18.4\xb5s \xb1 2%     6.3\xb5s \xb1 0%  -65.78%  (p=0.000 n=10+8)\\nRedisAddPresence-8        3.72\xb5s \xb1 1%    3.40\xb5s \xb1 1%   -8.74%  (p=0.000 n=10+10)\\n\\nname                      old alloc/op   new alloc/op   delta\\nRedisPublish-8              483B \xb1 0%      499B \xb1 0%   +3.37%  (p=0.000 n=9+10)\\nRedisPublish_History-8    1.30kB \xb1 0%    1.08kB \xb1 0%  -16.67%  (p=0.000 n=10+10)\\nRedisSubscribe-8            892B \xb1 2%      662B \xb1 6%  -25.83%  (p=0.000 n=10+10)\\nRedisRecover-8            1.25kB \xb1 1%    1.00kB \xb1 0%  -19.91%  (p=0.000 n=10+10)\\nRedisAddPresence-8          907B \xb1 0%      827B \xb1 0%   -8.82%  (p=0.002 n=7+8)\\n\\nname                      old allocs/op  new allocs/op  delta\\nRedisPublish-8              10.0 \xb1 0%       9.0 \xb1 0%  -10.00%  (p=0.000 n=10+10)\\nRedisPublish_History-8      29.0 \xb1 0%      25.0 \xb1 0%  -13.79%  (p=0.000 n=10+10)\\nRedisSubscribe-8            22.0 \xb1 0%      14.0 \xb1 0%  -36.36%  (p=0.000 n=8+7)\\nRedisRecover-8              29.0 \xb1 0%      23.0 \xb1 0%  -20.69%  (p=0.000 n=10+10)\\nRedisAddPresence-8          18.0 \xb1 0%      17.0 \xb1 0%   -5.56%  (p=0.000 n=10+10)\\n```\\n\\n:::danger\\n\\nPlease note that this benchmark is not a pure performance comparison of two Go libraries for Redis \u2013 it\'s a performance comparison of Centrifugo Engine methods upon switching to a new library.\\n\\n:::\\n\\nOr visualized in Grafana:\\n\\n![](/img/redis_vis01.png)\\n\\n:::note\\n\\nCentrifugo benchmarks results shown in the post use parallelism 128. If someone interested to check numbers for paralellism 1 or 16 \u2013 [check out this comment on Github](https://github.com/centrifugal/centrifugal.dev/pull/18#issuecomment-1356263272).\\n\\n:::\\n\\nWe observe a noticeable reduction in allocations in these benchmarks and in most benchmarks (presented here and other not listed in this post) we observed a reduced latency.\\n\\nOverall, results convinced us that the migration from `redigo` to `go-redis/redis` may provide Centrifugo with everything we aimed for \u2013 all the goals for a `redigo` alternative outlined above were successfully fullfilled.\\n\\nOne good thing `go-redis/redis` allowed us to do is to use Redis pipelining also in a Redis Cluster case. It\'s possible due to the fact that `go-redis/redis` [re-maps pipeline objects internally](https://github.com/go-redis/redis/blob/c561f3ca7e5cf44ce1f1d3ef30f4a10a9c674c8a/cluster.go#L1062) based on keys to execute pipeline on the correct node of Redis Cluster. Actually, we could do the same based on `redigo` + `mna/redisc`, but here we got it for free.\\n\\nBTW, there is [a page with comparison](https://redis.uptrace.dev/guide/go-redis-vs-redigo.html) between `redigo` and `go-redis/redis` in `go-redis/redis` docs which outlines some things I mentioned here and some others.\\n\\nBut we have not migrated to `go-redis/redis` in the end. And the reason is another library \u2013 `rueidis`.\\n\\n## Rueidis\\n\\nWhile results were good with `go-redis/redis` we also made an attempt to implement Redis Engine on top of [rueian/rueidis](https://github.com/rueian/rueidis) library written by [@rueian](https://github.com/rueian). According to docs, `rueidis` is:\\n\\n> A fast Golang Redis client that supports Client Side Caching, Auto Pipelining, Generics OM, RedisJSON, RedisBloom, RediSearch, RedisAI, RedisGears, etc.\\n\\nThe readme of `rueidis` contains benchmark results where it hugely outperforms `go-redis/redis` in terms of operation latency/throughput in both single Redis and Redis Custer setups:\\n\\n![](/img/rueidis_1.png)\\n\\n![](/img/rueidis_2.png)\\n\\n`rueidis` works with standalone Redis, Sentinel Redis and Redis Cluster out of the box. Just like `UniversalClient` of `go-redis/redis`. So it also allowed us to reduce code boilerplate to work with all these setups.\\n\\nAgain, let\'s try to write a simple program like we had for Redigo and Go-redis above:\\n\\n```go\\nfunc BenchmarkRueidis(b *testing.B) {\\n\\tclient, err := rueidis.NewClient(rueidis.ClientOption{\\n\\t\\tInitAddress: []string{\\":6379\\"},\\n\\t})\\n\\tif err != nil {\\n\\t\\tb.Fatal(err)\\n\\t}\\n\\n\\tb.ResetTimer()\\n\\tb.SetParallelism(128)\\n\\tb.ReportAllocs()\\n\\tb.RunParallel(func(pb *testing.PB) {\\n\\t\\tfor pb.Next() {\\n\\t\\t\\tcmd := client.B().Set().Key(\\"rueidis\\").Value(\\"test\\").Build()\\n\\t\\t\\tres := client.Do(context.Background(), cmd)\\n\\t\\t\\tif res.Error() != nil {\\n\\t\\t\\t\\tb.Fatal(res.Error())\\n\\t\\t\\t}\\n\\t\\t}\\n\\t})\\n}\\n```\\n\\nAnd run it:\\n\\n```\\nBenchmarkRedigo-8        228804        4648 ns/op        62 B/op         2 allocs/op\\nBenchmarkGoredis-8       268444        4561 ns/op       244 B/op         8 allocs/op\\nBenchmarkRueidis-8      2908591        418.5 ns/op        4 B/op         1 allocs/op\\n```\\n\\n`rueidis` library comes with **automatic implicit pipelining**, so you can send each request in isolated way while `rueidis` makes sure the request becomes part of the pipeline sent to Redis \u2013 thus utilizing the connection between an application and Redis most efficiently with maximized throughput. The idea of implicit pipelining with Redis is not new and Go ecosystem already had [joomcode/redispipe](https://github.com/joomcode/redispipe) library which implemented it (though it comes with some limitations which made it unsuitable for Centrifugo use case).\\n\\nSo **applications that use a pool-based approach** for communication with Redis may observe dramatic improvements in latency and throughput when switching to the Rueidis library.\\n\\nFor Centrifugo we didn\'t expect such a huge speed-up as shown in the above graphs since we already used pipelining in Redis Engine. But `rueidis` implements some ideas which allow it to be efficient. Insights about these ideas are provided by Rueidis author in a \\"Writing a High-Performance Golang Client Library\\" series of posts on Medium:\\n\\n* [Part 1: Batching on Pipeline](https://betterprogramming.pub/writing-high-performance-golang-client-library-part-1-batching-on-pipeline-97988fe3211)\\n* [Part 2: Reading Again From Channels?](https://betterprogramming.pub/working-on-high-performance-golang-client-library-reading-again-from-channels-5e98ff3538cf)\\n* [Part 3: Remove the Bad Busy Loops With the Sync.Cond](https://betterprogramming.pub/working-on-high-performance-golang-client-library-remove-the-bad-busy-loops-with-the-sync-cond-e262b3fcb458)\\n\\nI did some prototypes with `rueidis` which were super-promising in terms of performance. There were some issues found during that early prototyping (mostly with PUB/SUB) \u2013 but all of them were quickly resolved by Rueian.\\n\\nUntil `v0.0.80` release `rueidis` did not support RESP2 though, so we could not replace our Redis Engine implementation with it. But as soon as it got RESP2 support we opened [a pull request with alternative implementation](https://github.com/centrifugal/centrifuge/pull/262).\\n\\nSince auto-pipelining is used in `rueidis` by default we were able to remove some of our own pipelining management code \u2013 so the Engine implementation is more concise now. One more thing to mention is a simpler PUB/SUB code we were able to write with `rueidis`. One example is that in `redigo` case we had to periodically PING PUB/SUB connection to maintain it alive, `rueidis` does this automatically.\\n\\nRegarding performance, here are the benchmark results we got when comparing `redigo` (v1.8.9) implementation (old) and `rueidis` (v0.0.90) implementation (new):\\n\\n```\\n\u276f benchstat redigo_p128.txt rueidis_p128.txt\\nname                      old time/op    new time/op    delta\\nRedisPublish-8            1.45\xb5s \xb110%    0.56\xb5s \xb1 1%  -61.53%  (p=0.000 n=10+9)\\nRedisPublish_History-8    12.5\xb5s \xb1 6%     9.7\xb5s \xb1 1%  -22.43%  (p=0.000 n=10+9)\\nRedisSubscribe-8          1.47\xb5s \xb124%    1.45\xb5s \xb1 1%     ~     (p=0.484 n=10+9)\\nRedisRecover-8            18.4\xb5s \xb1 2%     6.2\xb5s \xb1 1%  -66.08%  (p=0.000 n=10+10)\\nRedisAddPresence-8        3.72\xb5s \xb1 1%    3.60\xb5s \xb1 1%   -3.34%  (p=0.000 n=10+10)\\n\\nname                      old alloc/op   new alloc/op   delta\\nRedisPublish-8              483B \xb1 0%       91B \xb1 0%  -81.16%  (p=0.000 n=9+10)\\nRedisPublish_History-8    1.30kB \xb1 0%    0.39kB \xb1 0%  -70.08%  (p=0.000 n=10+8)\\nRedisSubscribe-8            892B \xb1 2%      360B \xb1 0%  -59.66%  (p=0.000 n=10+10)\\nRedisRecover-8            1.25kB \xb1 1%    0.36kB \xb1 1%  -71.52%  (p=0.000 n=10+10)\\nRedisAddPresence-8          907B \xb1 0%      151B \xb1 1%  -83.34%  (p=0.000 n=7+9)\\n\\nname                      old allocs/op  new allocs/op  delta\\nRedisPublish-8              10.0 \xb1 0%       2.0 \xb1 0%  -80.00%  (p=0.000 n=10+10)\\nRedisPublish_History-8      29.0 \xb1 0%      10.0 \xb1 0%  -65.52%  (p=0.000 n=10+10)\\nRedisSubscribe-8            22.0 \xb1 0%       6.0 \xb1 0%  -72.73%  (p=0.002 n=8+10)\\nRedisRecover-8              29.0 \xb1 0%       7.0 \xb1 0%  -75.86%  (p=0.000 n=10+10)\\nRedisAddPresence-8          18.0 \xb1 0%       3.0 \xb1 0%  -83.33%  (p=0.000 n=10+10)\\n```\\n\\nOr visualized in Grafana:\\n\\n![](/img/redis_vis02.png)\\n\\n2.5x times more publication throughput than we had before! Instead of 700k publications/sec, we went towards 1.7 million publications/sec due to drastically decreased publish operation latency (1.45\xb5s -> 0.59\xb5s). This means that our previous Engine implementation under-utilized Redis, and Rueidis just pushes us towards Redis limits. The latency of most other operations is also reduced.\\n\\nThe allocation effectiveness of the implementation based on \\"rueidis\\" is best. As you can see `rueidis` helped us to generate sufficiently fewer memory allocations for all our Redis operations. Allocation improvements directly affect Centrifugo node CPU usage. Though we will talk about CPU more later below.\\n\\nFor Redis Cluster case we also got benchmark results similar to the standalone Redis results above.\\n\\nI might add that I enjoyed building commands with `rueidis`. All Redis commands may be constructed using a builder approach. Rueidis comes with builders generated for all Redis commands. As an illustration, this is a process of building a PUBLISH Redis command:\\n\\n<video width=\\"100%\\" loop={true} autoPlay=\\"autoplay\\" muted controls=\\"\\" src=\\"/img/rueidis_cmd.mp4\\"></video>\\n\\nThis drastically reduces a chance to make a stupid mistake while constructing a command. Instead of always opening Redis docs to see a command syntax it\'s now possible to just start typing - and quickly come to the complete command to send.\\n\\n## Switching to Rueidis: reducing CPU usage\\n\\nAfter making all these benchmarks and implementing Engine in Rueidis I decided to check whether Centrifugo consumes less CPU with it. I expected a notable CPU reduction as Rueidis Engine implementation allocates much less than Redigo-based. Turned out it\'s not that simple.\\n\\nI ran Centrifugo with some artificial load and noticed that CPU consumption of the new implementation is actually... worse than we had with Redigo-based engine under equal conditions!\ud83d\ude29 But why?\\n\\nAs I mentioned above Redis pipelining is a technique when several commands may be combined into one batch to send over the network. In case of automatic pipelining the size of generated batches start playing a crucial role in application and Redis CPU usage \u2013 since smaller command batches result into more read/write system calls to the kernel on both application and Redis server sides. That\'s why projects like [Twemproxy](https://github.com/twitter/twemproxy) which sit between app and Redis have sich a good effect on Redis CPU usage among other things. \\n\\nAs we have seen above, Rueidis provides a better throughput and latency, but it\'s more agressive in terms of flushing data to the network. So in its default configuration we get smaller batches under th equal conditions than we had before in our own pipelining implementation based on Redigo (shown in the beginning of this post).\\n\\nLuckily, there is an option in Rueidis called `MaxFlushDelay` which allows to slow down write loop a bit to give Rueidis a chance to collect more commands to send in one batch. When this option is used Rueidis will make a pause after each network flush not bigger than selected value of `MaxFlushDelay` (please note, that this is a delay after flushing collected pipeline commands, not an additional delay for each request). Using some reasonable value it\'s possible to drastically reduce both application and Redis CPU utilization.\\n\\nTo demonstrate this I created a repo: https://github.com/FZambia/pipelines.\\n\\nThis repo contains three benchmarks where we use automatic pipelining: based on `redigo`, based on `go-redis/redis` and `rueidis`. In these benchmarks we produce concurrent requests, but instead of pushing the system towards the limits we are limiting number of requests sent to Redis, so we put all libraries in equal conditions.\\n\\nTo rate limit requests we are using [uber-go/ratelimit](https://github.com/uber-go/ratelimit) library. For example, to allow rate no more than 100k commands per second we can do sth like this:\\n\\n```go\\nrl := ratelimit.New(100, ratelimit.Per(time.Millisecond))\\nfor {\\n\\trl.Take()\\n\\t...\\n}\\n```\\n\\nWe limit requests per second we could actually just write `ratelimit.New(100000)` \u2013 but we aim to get a more smooth distribution of requests over time - so using millisecond resolution.\\n\\nLet\'s run all the benchmarks in the default configuration:\\n\\n<video width=\\"100%\\" loop={true} autoPlay=\\"autoplay\\" muted controls=\\"\\" src=\\"/img/redis_b1.mp4\\"></video>\\n\\nAverage CPU usage during the test (a bit rough but enough for demonstration):\\n\\n|                     | Redigo      | Go-redis/redis | Rueidis |\\n| ------------------- | ----------- | ----------- |----------- |\\n| Application CPU, % | 95            | 99             |  <span style={{color: \'red\'}}>116</span>            |\\n| Redis CPU, %       | 36             | 35              | <span style={{color: \'red\'}}>42</span>              |\\n\\nOK, Rueidis-based implementation is the worst here despite of allocating less than others. So let\'s try to change this by setting `MaxFlushDelay` to sth like 100 microseconds:\\n\\n<video width=\\"100%\\" loop={true} autoPlay=\\"autoplay\\" muted controls=\\"\\" src=\\"/img/redis_b2.mp4\\"></video>\\n\\nNow CPU usage is:\\n\\n|                     | Redigo      | Go-redis/redis | Rueidis |\\n| ------------------- | ----------- | ----------- |----------- |\\n| Application CPU, % | 95            | 99             |  <span style={{color: \'green\'}}>59</span>            |\\n| Redis CPU, %       | 36             | 35              | <span style={{color: \'green\'}}>12</span>              |\\n\\nSo we can achieve great CPU usage reduction. CPU went from 116% to 59% for the application side, and from 42% to only 12% for Redis! We are sacrificing latency though. Given the fact the CPU utilization reduction is very notable the trade-off is pretty fair.\\n\\n:::caution\\n\\nIt\'s definitely possible to improve CPU usage in Redigo and Go-redis/redis cases too \u2013 using similar technique. But the goal here was to improve Rueidis-based engine implementation to make it comparable or better than our Redigo-based implementation in terms of CPU utilization. \\n\\n:::\\n\\nAs you can see we were able to achieve better CPU results just by using 100 microseconds delay after each network flush. In real life, where we are not running Redis on localhost and have some network latency in between application and Redis, this delay should be insignificant at all. Indeed, adding `MaxFlushDelay` can even improve (!) the latency you have. You may wonder what happened with benchmarks we showed above after we added `MaxFlushDelay` option. In Centrifugo we chose default value 100 microseconds, and here are results on localhost (`old` without delay, `new` with delay):\\n\\n```\\n> benchstat rueidis_p128.txt rueidis_delay_p128.txt\\nname                      old time/op    new time/op    delta\\nRedisPublish-8             559ns \xb1 1%     468ns \xb1 0%  -16.35%  (p=0.000 n=9+8)\\nRedisPublish_History-8    9.72\xb5s \xb1 1%    9.67\xb5s \xb1 1%   -0.52%  (p=0.007 n=9+8)\\nRedisSubscribe-8          1.45\xb5s \xb1 1%    1.27\xb5s \xb1 1%  -12.49%  (p=0.000 n=9+10)\\nRedisRecover-8            6.25\xb5s \xb1 1%    5.85\xb5s \xb1 0%   -6.32%  (p=0.000 n=10+10)\\nRedisAddPresence-8        3.60\xb5s \xb1 1%    3.33\xb5s \xb1 1%   -7.52%  (p=0.000 n=10+10)\\n\\n(rest is not important here...)\\n```\\n\\nIt\'s even better for this set of benchmarks. Though while it\'s better for these benchmarks the numbers may differ for other under different conditions. For example, in the benchmarks we run we use concurrency 128, if we reduce concurrency we will notice reduced throughput \u2013 as batches Rueidis collects become smaller. Smaller batches + some delay to collect = less requests per second to send.\\n\\nThe problem is that the value to pause Rueidis write loop is a very use case specific, it\'s pretty hard to provide a reasonable default for it. Depending on request rate/size, network latency etc. you may choose a larger or smaller delay. In v4.1.0 we start with hardcoded 100 microsecond `MaxFlushDelay` which seems sufficient for most use cases and showed good results in the benchmarks - though possibly we will have to make it tunable later.\\n\\nTo check that Centrifugo benchmarks also utilize less CPU I added rate limiter (50k rps per second) to benchmarks and compared version without `MaxFlushDelay` and with 100 microsecond `MaxFlushDelay`:\\n\\n| 50k req per second | Without delay | With 100mks delay |\\n|----- |----- |----|\\n| BenchmarkPublish | Centrifugo - 75%, Redis - 24% |  Centrifugo - 44%, Redis - 9% |\\n| BenchmarkPublish_History | Centrifugo - 80% , Redis - 67% |  Centrifugo - 55%, Redis - 50% |\\n| BenchmarkSubscribe | Centrifugo - 80%, Redis - 30% |  Centrifugo - 45% , Redis - 14% |\\n| BenchmarkRecover | Centrifugo - 84%, Redis - 51% |  Centrifugo - 51%, Redis - 36% |\\n| BenchmarkPresence | Centrifugo - 114%, Redis - 69% |  Centrifugo - 90%, Redis - 60% |\\n\\n:::note\\n\\nIn this test I replaced `BenchmarkAddPresence` with `BenchmarkPresence` (get information about all online subscribers in channel) to also make sure we have CPU reduction when using read-intensive method, i.e. when Redis response is reasonably large.\\n\\n:::\\n\\nWe observe a notable CPU usage improvement here.\\n\\nHope you understand now why increasing `numPipelineWorkers` value in the pipelining code showed before results into increased CPU usage on app and Redis sides \u2013 due to smaller batch sizes and more read/write system calls as the consequence.\\n\\n:::note\\n\\nBTW, would it be a nice thing if Go benchmarking suite could show a CPU usage of the process in addition to time and alloc stats? \ud83e\udd14\\n\\n:::\\n\\n## Adding latency\\n\\nThe last thing to check is how new implementation works upon increased RTT between application and Redis. To add artificial latency on localhost on Linux one can use `tc` tool as [shown here](https://daniel.haxx.se/blog/2010/12/14/add-latency-to-localhost/) by Daniel Stenberg. But I am on MacOS so the simplest way I found was using [Shopify/toxiproxy](https://github.com/Shopify/toxiproxy). Sth like running a server:\\n\\n```bash\\ntoxyproxy-server\\n```\\n\\nAnd then in another terminal I used `toxiproxy-cli` to create toxic Redis proxy with additional latency on port 26379:\\n\\n```bash\\ntoxiproxy-cli create -l localhost:26379 -u localhost:6379 toxic_redis\\ntoxiproxy-cli toxic add -t latency -a latency=5 toxic_redis\\n```\\n\\nThe benchmark results are (`old` is Redigo-based, new is Rueidis-based):\\n\\n```\\n> benchstat redigo_latency_p128.txt rueidis_delay_latency_p128.txt\\nname                      old time/op    new time/op    delta\\nRedisPublish-8            31.5\xb5s \xb1 1%     5.6\xb5s \xb1 3%   -82.26%  (p=0.000 n=9+10)\\nRedisPublish_History-8    62.8\xb5s \xb1 3%    10.6\xb5s \xb1 4%   -83.05%  (p=0.000 n=10+10)\\nRedisSubscribe-8          1.52\xb5s \xb1 5%    6.05\xb5s \xb1 8%  +298.70%  (p=0.000 n=8+10)\\nRedisRecover-8            48.3\xb5s \xb1 3%     7.3\xb5s \xb1 4%   -84.80%  (p=0.000 n=10+10)\\nRedisAddPresence-8        52.3\xb5s \xb1 4%     5.8\xb5s \xb1 2%   -88.94%  (p=0.000 n=10+10)\\n\\n(rest is not important here...)\\n```\\n\\nWe see that new Engine implementation behaves much better for most cases. But what happened to `Subscribe` operation? It did not change at all in Redigo case \u2013 the same performance as if there is no additional latency involved!\\n\\nTurned out that when we call `Subscribe` in Redigo case, Redigo only flushes data to the network without waiting synchronously for subscribe result.\\n\\nIt makes sense in general and we can listen to subscribe notifications asynchronously, but in Centrifugo we relied on the returned error thinking that it includes succesful subscription result from Redis - meaning that we already subscribed to a channel at that point. And this could theoretically lead to some rare bugs in Centrifugo.\\n\\nRueidis library waits for subscribe response. So here the behavior of `rueidis` while differs from `redigo` in terms of throughput under increased latency just fits Centrifugo better in terms of behavior. So we go with it.\\n\\n## Conclusion\\n\\nMigrating from Redigo to Rueidis library was not just a task of rewriting code, we had to carefully test various aspects of Redis Engine behaviour \u2013 latency, throughput, CPU utilization of application, and even CPU utilization of Redis itself under the equal application load conditions.\\n\\nI think that we will find more projects in Go ecosystem using `rueidis` library shortly. Not just because of its allocation efficiency and out-of-the-box throughput, but also due to a convenient type-safe command API.\\n\\nFor most Centrifugo users this migration means more efficient CPU usage as new implementation allocates less memory (less work to allocate and less strain on GC) and we tried to find a reasonable batch size to reduce the number of system calls for common operations. While latency and throughput of single Centrifugo node should be better as we make concurrent Redis calls from many goroutines.\\n\\nHopefully readers will learn some tips from this post which can help to achieve effective communication with Redis from Go or another programming language.\\n\\nA few key takeaways:\\n\\n* Redis pipelining may increase throughput and reduce latency, it can also reduce CPU utilization of Redis\\n* Don\'t blindly trust Go benchmark numbers but also think about CPU effect of changes you made (sometimes of the external system also)\\n* Reduce the number of system calls to decrease CPU utilization\\n* Everything is a trade-off \u2013 latency or resource usage? Your own WebSocket server or Centrifugo?\\n* Don\'t rely on someone\'s else benchmarks, including those published here. **Measure for your own use case**. Take into account your load profile, paralellism, network latency, data size, etc.\\n\\nP.S. One thing worth mentioning and which may be helpful for someone is that during our comparison experiments we discovered that Redis 7 has a major latency increase compared to Redis 6 when executing Lua scripts. So if you have performance sensitive code with Lua scripts take a look at [this Redis issue](https://github.com/redis/redis/issues/10981). With the help of Redis developers some things already improved in `unstable` Redis branch, hopefully that issue will be closed at the time you read this post."},{"id":"/2022/07/29/101-way-to-subscribe","metadata":{"permalink":"/blog/2022/07/29/101-way-to-subscribe","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2022-07-29-101-way-to-subscribe.md","source":"@site/blog/2022-07-29-101-way-to-subscribe.md","title":"101 ways to subscribe user on a personal channel in Centrifugo","description":"In this post we are discussing vaious ways developers can use to subscribe user to a personal channel in Centrifugo","date":"2022-07-29T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":10.64,"hasTruncateMarker":true,"authors":[{"name":"Alexander Emelin","title":"Author of Centrifugo","imageURL":"https://github.com/FZambia.png"}],"frontMatter":{"title":"101 ways to subscribe user on a personal channel in Centrifugo","tags":["centrifugo","tutorial"],"description":"In this post we are discussing vaious ways developers can use to subscribe user to a personal channel in Centrifugo","author":"Alexander Emelin","authorTitle":"Author of Centrifugo","authorImageURL":"https://github.com/FZambia.png","image":"/img/101-way_thumb.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Improving Centrifugo Redis Engine throughput and allocation efficiency with Rueidis Go library","permalink":"/blog/2022/12/20/improving-redis-engine-performance"},"nextItem":{"title":"Centrifugo v4 released \u2013 a little revolution","permalink":"/blog/2022/07/19/centrifugo-v4-released"}},"content":"![Centrifuge](/img/101-way.png)\\n\\nLet\'s say you develop an application and want a real-time connection which is subscribed to one channel. Let\'s also assume that this channel is used for user personal notifications. So only one user in the application can subcribe to that channel to receive its notifications in real-time.\\n\\nIn this post we will look at various ways to achieve this with Centrifugo, and consider trade-offs of the available approaches. The main goal of this tutorial is to help Centrifugo newcomers be aware of all the ways to control channel permissions by reading just one document.\\n\\nAnd... well, there are actually 8 ways I found, not 101 \ud83d\ude07\\n\\n\x3c!--truncate--\x3e\\n\\n## Setup\\n\\nTo make the post a bit easier to consume let\'s setup some things. Let\'s assume that the user for which we provide all the examples in this post has ID `\\"17\\"`. Of course in real-life the examples given here can be extrapolated to any user ID.\\n\\nWhen you create a real-time connection to Centrifugo the connection is authenticated using the one of the following ways:\\n\\n* using [connection JWT](/docs/server/authentication)\\n* using connection request proxy from Centrifugo to the configured endpoint of the application backend ([connect proxy](/docs/server/proxy#connect-proxy))\\n\\nAs soon as the connection is successfully established and authenticated Centrifugo knows the ID of connected user. This is important to understand.\\n\\nAnd let\'s define a namespace in Centrifugo configuration which will be used for personal user channels:\\n\\n```json\\n{\\n    ...\\n    \\"namespaces\\": [\\n        {\\n            \\"name\\": \\"personal\\",\\n            \\"presence\\": true\\n        }\\n    ]\\n}\\n```\\n\\nDefining namespaces for each new real-time feature is a good practice in Centrifugo. As an awesome improvement we also enabled `presence` in the `personal` namespace, so whenever users subscribe to a channel in this namespace Centrifugo will maintain online presence information for each channel. So you can find out all connections of the specific user existing at any moment. Defining `presence` is fully optional though - turn it of if you don\'t need presence information and don\'t want to spend additional server resources on maintaining presence.\\n\\n## #1 \u2013 user-limited channel\\n\\n:::tip\\n\\nProbably the most performant approach.\\n\\n:::\\n\\nAll you need to do is to extend namespace configuration with `allow_user_limited_channels` option:\\n\\n```json\\n{\\n    \\"namespaces\\": [\\n        {\\n            \\"name\\": \\"personal\\",\\n            \\"presence\\": true,\\n            \\"allow_user_limited_channels\\": true\\n        }\\n    ]\\n}\\n```\\n\\nOn the client side you need to have sth like this (of course the ID of current user will be dynamic in real-life):\\n\\n```javascript\\nconst sub = centrifuge.newSubscription(\'personal:#17\');\\nsub.on(\'publication\', function(ctx) {\\n    console.log(ctx.data);\\n})\\nsub.subscribe();\\n```\\n\\nHere you are subscribing to a channel in `personal` namespace and listening to publications coming from a channel. Having `#` in channel name tells Centrifugo that this is a user-limited channel (because `#` is a special symbol that is treated in a special way by Centrifugo as soon as `allow_user_limited_channels` enabled).\\n\\nIn this case the user ID part of user-limited channel is `\\"17\\"`. So Centrifugo allows user with ID `\\"17\\"` to subscribe on `personal:#17` channel. Other users won\'t be able to subscribe on it.\\n\\nTo publish updates to subscription all you need to do is to publish to `personal:#17` using server publish API (HTTP or GRPC).\\n\\n## #2 - channel token authorization\\n\\n:::tip\\n\\nProbably the most flexible approach, with reasonably good performance characteristics.\\n\\n:::\\n\\nAnother way we will look at is using subscription JWT for subscribing. When you create Subscription object on the client side you can pass it a subscription token, and also provide a function to retrieve subscription token (useful to automatically handle token refresh, it also handles initial token loading).\\n\\n```javascript\\nconst token = await getSubscriptionToken(\'personal:17\');\\n\\nconst sub = centrifuge.newSubscription(\'personal:17\', {\\n    token: token\\n});\\nsub.on(\'publication\', function(ctx) {\\n    console.log(ctx.data);\\n})\\nsub.subscribe();\\n```\\n\\nInside `getSubscriptionToken` you can issue a request to the backend, for example in browser it\'s possible to do with fetch API.\\n\\nOn the backend side you know the ID of current user due to the native session mechanism of your app, so you can decide whether current user has permission to subsribe on `personal:17` or not. If yes \u2013 return subscription JWT according to our rules. If not - return empty string so subscription will go to unsubscribed state with `unauthorized` reason.\\n\\nHere are examples for generating subscription HMAC SHA-256 JWTs for channel `personal:17` and HMAC secret key `secret`:\\n\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n````mdx-code-block\\n<Tabs\\n  className=\\"unique-tabs\\"\\n  defaultValue=\\"python\\"\\n  values={[\\n    {label: \'Python\', value: \'python\'},\\n    {label: \'NodeJS\', value: \'node\'},\\n  ]\\n}>\\n<TabItem value=\\"python\\">\\n\\n```python\\nimport jwt\\nimport time\\n\\nclaims = {\\n    \\"sub\\": \\"17\\",\\n    \\"channel\\": \\"personal:17\\"\\n    \\"exp\\": int(time.time()) + 30*60\\n}\\n\\ntoken = jwt.encode(claims, \\"secret\\", algorithm=\\"HS256\\").decode()\\nprint(token)\\n```\\n\\n</TabItem>\\n<TabItem value=\\"node\\">\\n\\n```javascript\\nconst jose = require(\'jose\')\\n\\n(async function main() {\\n  const secret = new TextEncoder().encode(\'secret\')\\n  const alg = \'HS256\'\\n\\n  const token = await new jose.SignJWT({ \'sub\': \'17\', \'channel\': \'personal:17\' })\\n    .setProtectedHeader({ alg })\\n    .setExpirationTime(\'30m\')\\n    .sign(secret)\\n\\n  console.log(token);\\n})();\\n```\\n\\n</TabItem>\\n</Tabs>\\n````\\n\\nSince we set expiration time for subscription JWT tokens we also need to provide a `getToken` function to a client on the frontend side:\\n\\n```javascript\\nconst sub = centrifuge.newSubscription(\'personal:17\', {\\n    getToken: async function (ctx) {\\n        const token = await getSubscriptionToken(\'personal:17\');\\n        return token;\\n    }\\n});\\nsub.on(\'publication\', function(ctx) {\\n    console.log(ctx.data);\\n})\\nsub.subscribe();\\n```\\n\\nThis function will be called by SDK automatically to refresh subscription token when it\'s going to expire. And note that we omitted setting `token` option here \u2013 since SDK is smart enough to call provided `getToken` function to extract initial subscription token from the backend.\\n\\nThe good thing in using subscription JWT approach is that you can provide token expiration time, so permissions to subscribe on a channel will be validated from time to time while connection is active. You can also provide additional channel context info which will be attached to presence information (using `info` claim of subscription JWT). And you can granularly control channel permissions using `allow` claim of token \u2013 and give client capabilities to publish, call history or presence information (this is Centrifugo PRO feature at this point). Token also allows to override some namespace options on per-subscription basis (with `override` claim).\\n\\nUsing subscription tokens is a general approach for any channels where you need to check access first, not only for personal user channels.\\n\\n## #3 - subscribe proxy\\n\\n:::tip\\n\\nProbably the most secure approach.\\n\\n:::\\n\\nSubscription JWT gives client a way to subscribe on a channel, and avoid requesting your backend for permission on every resubscribe. Token approach is very good in massive reconnect scenario, when you have many connections and they all resubscribe at once (due to your load balancer reload, for example). But this means that if you unsubscribed client from a channel using server API, client can still resubscribe with token again - until token will expire. In some cases you may want to avoid this.\\n\\nAlso, in some cases you want to be notified when someone subscribes to a channel.\\n\\nIn this case you may use subscribe proxy feature. When using subscribe proxy every attempt of a client to subscribe on a channel will be translated to request (HTTP or GRPC) from Centrifugo to the application backend. Application backend can decide whether client is allowed to subscribe or not.\\n\\nOne advantage of using subscribe proxy is that backend can additionally provide initial channel data for the subscribing client. This is possible using `data` field of subscribe result generated by backend subscribe handler.\\n\\n```json\\n{\\n    \\"proxy_subscribe_endpoint\\": \\"http://localhost:9000/centrifugo/subscribe\\",\\n    \\"namespaces\\": [\\n        {\\n            \\"name\\": \\"personal\\",\\n            \\"presence\\": true,\\n            \\"proxy_subscribe\\": true\\n        }\\n    ]\\n}\\n```\\n\\nAnd on the backend side define a route `/centrifugo/subscribe`, check permissions of user upon subscription and return result to Centrifugo according to our subscribe proxy docs. Or simply run GRPC server using our proxy definitions and react on subscription attempt sent from Centrifugo to backend over GRPC.\\n\\nOn the client-side code is as simple as:\\n\\n```javascript\\nconst sub = centrifuge.newSubscription(\'personal:17\');\\nsub.on(\'publication\', function(ctx) {\\n    console.log(ctx.data);\\n})\\nsub.subscribe();\\n```\\n\\n## #4 - server-side channel in connection JWT\\n\\n:::tip\\n\\nThe approach where you don\'t need to manage client-side subscriptions.\\n\\n:::\\n\\n[Server-side subscriptions](/docs/server/server_subs) is a way to consume publications from channels without even create Subscription objects on the client side. In general, client side Subscription objects provide a more flexible and controllable way to work with subscriptions. Clients can subscribe/unsubscribe on channels at any point. Client-side subscriptions provide more details about state transitions.\\n\\nWith server-side subscriptions though you are consuming publications directly from Client instance:\\n\\n```javascript\\nconst client = new Centrifuge(\'ws://localhost:8000/connection/websocket\', {\\n    token: \'CONNECTION-JWT\'\\n});\\nclient.on(\'publication\', function(ctx) {\\n    console.log(\'publication received from server-side channel\', ctx.channel, ctx.data);\\n});\\nclient.connect();\\n```\\n\\nIn this case you don\'t have separate Subscription objects and need to look at `ctx.channel` upon receiving publication or to publication content to decide how to handle it. Server-side subscriptions could be a good choice if you are using Centrifugo unidirectional transports and don\'t need dynamic subscribe/unsubscribe behavior.\\n\\nThe first way to subscribe client on a server-side channel is to include `channels` claim into connection JWT:\\n\\n```json\\n{\\n    \\"sub\\": \\"17\\",\\n    \\"channels\\": [\\"personal:17\\"]\\n}\\n```\\n\\nUpon successful connection user will be subscribed to a server-side channel by Centrifugo. One downside of using server-side channels is that errors in one server-side channel (like impossible to recover missed messages) may affect the entire connection and result into reconnects, while with client-side subscriptions individual subsription failures do not affect the entire connection.\\n\\nBut having one server-side channel per-connection seems a very reasonable idea to me in many cases. And if you have stable set of subscriptions which do not require lifetime state management \u2013 this can be a nice approach without additional protocol/network overhead involved.\\n\\n## #5 - server-side channel in connect proxy\\n\\nSimilar to the previous one for cases when you are authenticating connections over connect proxy instead of using JWT.\\n\\nThis is possible using `channels` field of connect proxy handler result. The code on the client-side is the same as in Option #4 \u2013 since we only change the way how list of server-side channels is provided.\\n\\n## #6 - automatic personal channel subscription\\n\\n:::tip\\n\\nAlmost no code approach.\\n\\n:::\\n\\nAs we pointed above Centrifugo knows an ID of the user due to authentication process. So why not combining this knowledge with automatic server-side personal channel subscription? Centrifugo provides exactly this with user personal channel feature.\\n\\n```json\\n{\\n    \\"user_subscribe_to_personal\\": true,\\n    \\"user_personal_channel_namespace\\": \\"personal\\",\\n    \\"namespaces\\": [\\n        {\\n            \\"name\\": \\"personal\\",\\n            \\"presence\\": true\\n        }\\n    ]\\n}\\n```\\n\\nThis feature only subscribes non-anonymous users to personal channels (those with non-empty user ID). The configuration above will subscribe our user `\\"17\\"` to channel `personal:#17` automatically after successful authentication.\\n\\n## #7 \u2013 capabilities in connection JWT\\n\\nAllows using client-side subscriptions, but skip receiving subscription token. This is only available in Centrifugo PRO at this point.\\n\\nSo when generating JWT you can provide additional `caps` claim which contains channel resource capabilities:\\n\\n````mdx-code-block\\n<Tabs\\n  className=\\"unique-tabs\\"\\n  defaultValue=\\"python\\"\\n  values={[\\n    {label: \'Python\', value: \'python\'},\\n    {label: \'NodeJS\', value: \'node\'},\\n  ]\\n}>\\n<TabItem value=\\"python\\">\\n\\n```python\\nimport jwt\\nimport time\\n\\nclaims = {\\n    \\"sub\\": \\"17\\",\\n    \\"exp\\": int(time.time()) + 30*60,\\n    \\"caps\\": [\\n        {\\n            \\"channels\\": [\\"personal:17\\"],\\n            \\"allow\\": [\\"sub\\"]\\n        }\\n    ]\\n}\\n\\ntoken = jwt.encode(claims, \\"secret\\", algorithm=\\"HS256\\").decode()\\nprint(token)\\n```\\n\\n</TabItem>\\n<TabItem value=\\"node\\">\\n\\n```javascript\\nconst jose = require(\'jose\');\\n\\n(async function main() {\\n  const secret = new TextEncoder().encode(\'secret\')\\n  const alg = \'HS256\'\\n\\n  const token = await new jose.SignJWT({\\n    sub: \'17\',\\n    caps: [\\n      {\\n        \\"channels\\": [\\"personal:17\\"],\\n        \\"allow\\": [\\"sub\\"]\\n      }\\n    ]\\n  })\\n    .setProtectedHeader({ alg })\\n    .setExpirationTime(\'30m\')\\n    .sign(secret)\\n\\n  console.log(token);\\n})();\\n```\\n\\n</TabItem>\\n</Tabs>\\n````\\n\\nWhile in case of single channel the benefit of using this approach is not really obvious, it can help when you are using several channels with stric access permissions per connection, where providing capabilities can help to save some traffic and CPU resources since we avoid generating subscription token for each individual channel.\\n\\n## #8 \u2013 capabilities in connect proxy\\n\\nThis is very similar to the previous approach, but capabilities are passed to Centrifugo in connect proxy result. So if you are using connect proxy for auth then you can still provide capabilities in the same form as in JWT. This is also a Centrifugo PRO feature.\\n\\n## Teardown\\n\\nWhich way to choose? Well, it depends. Since your application will have more than only a personal user channel in many cases you should decide which approach suits you better in each particular case \u2013 it\'s hard to give the universal advice.\\n\\nClient-side subscriptions are more flexible in general, so I\'d suggest using them whenever possible. Though you may use unidirectional transports of Centrifugo where subscribing to channels from the client side is not simple to achieve (though still possible using our server subscribe API). Server-side subscriptions make more sense there.\\n\\nThe good news is that all our official bidirectional client SDKs support all the approaches mentioned in this post. Hope designing the channel configuration on top of Centrifugo will be a pleasant experience for you.\\n\\n:::note Attributions\\n\\n* <a href=\\"https://www.freepik.com/vectors/internet-network\\">Internet network vector created by rawpixel.com - www.freepik.com</a>\\n* <a href=\\"https://www.flaticon.com/free-icons/cyber-security\\" title=\\"cyber security icons\\">Cyber security icons created by Smashicons - Flaticon</a>\\n\\n:::"},{"id":"/2022/07/19/centrifugo-v4-released","metadata":{"permalink":"/blog/2022/07/19/centrifugo-v4-released","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2022-07-19-centrifugo-v4-released.md","source":"@site/blog/2022-07-19-centrifugo-v4-released.md","title":"Centrifugo v4 released \u2013 a little revolution","description":"Centrifugo v4 provides an optimized client protocol, modern WebSocket emulation, improved channel security, redesigned client SDK behavior, experimental HTTP/3 and WebTransport support.","date":"2022-07-19T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"release","permalink":"/blog/tags/release"}],"readingTime":20.905,"hasTruncateMarker":true,"authors":[{"name":"Centrifugal team","title":"Let the Centrifugal force be with you","imageURL":"/img/logo_animated.svg"}],"frontMatter":{"title":"Centrifugo v4 released \u2013 a little revolution","tags":["centrifugo","release"],"description":"Centrifugo v4 provides an optimized client protocol, modern WebSocket emulation, improved channel security, redesigned client SDK behavior, experimental HTTP/3 and WebTransport support.","author":"Centrifugal team","authorTitle":"Let the Centrifugal force be with you","authorImageURL":"/img/logo_animated.svg","image":"/img/v4_thumb.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"101 ways to subscribe user on a personal channel in Centrifugo","permalink":"/blog/2022/07/29/101-way-to-subscribe"},"nextItem":{"title":"Building a multi-room chat application with Laravel and Centrifugo","permalink":"/blog/2021/12/14/laravel-multi-room-chat-tutorial"}},"content":"![Centrifuge](/img/v4.jpg)\\n\\nToday we are excited to announce the next generation of Centrifugo \u2013 Centrifugo v4. The release takes Centrifugo to the next level in terms of client protocol performance, WebSocket fallback simplicity, SDK ecosystem and channel security model. It also comes with a couple of cutting-edge technologies to experiment with such as HTTP/3 and WebTransport.\\n\\n\x3c!--truncate--\x3e\\n\\n:::info About Centrifugo\\n\\nIf you\'ve never heard of Centrifugo before, it\'s an open-source scalable real-time messaging server written in Go language. Centrifugo can instantly deliver messages to application online users connected over supported transports (WebSocket, HTTP-streaming, SSE/EventSource, GRPC, SockJS). Centrifugo has the concept of a channel \u2013 so it\'s a user-facing PUB/SUB server.\\n\\nCentrifugo is language-agnostic and can be used to build chat apps, live comments, multiplayer games, real-time data visualizations, collaborative tools, etc. in combination with any backend. It is well suited for modern architectures and allows decoupling the business logic from the real-time transport layer.\\n\\nSeveral official client SDKs for browser and mobile development wrap the bidirectional protocol. In addition, Centrifugo supports a unidirectional approach for simple use cases with no SDK dependency.\\n\\n:::\\n\\n## Centrifugo v3 flashbacks\\n\\nLet\'s start from looking back a bit. Centrifugo v3 was released last year. It had a great list of improvements \u2013 like unidirectional transports support (EventSource, HTTP-streaming and GRPC), GRPC transport for proxy, history iteration API, faster JSON protocol, super-fast but experimental Tarantool engine implementation, and others.\\n\\nDuring the Centrifugo v3 lifecycle we added even more JSON protocol optimizations and introduced a granular proxy mode. Experimental Tarantool engine has also evolved a bit.\\n\\nBut Centrifugo v3 did not contain anything... let\'s say **revolutional**. Revolutional for Centrifugo itself, community, or even the entire field of open-source real-time messaging.\\n\\nWith this release, we feel that we bring innovation to the ecosystem. Now let\'s talk about it and introduce all the major things of the brand new v4 release.\\n\\n<video width=\\"100%\\" loop={true} autoPlay=\\"autoplay\\" muted controls=\\"\\" src=\\"/img/v4_logo.mp4\\"></video>\\n\\n## Unified client SDK API\\n\\nThe most challenging part of Centrifugo project is not a server itself. Client SDKs are the hardest part of the ecosystem. We try to time additional improvements to the SDKs with each major release of the server. But this time the SDKs are the centerpiece of the v4 release.\\n\\nCentrifugo uses bidirectional asynchronous protocol between client and server. On top of this protocol SDK provides a request-response over an asynchronous connection, reconnection logic, subscription management and multiplexing, timeout and error handling, ping-pong, token refresh, etc. Some of these things are not that trivial to implement. And all this should be implemented in different programming languages. As you may know, we have official real-time SDKs in Javascript, Dart, Swift, Java and Go.\\n\\nWhile implementing the same protocol and same functions, all SDKs behaved slightly differently. That was the result of the missing SDK specification. Without a strict SDK spec, it was hard to document things, hard to explain the exact details of the real-time SDK behavior. What we did earlier in the Centrifugo documentation \u2013 was pointing users to specific SDK Github repo to look for behaviour details.\\n\\nThe coolest thing about Centrifugo v4 is the next generation SDK API. We now have a [client SDK API specification](/docs/transports/client_api). It\'s a source of truth for SDKs behavior which try to follow the spec closely.\\n\\nThe new SDK API is the result of several iterations and reflections on possible states, transitions, token refresh mechanism, etc. Users in our Telegram group may remember how it all started:\\n\\n![Centrifugo scheme](/img/states_prototype.jpg)\\n\\nAnd after several iterations these prototypes turned into working mechanisms with well-defined behaviour:\\n\\n![Centrifugo scheme](/img/client_state.png)\\n\\nA few things that have been revised from the ground up:\\n\\n* Client states, transitions, events\\n* Subscription states, transitions, events\\n* Connection and subscription token refresh behavior\\n* Ping-pong behavior (see details below)\\n* Resubscribe logic (SDKs can now resubscribe with backoff)\\n* Error handling\\n* Unified backoff behavior (based on [full jitter technique](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/)) \\n\\nWe now also have a separation between temporary and non-temporary protocol errors \u2013 this allows us to handle subscription internal server errors on the SDK level, making subscriptions more resilient, with automatic resubscriptions, and to ensure individual subscription failures do not affect the entire connection.\\n\\nThe mechanics described in the client SDK API specification are now implemented in all of our official SDKs. The SDKs now support all major client protocol features that currently exist. We believe this is a big step forward for the Centrifugo ecosystem and community.\\n\\n## Modern WebSocket emulation in Javascript\\n\\nWebSocket is supported almost everywhere these days. But there is a case that we believe is the last one preventing users to connect over WebSocket - corporate proxies. With the root certificate installed on employee computer machines, these proxies can block WebSocket traffic, even if it\'s wrapped in a TLS layer. That\'s really annoying, and often developers choose to not support clients connecting from such \\"broken\\" environments at all.\\n\\nPrior to v4, Centrifugo users could use the SockJS polyfill library to fill this gap.\\n\\nSockJS is great software \u2013 stable and field proven. It is still used by some huge real-time messaging players out there to polyfill the WebSocket transport.\\n\\nBut SockJS is an extra frontend dependency with a bunch of legacy transports, and [the future of it is unknown](https://github.com/sockjs/sockjs-client/issues/592).\\n\\nSockJS comes with a notable overhead \u2013 it\'s an aditional protocol wrapper, consumes more memory per connection on a server (at least when using SockJS-Go library \u2013 the only choice for implementing SockJS server in Go language these days). When using SockJS, Centrifugo users were losing the ability to use our main pure WebSocket transport because SockJS uses its own WebSocket implementation on a server side.\\n\\nSockJS does not support binary data transfer \u2013 only JSON format can be used with it. As you know, our main WebSocket transport works fine with binary in case of using Protobuf protocol format. So with SockJS we don\'t have fallback for WebSocket with a binary data transfer.\\n\\nAnd finally, if you want to use SockJS with a distributed backend, you must enable sticky session support on the load-balancer level. This way you can point requests from the client to the server to the correct server node \u2013 the one which maintains a persistent unidirectional HTTP connection.\\n\\nWe danced around the idea of replacing SockJS for a long time. But only now we are ready to provide our alternative to it \u2013 meet Centrifugo own **bidirectional emulation layer**. It\'s based on two additional transports:\\n\\n* HTTP-streaming (using modern browser [ReadableStream API](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream) in JavaScript, supports both binary Protobuf and JSON transfer)\\n* Eventsource (Server-Sent Events, SSE) \u2013 while a bit older choice and works with JSON only EventSource transport is loved by many developers and can provide fallback in slightly older browsers which don\'t have ReadableStream, so we implemented bidirectional emulation with it too.\\n\\nSo when the fallback is used, you always have a real-time, persistent connection in server -> to -> client direction. Requests in client -> to -> server direction are regular HTTP \u2013 similar to how SockJS works. But our bidirectional emulation layer does not require sticky sessions \u2013 Centrifugo can proxy client-to-server requests to the correct node in the cluster. **Having sticky sessions is an optimization** for Centrifugo bidirectional emulation layer, **not a requirement**. We believe that this is a game changer for our users \u2013 no need to bother about proper load balancing, especially since in most cases 95% or even more users will be able to connect using the WebSocket transport.\\n\\nHere is a simplified diagram of how it works:\\n\\n![Scheme](/img/emulation_scheme.png)\\n\\nThe bidirectional emulation layer is only supported by the Javascript SDK (`centrifuge-js`) \u2013 as we think fallbacks mostly make sense for browsers. If we find use cases where other SDKs can benefit from HTTP based transport \u2013 we can expand on them later.\\n\\nLet\'s look at example of using this feature from the Javascript side. To use fallbacks, all you need to do is to set up a list of desired transports with endpoints:\\n\\n```javascript\\nconst transports = [\\n    {\\n        transport: \'websocket\',\\n        endpoint: \'wss://your_centrifugo.com/connection/websocket\'\\n    },\\n    {\\n        transport: \'http_stream\',\\n        endpoint: \'https://your_centrifugo.com/connection/http_stream\'\\n    },\\n    {\\n        transport: \'sse\',\\n        endpoint: \'https://your_centrifugo.com/connection/sse\'\\n    }\\n];\\nconst centrifuge = new Centrifuge(transports);\\ncentrifuge.connect()\\n```\\n\\n:::note\\n\\nWe are using explicit transport endpoints in the above example due to the fact that transport endpoints can be configured separately in Centrifugo \u2013 there is no single entry point for all transports. Like the one in Socket.IO or SockJS when developer can only point client to the base address. In Centrifugo case, we are requesting an explicit transport/endpoint configuration from the SDK user.\\n\\n:::\\n\\nBy the way, a few advantages of HTTP-based transport over WebSocket:\\n\\n* Sessions can be automatically multiplexed within a single connection by the browser when the server is running over HTTP/2, while with WebSocket browsers open a separate connection in each browser tab\\n* Better compression support (may be enabled on load balancer level)\\n* WebSocket requires special configuration in some load balancers to get started (ex. Nginx)\\n\\nSockJS is still supported by Centrifugo and `centrifuge-js`, but it\'s now DEPRECATED.\\n\\n## No layering in client protocol\\n\\nNot only the API of client SDK has changed, but also the format of Centrifugo protocol messages. New format is more human-readable (in JSON case, of course), has a more compact ping message size (more on that below).\\n\\nThe client protocol is now one-shot encode/decode compatible. Previously, Centrifugo protocol had a layered structure and we had to encode some messages before appending them to the top-level message. Or decode two or three times to unwrap the message envelope. To achieve good performance when encoding and decoding client protocol messages, Centrifugo had to use various optimization techniques \u2013 like buffer memory pools, byte slice memory pools.\\n\\nBy restructuring the message format, we were able to avoid layering, which allowed us to slightly increase the performance of encoding/decoding without additional optimization tricks.\\n\\n![Scheme](/img/avoid_protocol_nesting.png)\\n\\nWe also simplified the [client protocol](/docs/transports/client_protocol) documentation overview a bit.\\n\\n## Redesigned PING-PONG\\n\\nIn many cases in practice (when dealing with persistent connections like WebSocket), pings and pongs are the most dominant types of messages passed between client and server. Your application may have many concurrent connections, but only a few of them receive the useful payload. But at the same time, we still need to send pings and respond with pongs. Thus, optimizing the ping-pong process can significantly reduce server resource usage.\\n\\nOne optimization comes from the revised PING-PONG behaviour. Previous versions of Centrifugo and SDKs sent ping/pong in both \\"client->to->server\\" and \\"server->to->client\\" directions (for WebSocket transport). This allowed finding non-active connections on both client and server sides.\\n\\nIn Centrifugo v4 we only send pings from a server to a client and expect pong from a client. On the client-side, we have a timer which fires if there hasn\'t been a ping from the server within the configured time, so we still have a way to detect closed connections.\\n\\nSending pings only in one direction results in 2 times less ping-pong messages - and this should be really noticable for Centrifugo installations with thousands of concurrent connections. In our experiments with 10k connections, server CPU usage was reduced by 30% compared to Centrifugo v3.\\n\\n![Scheme](/img/ping_pong_v3_v4.png)\\n\\nPings and pongs are application-level messages. Ping is just an empty asynchronous reply \u2013 for example in JSON case it\'s a 2-byte message: `{}`. Pong is an empty command \u2013 also, `{}` in JSON case. Having application-level pings from the server also allows unifying the PING format for all unidirectional transports.\\n\\nAnother improvement is that Centrifugo now randomizes the time it sends first ping to the client (but no longer than the configured ping interval). This allows to spread ping-pongs in time, providing a smoother CPU profile, especially after a massive reconnect scenario.\\n\\n## Secure by default channel namespaces\\n\\nData security and privacy are more important than ever in today\'s world. And as Centrifugo becomes more popular and widely used, the need to be `secure by default` only increases. \\n\\nPreviously, by default, clients could subcribe to all channels in a namespace (except private channels, which are now revised \u2013 see details below). It was possible to use `\\"protected\\": true` option to make namespace protected, but we are not sure if everyone did that. This is extra configuration and additional knowledge on how Centrifugo works.\\n\\nAlso, a common confusion we ran into: if server-side subscriptions were dictated by a connection JWT, many users would expect client-side subscriptions to those channels to not work. But without the `protected` option enabled, this was not the case.\\n\\nIn Centrifugo v4, by default, it is not possible to subscribe to a channel in a namespace. The namespace must be configured to allow subscriptions from clients, or token authorization must be used. There are a bunch of new namespace options to tune the namespace behavior. Also the ability to provide a regular expression for channels in the namespace.\\n\\nThe new permission-related channel option names better reflect the purpose of the option. For example, compare `\\"publish\\": true` and `\\"allow_publish_for_client\\": true`. The second one is more readable and provides a better understanding of the effect once turned on.\\n\\nCentrifugo is now more strict when checking channel name. Only ASCII symbols allowed \u2013 it was already mentioned in docs before, but wasn\'t actually enforced. Now we are fixing this.\\n\\nWe understand that these changes will make running Centrifugo more of a challenge, especially when all you want is a public access to all the channels without worrying too much about permissions. It\'s still possible to achieve, but now the intent must be expicitly expressed in the config.\\n\\nCheck out the updated documentation about [channels and namespaces](/docs/server/channels). Our v4 migration guide contains an **automatic converter** for channel namespace options.\\n\\n## Private channel concept revised\\n\\nA private channel is a special channel starting with `$` that could not be subscribed to without a subscription JWT. Prior to v4, having a known prefix allowed us to distinguish between public channels and private channels. But since namespaces are now non-public by default, this distinction is not really important.\\n\\nThis means 2 things:\\n\\n* it\'s now possible to subscribe to any channel by having a valid subscription JWT (not just those that start with `$`)\\n* channels beginning with `$` can only be subscribed with a subscription JWT, even if they belong to a namespace where subscriptions allowed for all clients. This is for security compatibility between v3 and v4.\\n\\nAnother notable change in a subscription JWT \u2013 `client` claim is now DEPRECATED. There is no need to put it in the subscription token anymore. Centrifugo supports it only for backwards compatibility, but it will be completely removed in the future releases.\\n\\nThe reason we\'re removing `client` claim is actually interesting. Due to the fact that `client` claim was a required part of the subscription JWT applications could run into a situation where during the [massive reconnect scenario](/blog/2020/11/12/scaling-websocket#massive-reconnect) (say, million connections reconnect) many requests for new subscription tokens can be generated because the subscription token must contain the client ID generated by Centrifugo for the new connection. That could make it unusually hard for the application backend to handle the load. With a connection JWT we had no such problem \u2013 as connections could simply reuse the previous token to reconnect to Centrifugo.\\n\\nNow the subscription token behaves just like the connection token, so we get a scalable solution for token-based subscriptions as well.\\n\\nWhat\'s more, this change paved the way for another big improvement...\\n\\n## Optimistic subscriptions\\n\\nThe improvement we just mentioned is called optimistic subscriptions. If any of you are familiar with the [QUIC](https://en.wikipedia.org/wiki/QUIC) protocol, then optimistic subscriptions are somewhat similar to the 0-RTT feature in QUIC. The idea is simple \u2013 we can include subscription commands to the first frame sent to the server.\\n\\nPreviously, we sent subscriptions only after receiving a successful Connect Reply to a Connect Command from a server. But with the new changes in token behaviour, it seems so logical to put subscribe commands within the initial connect frame. Especially since Centrifugo protocol always supported batching of commands. Even token-based subscriptions can now be included into the initial frame during reconnect process, since the previous token can be reused now.\\n\\n![](/img/optimistic_subs.png)\\n\\nThe benefit is awesome \u2013 in most scenarios, we save one RTT of latency when connecting to Centrifugo and subscribing to channels (which is actually the most common way to use Centrifugo). While not visible on localhost, this is pretty important in real-life. And this is less syscalls for the server after all, resulting in less CPU usage.\\n\\nOptimistic subscriptions are also great for bidirectional emulation with HTTP, as they avoid the long path of proxying a request to the correct Centrifugo node when connecting.\\n\\nOptimistic subscriptions are now only part of `centrifuge-js`. At some point, we plan to roll out this important optimization to all other client SDKs.\\n\\n## Channel capabilities\\n\\nThe channel capabilities feature is introduced as part of [Centrifugo PRO](/docs/pro/overview). Initially, we aimed to make it a part of the OSS version. But the lack of feedback on this feature made us nervous it\'s really needed. So adding it to PRO, where we still have room to evaluate the idea, seemed like the safer decision at the moment.\\n\\nCentrifugo allows configuring channel permissions on a per-namespace level. When creating a new real-time feature, it is recommended to create a new namespace for it and configure permissions. But to achieve a better channel permission control within a namespace the Channel capabilities can be used now.\\n\\nThe channel capability feature provides a possibility to set capabilities on an individual connection basis, or an individual channel subscription basis.\\n\\nFor example, in a connection JWT developers can set sth like:\\n\\n```json\\n{\\n    \\"caps\\": [\\n        {\\n            \\"channels\\": [\\"news\\", \\"user_42\\"],\\n            \\"allow\\": [\\"sub\\"]\\n        }\\n    ]\\n}\\n```\\n\\nAnd this tells Centrifugo that the connection is able to subscribe on channels `news` or `user_42` using client-side subscriptionsat any time while the connection is active. Centrifugo also supports wildcard and regex channel matches.\\n\\nSubscription JWT can provide capabilities for the channel too, so permissions may be controlled on an individual subscription basis, ex. the ability to publish and call history API may be expressed with `allow` claim in subscription JWT:\\n\\n```json\\n{\\n    \\"allow\\": [\\"pub\\", \\"hst\\"]\\n}\\n```\\n\\nRead more about this mechanism in [Channel capabilities](/docs/pro/capabilities) chapter.\\n\\n## Better connections API\\n\\nAnother addition to Centrifugo PRO is the improved [connection API](/docs/pro/connections). Previously, we could only return all connections from a specific user. \\n\\nThe API now supports filtering all connections: by user ID, by subscribed channel, by additional meta information attached to the connection.\\n\\nThe filtering works by user ID or with a help of [CEL expressions](https://opensource.google/projects/cel) (Common Expression Language). CEL expressions provide a developer-friendly, fast and secure (as they are not Turing-complete) way to evaluate some conditions. They are used in some Google services (ex. Firebase), in Envoy RBAC configuration, etc. If you\'ve never seen it before \u2013 take a look, cool project. We are also evaluating how to use CEL expressions for a dynamic and efficient channel permission checks, but that\'s an early story.\\n\\nThe `connections` API call result contains more useful information: a list of client\'s active channels, information about the tokens used to connect and subscribe, meta information attached to the connection.\\n\\n## Javascript client moved to TypeScript\\n\\nIt\'s no secret that `centrifuge-js` is the most popular SDK in the Centrifugo ecosystem. We put additional love to it \u2013 and `centrifuge-js` is now fully written in Typescript \u2764\ufe0f\\n\\nThis was a long awaited improvement, and it finally happened! The entire public API is strictly typed. The cool thing is that even `EventEmitter` events and event handlers are the subject to type checks - this should drastically simplify and speedup development and also help to reduce error possibility.\\n\\n## Experimenting with HTTP/3\\n\\nCentrifugo v4 has an **experimental** [HTTP/3](https://en.wikipedia.org/wiki/HTTP/3) support. Once TLS is enabled and `\\"http3\\": true` option is set all the endpoints on an external port will be served by a HTTP/3 server based on [lucas-clemente/quic-go](https://github.com/lucas-clemente/quic-go) implementation.\\n\\nIt\'s worth noting that WebSocket will still use HTTP/1.1 for its Upgrade request (there is an interesting IETF draft BTW about [Bootstrapping WebSockets with HTTP/3](https://www.ietf.org/archive/id/draft-ietf-httpbis-h3-websockets-02.html)). But HTTP-streaming and EventSource should work just fine with HTTP/3.\\n\\nHTTP/3 does not currently work with our ACME autocert TLS - i.e. you need to explicitly provide paths to cert and key files [as described here](/docs/server/tls#using-crt-and-key-files).\\n\\n## Experimenting with WebTransport\\n\\nHaving HTTP/3 on board allowed us to make one more thing. Some of you may remember the post [Experimenting with QUIC and WebTransport](/blog/2020/10/16/experimenting-with-quic-transport) published in our blog before. We danced around the idea to add [WebTransport](https://web.dev/webtransport/) to Centrifugo since then. [WebTransport IETF specification](https://datatracker.ietf.org/doc/draft-ietf-webtrans-http3/) is still a draft, it changed a lot since our first blog post about it. But WebTransport object is already part of Chrome (since v97) and things seem to be very close to the release.\\n\\nSo we added experimental WebTransport support to Centrifugo v4. This is made possible with the help of the [marten-seemann/webtransport-go](https://github.com/marten-seemann/webtransport-go) library.\\n\\nTo use WebTransport you need to run HTTP/3 experimental server and enable WebTransport endpoint with `\\"webtransport\\": true` option in the configuration. Then you can connect to that endpoint using `centrifuge-js`. For example, let\'s enable WebTransport and use WebSocket as a fallback option:\\n\\n```javascript\\nconst transports = [\\n    {\\n        transport: \'webtransport\',\\n        endpoint: \'https://your_centrifugo.com/connection/webtransport\'\\n    },\\n    {\\n        transport: \'websocket\',\\n        endpoint: \'wss://your_centrifugo.com/connection/websocket\'\\n    }\\n];\\nconst centrifuge = new Centrifuge(transports);\\ncentrifuge.connect()\\n```\\n\\nNote, that we are using secure schemes here \u2013 `https://` and `wss://`. While in WebSocket case you could opt for non-TLS communication, in HTTP/3 and specifically WebTransport non-TLS communication is simply not supported by the specification.\\n\\nIn Centrifugo case, we utilize the bidirectional reliable stream of WebTransport to pass our protocol between client and server. Both JSON and Protobuf communication formats are supported. There are some issues with the proper passing of the disconnect advice in some cases, otherwise it\'s fully functional.\\n\\nObviously, due to the limited WebTransport support in browsers at the moment, possible breaking changes in the WebTransport specification we can not recommended it for production usage for now. At some point in the future, it may become a reasonable alternative to WebSocket, now we are more confident that Centrifugo will be able to provide a proper support of it.\\n\\n## Migration guide\\n\\nThe [migration guide](/docs/getting-started/migration_v4) contains steps to upgrade your Centrifugo from version 3 to version 4. While there are many changes in the v4 release, it should be possible to migrate to Centrifugo v4 without changing the code on the client side at all. And then, after updating the server, gradually update the client-side to the latest version of the stack.\\n\\n## Conclusion\\n\\n![](/img/bg_cat.jpg)\\n\\nTo sum it up, here are some benefits of Centrifugo v4:\\n\\n* unified experience thoughout application frontend environments\\n* an optimized protocol which is generally faster, more compact and human-readable in JSON case, provides more resilient behavior for subscriptions\\n* revised channel namespace security model, more granular permission control\\n* more efficient and flexible use of subscription tokens\\n* better initial latency \u2013 thanks to optimistic subscriptions and the ability to pre-create subscription tokens (as the `client` claim not needed anymore)\\n* the ability to use more efficient WebSocket bidirectional emulation in the browser without having to worry about sticky sessions, unless you want to optimize the real-time infrastructure\\n\\nThat\'s it. We now begin the era of v4 and it is going to be awesome, no doubt.\\n\\n## Join community\\n\\nThe release contains many changes that strongly affect developing with Centrifugo. And of course you may have some questions or issues regarding new or changed concepts. Join our communities in Telegram (the most active) and Discord:\\n\\n[![Join the chat at https://t.me/joinchat/ABFVWBE0AhkyyhREoaboXQ](https://img.shields.io/badge/Telegram-Group-orange?style=flat&logo=telegram)](https://t.me/joinchat/ABFVWBE0AhkyyhREoaboXQ) &nbsp;[![Join the chat at https://discord.gg/tYgADKx](https://img.shields.io/discord/719186998686122046?style=flat&label=Discord&logo=discord)](https://discord.gg/tYgADKx)\\n\\nEnjoy Centrifugo v4, and let the Centrifugal force be with you.\\n\\n## Special thanks\\n\\nThe refactoring of client SDKs and introducing unified behavior based on the common spec was the hardest part of Centrifugo v4 release. Many thanks to [Vitaly Puzrin](https://github.com/puzrin) (who is the author of several popular open-source libraries such as [markdown-it](https://github.com/markdown-it/markdown-it), [fontello](https://github.com/fontello/fontello), and others). We had a series of super productive sessions with him on client SDK API design. Some great ideas emerged from these sessions and the result seems like a huge step forward for Centrifugal projects.\\n\\nAlso, thanks to [Anton Silischev](https://github.com/silischev) who helped a lot with WebTransport prototypes earlier this year, so we could quickly adopt WebTransport for v4.\\n\\n:::tip\\n\\nAs some of you know, Centrifugo server is built on top of the [Centrifuge](https://github.com/centrifugal/centrifuge) library for Go. Most of the optimizations and improvements described here are now also part of Centrifuge library.\\n\\nWith its new unified SDK behavior and bidirectional emulation layer, it seems a solid alternative to Socket.IO in the Go language ecosystem.\\n\\nIn some cases, Centrifuge library can be a more flexible solution than Centrifugo, since Centrifugo (as a standalone server) dictates some mechanics and rules that must be followed. In the case of Centrifugo, the business logic must live on the application backend side, with Centrifuge library it can be kept closer to the real-time transport layer.\\n\\n:::\\n\\n:::note Attributions\\n\\nThis post used images from freepik.com: [background](https://www.freepik.com/free-vector/abstract-background-consisting-colorful-arcs-illustration_14803794.htm#&position=5&from_view=author) by [liuzishan](https://www.freepik.com/author/liuzishan). Also [image](https://www.freepik.com/free-vector/abstract-black-circles-layers-dark-background-paper-cut_17303270.htm) by [kenshinstock](https://www.freepik.com/author/kenshinstock).\\n\\n:::"},{"id":"/2021/12/14/laravel-multi-room-chat-tutorial","metadata":{"permalink":"/blog/2021/12/14/laravel-multi-room-chat-tutorial","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2021-12-14-laravel-multi-room-chat-tutorial.md","source":"@site/blog/2021-12-14-laravel-multi-room-chat-tutorial.md","title":"Building a multi-room chat application with Laravel and Centrifugo","description":"In this tutorial, we are integrating Laravel framework with Centrifugo real-time messaging server to make a multi-room chat application.","date":"2021-12-14T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"tutorial","permalink":"/blog/tags/tutorial"},{"label":"laravel","permalink":"/blog/tags/laravel"},{"label":"php","permalink":"/blog/tags/php"}],"readingTime":10.75,"hasTruncateMarker":true,"authors":[{"name":"Anton Silischev","title":"Centrifugo contributor","imageURL":"https://github.com/silischev.png"}],"frontMatter":{"title":"Building a multi-room chat application with Laravel and Centrifugo","tags":["centrifugo","tutorial","laravel","php"],"description":"In this tutorial, we are integrating Laravel framework with Centrifugo real-time messaging server to make a multi-room chat application.","author":"Anton Silischev","authorTitle":"Centrifugo contributor","authorImageURL":"https://github.com/silischev.png","image":"/img/laravel_centrifugo.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Centrifugo v4 released \u2013 a little revolution","permalink":"/blog/2022/07/19/centrifugo-v4-released"},"nextItem":{"title":"Centrifugo integration with Django \u2013 building a basic chat application","permalink":"/blog/2021/11/04/integrating-with-django-building-chat-application"}},"content":"![Image](/img/laravel_centrifugo.jpg)\\n\\nIn this tutorial, we will create a multi-room chat server using [Laravel framework](https://laravel.com/) and [Centrifugo](https://centrifugal.dev/) real-time messaging server.\\n\\nAuthenticated users of our chat app will be able to create new chat rooms, join existing rooms and instantly communicate inside rooms with the help of Centrifugo WebSocket real-time transport.\\n\\n\x3c!--truncate--\x3e\\n\\n:::caution\\n\\nThis tutorial was written for Centrifugo v3. We recently released [Centrifugo v4](/blog/2022/07/19/centrifugo-v4-released) which makes some parts of this tutorial obsolete. The core concepts are similar though \u2013 so this can still be used as a Centrifugo learning step.\\n\\n:::\\n\\n## Application overview\\n\\nThe result will look like this:\\n\\n<video width=\\"100%\\" controls>\\n  <source src=\\"/img/laravel_chat_demo.mp4\\" type=\\"video/mp4\\" />\\n  Sorry, your browser doesn\'t support embedded video.\\n</video>\\n\\nFor the backend, we are using Laravel (version 8.65) as one of the most popular PHP frameworks. Centrifugo v3 will accept WebSocket client connections. And we will implement an integration layer between Laravel and Centrifugo.\\n\\nFor CSS styles we are using recently released Bootstrap 5. Also, some vanilla JS instead of frameworks like React/Vue/whatever to make frontend Javascript code simple \u2013 so most developers out there could understand the mechanics. \\n\\nWe are also using a bit old-fashioned server rendering here where server renders templates for different room routes (URLs) \u2013 i.e. our app is not a SPA app \u2013 mostly for the same reasons: to keep example short and let reader focus on Centrifugo and Laravel integration parts.\\n\\nTo generate fake user avatars we are requesting images from https://robohash.org/ which can generate unique robot puctures based on some input string (username in our case). Robots like to chat with each other!\\n\\n<img src=\\"https://robohash.org/1.png\\" width=\\"30%\\" />\\n<img src=\\"https://robohash.org/2.png\\" width=\\"30%\\" />\\n<img src=\\"https://robohash.org/4.png\\" width=\\"30%\\" />\\n<br /><br /><br />\\n\\n:::tip\\n\\nWe also have some ideas on further possible app improvements at the end of this post.\\n\\n:::\\n\\n## Why integrate Laravel with Centrifugo?\\n\\nWhy would Laravel developers want to integrate a project with Centrifugo for real-time messaging functionality? That\'s a good question. There are several points which could be a good motivation:\\n\\n* Centrifugo is [open-source](https://github.com/centrifugal/centrifugo) and **self-hosted**. So you can run it on your own infrastructure. Popular Laravel real-time broadcasting intergrations (Pusher and Ably) are paid cloud solutions. At scale Centrifugo will cost you less than cloud solutions. Of course cloud solutions do not require additional server setup \u2013 but everything is a trade-off right? So you should decide for youself.\\n* Centrifugo is fast and scales well. It has an optimized Redis Engine with client-side sharding and Redis Cluster support. Centrifugo can also scale with KeyDB, Nats, or Tarantool. So it\'s possible to handle millions of connections distributed over different Centrifugo nodes.\\n* Centrifugo provides a variety of features out-of-the-box \u2013 some of them are unique, especially for self-hosted real-time servers that scale to many nodes (like fast message history cache, or maintaining single user connection, both client-side and server-side subscriptions, etc).\\n* Centrifugo is lightweight, single binary server which works as a separate service \u2013 it can be a universal tool in the developer\'s pocket, can migrate with you from one project to another, no matter what programming language or framework is used for business logic.\\n\\nHope this makes sense as a good motivation to give Centrifugo a try in your Laravel project. Let\'s get started!\\n\\n## Setup and start a project\\n\\nFor the convenience of working with the example, we [wrapped the end result into docker compose](https://github.com/centrifugal/examples/blob/master/v3/php_laravel_chat_tutorial/docker-compose.yml).\\n\\nTo start the app clone [examples repo](https://github.com/centrifugal/examples), cd into `v3/php_laravel_chat_tutorial` directory and run:\\n\\n```bash\\ndocker compose up\\n```\\n\\nAt the first launch, the necessary images will be downloaded (will take some time and network bytes). When the main service is started, you should see something like this in container logs:\\n\\n```\\n...\\napp           | Database seeding completed successfully.\\napp           | [10-Dec-2021 12:25:05] NOTICE: fpm is running, pid 112\\napp           | [10-Dec-2021 12:25:05] NOTICE: ready to handle connections\\n```\\n\\nThen go to [http://localhost/](http://localhost/) \u2013 you should see:\\n\\n![Image](/img/laravel_main_page.jpg)\\n\\nRegister (using some fake credentials) or sign up \u2013 and proceed to the chat rooms.\\n\\nPay attention to the [configuration](https://github.com/centrifugal/examples/tree/master/v3/php_laravel_chat_tutorial/docker/conf) of Centrifugo and Nginx. Also, on [entrypoint](https://github.com/centrifugal/examples/blob/master/v3/php_laravel_chat_tutorial/docker/entrypoints/app.sh) which does some things:\\n\\n- dependencies are installed via composer\\n- copying settings from .env.example\\n- db migrations are performed and the necessary npm packages are installed\\n- php-fpm starts\\n\\n## Application structure\\n\\nWe assume you already familar with Laravel concepts, so we will just point you to some core aspects of the Laravel application structure and will pay more attention to Centrifugo integration parts.\\n\\n### Environment settings\\n\\nAfter the first launch of the application, all settings will be copied from the file [`.env.example`](https://github.com/centrifugal/examples/blob/master/v3/php_laravel_chat_tutorial/app/.env.example) to `.env`. Next, we will take a closer look at some settings.\\n\\n### Database migrations and models\\n\\nYou can view the database structure [here](https://github.com/centrifugal/examples/tree/master/v3/php_laravel_chat_tutorial/app/database/migrations).\\n\\nWe will use the following tables which will be then translated to the application models:\\n\\n- Laravel standard user authentication tables. See https://laravel.com/docs/8.x/authentication. In the service we are using Laravel Breeze. For more information [see official docs](https://laravel.com/docs/8.x/starter-kits#laravel-breeze).\\n- [rooms](https://github.com/centrifugal/examples/blob/master/v3/php_laravel_chat_tutorial/app/database/migrations/2021_11_21_000001_create_rooms_table.php) table. Basically - describes different rooms in the app every user can create.\\n- rooms [many-to-many relation](https://github.com/centrifugal/examples/blob/master/v3/php_laravel_chat_tutorial/app/database/migrations/2021_11_21_000002_create_users_rooms_table.php) to users. Allows to add users into rooms when `join` button clicked or automatically upon room creation.\\n- [messages](https://github.com/centrifugal/examples/blob/master/v3/php_laravel_chat_tutorial/app/database/migrations/2021_11_21_000003_create_messages_table.php). Keeps message history in rooms.\\n\\n### Broadcasting\\n\\nFor broadcasting we are using [laravel-centrifugo](https://github.com/denis660/laravel-centrifugo) library. It helps to simplify interaction between Laravel and Centrifugo by providing some convenient wrappers.\\n\\nStep-by-step configuration can be viewed in the [readme](https://github.com/denis660/laravel-centrifugo) file of this library.\\n\\nPay attention to the `CENTRIFUGO_API_KEY` setting. It is used to send API requests from Laravel to Centrifugo and must match in `.env` and `centrifugo.json` files. And we also telling `laravel-centrifugo` the URL of Centrifugo. That\'s all we need to configure for this example app.\\n\\nSee more information about Laravel broadcasting [here](https://laravel.com/docs/8.x/broadcasting).\\n\\n:::tip\\n\\nAs an alternative to `laravel-centrifugo`, you can use [phpcent](https://github.com/centrifugal/phpcent) \u2013 it\'s an official generic API client which allows publishing to Centrifugo HTTP API. But it does know nothing about Laravel broadcasting specifics.\\n\\n:::\\n\\n### Interaction with Centrifugo\\n\\nWhen user opens a chat app it connects to Centrifugo over WebSocket transport.\\n\\nLet\'s take a closer look at Centrifugo server configuration file we use for this example app:\\n\\n```json\\n{\\n  \\"port\\": 8000,\\n  \\"engine\\": \\"memory\\",\\n  \\"api_key\\": \\"some-long-api-key-which-you-should-keep-secret\\",\\n  \\"allowed_origins\\": [\\n    \\"http://localhost\\",\\n  ],\\n  \\"proxy_connect_endpoint\\": \\"http://nginx/centrifugo/connect/\\",\\n  \\"proxy_http_headers\\": [\\n    \\"Cookie\\"\\n  ],\\n  \\"namespaces\\": [\\n    {\\n      \\"name\\": \\"personal\\"\\n    }\\n  ]\\n}\\n```\\n\\nThis configuration defines a connect proxy endpoint which is targeting Nginx and then proxied to Laravel. Centrifugo will proxy `Cookie` header of WebSocket HTTP Upgrade requests to Laravel \u2013 this allows using native Laravel authentication.\\n\\nWe also defined a `\\"personal\\"` namespace \u2013 we will subscribe each user to a personal channel in this namespace inside connect proxy handler. Using namespaces for different real-time features is one of Centrifugo best-practices.\\n\\nAllowed origins must be properly set to prevent [cross-site WebSocket connection hijacking](https://christian-schneider.net/CrossSiteWebSocketHijacking.html).\\n\\n### Connect proxy controller\\n\\nTo use native Laravel user authentication middlewares, we will use [Centrifugo proxy feature](https://centrifugal.dev/docs/server/proxy).\\n\\nWhen user connects to Centrifugo it\'s connection attempt will be transformed into HTTP request from Centrifugo to Laravel and will hit the [connect proxy controller](https://github.com/centrifugal/examples/blob/master/v3/php_laravel_chat_tutorial/app/app/Http/Controllers/CentrifugoProxyController.php):\\n\\n```php\\nclass CentrifugoProxyController extends Controller\\n{\\n    public function connect()\\n    {\\n        return new JsonResponse([\\n            \'result\' => [\\n                \'user\' => (string) Auth::user()->id,\\n                \'channels\' => [\\"personal:#\\".Auth::user()->id],\\n            ]\\n        ]);\\n    }\\n}\\n```\\n\\nThis controller [protected by auth middleware](https://github.com/centrifugal/examples/blob/master/v3/php_laravel_chat_tutorial/app/routes/api.php).\\n\\nSince Centrifugo proxies `Cookie` header of initial WebSocket HTTP Upgrade request Laravel auth layer will work just fine. So in a controller you already has access to the current authenticated user.\\n\\nIn the response from controller we tell Centrifugo the ID of connecting user and subscribe user to its personal channel (using [user-limited channel](https://centrifugal.dev/docs/server/channels#user-channel-boundary-) feature of Centrifugo). Returning a channel in such way will subscribe user to it using [server-side subscriptions](https://centrifugal.dev/docs/server/server_subs) mechanism.\\n\\n:::tip\\n\\nNote, that in our chat app we are using a single personal channel for each user to receive real-time updates from all rooms. We are not creating separate subscriptions for each room user joined too. This will allow us to scale more easily in the future, and basically the only viable solution in case of room list pagination in chat application like this. It does not mean you can not combine personal user channels and separate room channels for different tasks though.\\n\\nSome additional tips can be found in [Centrifugo FAQ](https://centrifugal.dev/docs/faq/index#what-about-best-practices-with-the-number-of-channels).\\n\\n:::\\n\\n### Room controller\\n\\nIn [RoomController](https://github.com/centrifugal/examples/blob/master/v3/php_laravel_chat_tutorial/app/app/Http/Controllers/RoomController.php) we perform various actions with rooms:\\n\\n* displaying rooms\\n* create rooms\\n* join users to rooms\\n* publish messages\\n\\nWhen we publish a message in a room, we send a message to the personal channel of all users joined to the room using the [`broadcast` method of Centrifugo API](https://centrifugal.dev/docs/server/server_api#broadcast). It allows publishing the same message into many channels. \\n\\n```php\\n$message = Message::create([\\n    \'sender_id\' => Auth::user()->id,\\n    \'message\' => $requestData[\\"message\\"],\\n    \'room_id\' => $id,\\n]);\\n\\n$room = Room::with(\'users\')->find($id);\\n\\n$channels = [];\\nforeach ($room->users as $user) {\\n    $channels[] = \\"personal:#\\" . $user->id;\\n}\\n\\n$this->centrifugo->broadcast($channels, [\\n    \\"text\\" => $message->message,\\n    \\"createdAt\\" => $message->created_at->toDateTimeString(),\\n    \\"roomId\\" => $id,\\n    \\"senderId\\" => Auth::user()->id,\\n    \\"senderName\\" => Auth::user()->name,\\n]);\\n```\\n\\nWe also add some fields to the published message which will be used when dynamically displaying a message coming from a WebSocket connection (see [Client side](#client-side) below).\\n\\n### Client side\\n\\nOur chat is basically a one page with some variations dependng on the current route. So we use [a single view](https://github.com/centrifugal/examples/blob/master/v3/php_laravel_chat_tutorial/app/resources/views/rooms/index.blade.php) for the entire chat app.\\n\\nOn the page we have a form for creating rooms. The user who created the room automatically joins it upon creation. Other users need to join manually (using `join` button in the room).\\n\\nWhen sending a message (using the chat room message input), we make an AJAX request that hits `RoomController` shown above. A message saved into the database and then broadcasted to all users who joined this room. Here is a code that processes sending on ENTER:\\n\\n```js\\nmessageInput.onkeyup = function(e) {\\n    if (e.keyCode === 13) {\\n        e.preventDefault();\\n        const message = messageInput.value;\\n        if (!message) {\\n            return;\\n        }\\n        const xhttp = new XMLHttpRequest();\\n        xhttp.open(\\"POST\\", \\"/rooms/\\" + roomId + \\"/publish\\");\\n        xhttp.setRequestHeader(\\"X-CSRF-TOKEN\\", csrfToken);\\n        xhttp.send(JSON.stringify({\\n            message: message\\n        }));\\n        messageInput.value = \'\';\\n    }\\n};\\n```\\n\\nAfter the message is processed on the server and broadcasted to Centrifugo it instantly comes to client-side. To receive the message we are connecting to Centrifugo WebSocket endpoint and wait for a message in the `publish` event handler:\\n\\n```js\\nconst url = \\"ws://\\" + window.location.host + \\"/connection/websocket\\";\\nconst centrifuge = new Centrifuge(url);\\n\\ncentrifuge.on(\'connect\', function(ctx) {\\n    console.log(\\"connected to Centrifugo\\", ctx);\\n});\\n\\ncentrifuge.on(\'disconnect\', function(ctx) {\\n    console.log(\\"disconnected from Centrifugo\\", ctx);\\n});\\n\\ncentrifuge.on(\'publish\', function(ctx) {\\n    if (ctx.data.roomId.toString() === currentRoomId) {\\n        addMessage(ctx.data);\\n        scrollToLastMessage();\\n    }\\n    addRoomLastMessage(ctx.data);\\n});\\n\\ncentrifuge.connect();\\n```\\n\\nWe are using [centrifuge-js](https://github.com/centrifugal/centrifuge-js) client connector library to communicate with Centrifugo. This client abstracts away bidirectional asynchronous protocol complexity for us providing a simple way to listen connect, disconnect events and communicate with a server in various ways.\\n\\nIn publish event handler we check whether the message belongs to the room the user is currently in. If yes, then we add it to the message history of the room. We also add this message to the room in the list on the left as the last chat message in room. If necessary, we crop the text for normal display.\\n\\n:::tip\\n\\nIn our example we only subscribe each user to a single channel, but user can be subscribed to several server-side channels. To distinguish between them use `ctx.channel` inside publish event handler.\\n\\n:::\\n\\nAnd that\'s it! We went through all the main parts of the integration.\\n\\n## Possible improvements\\n\\nAs promised, here is a list with several possible app improvements:\\n\\n* Transform to a single page app, use productive Javascript frameworks like React or VueJS instead of vanilla JS.\\n* Add message read statuses - as soon as one of the chat participants read the message mark it read in the database.\\n* Introduce user-to-user chats.\\n* Support pagination for the message history, maybe for chat room list also.\\n* Don\'t show all rooms in the system \u2013 add functionality to search room by name.\\n* Horizontal scaling (using multiple nodes of Centrifugo, for example with [Redis Engine](https://centrifugal.dev/docs/server/engines#redis-engine)) \u2013 mostly one line in Centrifugo config if you have Redis running.\\n* Gracefully handle temporary disconnects by loading missed messages from the database or Centrifugo channel history cache.\\n* Optionally replace connect proxy with [JWT authentication](https://centrifugal.dev/docs/server/authentication) to reduce HTTP calls from Centrifugo to Laravel. This may drastically reduce resources for Laravel backend at scale.\\n* Try using [Centrifugo RPC proxy](https://centrifugal.dev/docs/server/proxy#rpc-proxy) feature to use WebSocket connection for message publish instead of issuing AJAX request.\\n\\n## Conclusion\\n\\nWe built a chat app with Laravel and Centrifugo. While there is still an area for improvements, this example is not really the basic. It\'s already valuable in the current form and may be transformed into part of your production system with minimal tweaks.\\n\\nHope you enjoyed this tutorial. If you have any questions after reading \u2013 join our [community channels](/docs/getting-started/introduction#join-community). We touched only part of Centrifugo concepts here \u2013 take a look at detailed Centrifugo docs nearby. And let the Centrifugal force be with you!"},{"id":"/2021/11/04/integrating-with-django-building-chat-application","metadata":{"permalink":"/blog/2021/11/04/integrating-with-django-building-chat-application","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2021-11-04-integrating-with-django-building-chat-application.md","source":"@site/blog/2021-11-04-integrating-with-django-building-chat-application.md","title":"Centrifugo integration with Django \u2013 building a basic chat application","description":"In this tutorial, we are integrating Django with Centrifugo to make a basic chat application. We are using Centrifugo proxy feature to proxy WebSocket connection events to a Django backend.","date":"2021-11-04T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"tutorial","permalink":"/blog/tags/tutorial"},{"label":"django","permalink":"/blog/tags/django"}],"readingTime":15.69,"hasTruncateMarker":true,"authors":[{"name":"Alexander Emelin","title":"Ex-Pythonista","imageURL":"https://github.com/FZambia.png"}],"frontMatter":{"title":"Centrifugo integration with Django \u2013 building a basic chat application","tags":["centrifugo","tutorial","django"],"description":"In this tutorial, we are integrating Django with Centrifugo to make a basic chat application. We are using Centrifugo proxy feature to proxy WebSocket connection events to a Django backend.","author":"Alexander Emelin","authorTitle":"Ex-Pythonista","authorImageURL":"https://github.com/FZambia.png","image":"/img/django_tutorial.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Building a multi-room chat application with Laravel and Centrifugo","permalink":"/blog/2021/12/14/laravel-multi-room-chat-tutorial"},"nextItem":{"title":"Centrifugo integration with NodeJS tutorial","permalink":"/blog/2021/10/18/integrating-with-nodejs"}},"content":"![Centrifuge](/img/django_tutorial.jpg)\\n\\nIn this tutorial, we will create a basic chat server using the [Django framework](https://www.djangoproject.com/) and [Centrifugo](https://centrifugal.dev/). Our chat application will have two pages:\\n\\n1. A page that lets you type the name of a chat room to join.\\n1. A room view that lets you see messages posted in a chat room you joined.\\n\\nThe room view will use a WebSocket to communicate with the Django server (with help from Centrifugo) and listen for any messages that are published to the room channel.\\n\\n\x3c!--truncate--\x3e\\n\\n:::caution\\n\\nThis tutorial was written for Centrifugo v3. We recently released [Centrifugo v4](/blog/2022/07/19/centrifugo-v4-released) which makes some parts of this tutorial obsolete. The core concepts are similar though \u2013 so this can still be used as a Centrifugo learning step.\\n\\n:::\\n\\nThe result will look like this:\\n\\n![demo](/img/django_chat.gif)\\n\\n:::tip\\n\\nSome of you will notice that this tutorial looks very similar to [Chat app tutorial of Django Channels](https://channels.readthedocs.io/en/stable/tutorial/index.html). This is intentional to let Pythonistas already familiar with Django Channels feel how Centrifugo compares to Channels in terms of the integration process.\\n\\n:::\\n\\n## Why integrate Django with Centrifugo\\n\\nWhy would Django developers want to integrate a project with Centrifugo for real-time messaging functionality? This is a good question especially since there is a popular Django Channels project which solves the same task.\\n\\nI found several points which could be a good motivation:\\n\\n* Centrifugo is fast and scales well. We have an optimized Redis Engine with client-side sharding and Redis Cluster support. Centrifugo can also scale with KeyDB, Nats, or Tarantool. So it\'s possible to handle millions of connections distributed over different server nodes.\\n* Centrifugo provides a variety of features out-of-the-box \u2013 some of them are unique, especially for real-time servers that scale to many nodes. Check out our doc!\\n* With Centrifugo you don\'t need to rewrite the existing application to introduce real-time messaging features to your users.\\n* Centrifugo works as a separate service \u2013 so can be a universal tool in the developer\'s pocket, can migrate from one project to another, no matter what programming language or framework is used for business logic.\\n\\n## Prerequisites\\n\\nWe assume that you are already familiar with basic Django concepts. If not take a look at the official [Django tutorial](https://docs.djangoproject.com/en/stable/intro/tutorial01/) first and then come back to this tutorial.\\n\\nAlso, make sure you read a bit about Centrifugo \u2013 [introduction](https://centrifugal.dev/docs/getting-started/introduction) and [quickstart tutorial](https://centrifugal.dev/docs/getting-started/quickstart).\\n\\nWe also assume that you have [Django installed](https://docs.djangoproject.com/en/stable/intro/install/) already.\\n\\nOne possible way to quickly install Django locally is to create virtualenv, activate it, and install Django:\\n\\n```bash\\npython3 -m venv env\\n. env/bin/activate\\npip install django\\n```\\n\\nAlos, make sure you have Centrifugo v3 [installed](/docs/getting-started/installation) already.\\n\\nThis tutorial also uses Docker to run Redis. We use Redis as a Centrifugo engine \u2013 this allows us to have a scalable solution in the end. Using Redis is optional actually, Centrifugo uses a Memory engine by default (but it does not allow scaling Centrifugo nodes). We will also run Nginx with Docker to serve the entire app. [Install Docker](https://www.docker.com/get-started) from its official website but I am sure you already have one.\\n\\n## Creating a project\\n\\nFirst, let\'s create a Django project.\\n\\nFrom the command line, `cd` into a directory where you\u2019d like to store your code, then run the following command:\\n\\n```bash\\ndjango-admin startproject mysite\\n```\\n\\nThis will create a mysite directory in your current directory with the following contents:\\n\\n```\\n\u276f tree mysite\\nmysite\\n\u251c\u2500\u2500 manage.py\\n\u2514\u2500\u2500 mysite\\n    \u251c\u2500\u2500 __init__.py\\n    \u251c\u2500\u2500 asgi.py\\n    \u251c\u2500\u2500 settings.py\\n    \u251c\u2500\u2500 urls.py\\n    \u2514\u2500\u2500 wsgi.py\\n```\\n\\n## Creating the chat app\\n\\nWe will put the code for the chat server inside `chat` app.\\n\\nMake sure you\u2019re in the same directory as `manage.py` and type this command:\\n\\n```bash\\npython3 manage.py startapp chat\\n```\\n\\nThat\u2019ll create a directory chat, which is laid out like this:\\n\\n```\\n\u276f tree chat\\nchat\\n\u251c\u2500\u2500 __init__.py\\n\u251c\u2500\u2500 admin.py\\n\u251c\u2500\u2500 apps.py\\n\u251c\u2500\u2500 migrations\\n\u2502   \u2514\u2500\u2500 __init__.py\\n\u251c\u2500\u2500 models.py\\n\u251c\u2500\u2500 tests.py\\n\u2514\u2500\u2500 views.py\\n```\\n\\nFor this tutorial, we will only be working with `chat/views.py` and `chat/__init__.py`. Feel free to remove all other files from the chat directory.\\n\\nAfter removing unnecessary files, the chat directory should look like this:\\n\\n```\\n\u276f tree chat\\nchat\\n\u251c\u2500\u2500 __init__.py\\n\u2514\u2500\u2500 views.py\\n```\\n\\nWe need to tell our project that the chat app is installed. Edit the `mysite/settings.py` file and add \'chat\' to the `INSTALLED_APPS` setting. It\u2019ll look like this:\\n\\n```python\\n# mysite/settings.py\\nINSTALLED_APPS = [\\n    \'chat\',\\n    \'django.contrib.admin\',\\n    \'django.contrib.auth\',\\n    \'django.contrib.contenttypes\',\\n    \'django.contrib.sessions\',\\n    \'django.contrib.messages\',\\n    \'django.contrib.staticfiles\',\\n]\\n```\\n\\n## Add the index view\\n\\nWe will now create the first view, an index view that lets you type the name of a chat room to join.\\n\\nCreate a templates directory in your chat directory. Within the templates directory, you have just created, create another directory called `chat`, and within that create a file called `index.html` to hold the template for the index view.\\n\\nYour chat directory should now look like this:\\n\\n```\\n\u276f tree chat\\nchat\\n\u251c\u2500\u2500 __init__.py\\n\u251c\u2500\u2500 templates\\n\u2502   \u2514\u2500\u2500 chat\\n\u2502       \u2514\u2500\u2500 index.html\\n\u2514\u2500\u2500 views.py\\n```\\n\\nPut the following code in chat/templates/chat/index.html:\\n\\n```html title=\\"chat/templates/chat/index.html\\"\\n<!DOCTYPE html>\\n<html>\\n\\n<head>\\n    <meta charset=\\"utf-8\\" />\\n    <title>Select a chat room</title>\\n</head>\\n\\n<body>\\n    <div class=\\"center\\">\\n        <div class=\\"input-wrapper\\">\\n            <input type=\\"text\\" id=\\"room-name-input\\" />\\n        </div>\\n        <div class=\\"input-help\\">\\n            Type a room name to <a id=\\"room-name-submit\\" href=\\"#\\">JOIN</a>\\n        </div>\\n    </div>\\n    <script>\\n        const nameInput = document.querySelector(\'#room-name-input\');\\n        const nameSubmit = document.querySelector(\'#room-name-submit\');\\n        nameInput.focus();\\n        nameInput.onkeyup = function (e) {\\n            if (e.keyCode === 13) {  // enter, return\\n                nameSubmit.click();\\n            }\\n        };\\n        nameSubmit.onclick = function (e) {\\n            e.preventDefault();\\n            var roomName = nameInput.value;\\n            if (!roomName) {\\n                return;\\n            }\\n            window.location.pathname = \'/chat/room/\' + roomName + \'/\';\\n        };\\n    <\/script>\\n</body>\\n\\n</html>\\n```\\n\\nCreate the view function for the room view. Put the following code in `chat/views.py`:\\n\\n```python title=\\"chat/views.py\\"\\nfrom django.shortcuts import render\\n\\ndef index(request):\\n    return render(request, \'chat/index.html\')\\n```\\n\\nTo call the view, we need to map it to a URL - and for this, we need a URLconf.\\n\\nTo create a URLconf in the chat directory, create a file called `urls.py`. Your app directory should now look like this:\\n\\n```\\n\u276f tree chat\\nchat\\n\u251c\u2500\u2500 __init__.py\\n\u251c\u2500\u2500 templates\\n\u2502   \u2514\u2500\u2500 chat\\n\u2502       \u2514\u2500\u2500 index.html\\n\u2514\u2500\u2500 views.py\\n\u2514\u2500\u2500 urls.py\\n```\\n\\nIn the `chat/urls.py` file include the following code:\\n\\n```python title=\\"chat/urls.py\\"\\nfrom django.urls import path\\n\\nfrom . import views\\n\\nurlpatterns = [\\n    path(\'\', views.index, name=\'index\'),\\n]\\n```\\n\\nThe next step is to point the root URLconf at the `chat.urls` module. In `mysite/urls.py`, add an import for `django.conf.urls.include` and insert an include() in the urlpatterns list, so you have:\\n\\n```python title=\\"mysite/urls.py\\"\\nfrom django.conf.urls import include\\nfrom django.urls import path\\nfrom django.contrib import admin\\n\\nurlpatterns = [\\n    path(\'chat/\', include(\'chat.urls\')),\\n    path(\'admin/\', admin.site.urls),\\n]\\n```\\n\\nLet\u2019s verify that the index view works. Run the following command:\\n\\n```bash\\npython3 manage.py runserver\\n```\\n\\nYou\u2019ll see the following output on the command line:\\n\\n```\\nWatching for file changes with StatReloader\\nPerforming system checks...\\n\\nSystem check identified no issues (0 silenced).\\n\\nYou have 18 unapplied migration(s). Your project may not work properly until you apply the migrations for app(s): admin, auth, contenttypes, sessions.\\nRun \'python manage.py migrate\' to apply them.\\nOctober 21, 2020 - 18:49:39\\nDjango version 3.1.2, using settings \'mysite.settings\'\\nStarting development server at http://localhost:8000/\\nQuit the server with CONTROL-C.\\n```\\n\\nGo to [http://localhost:8000/chat/](http://localhost:8000/chat/) in your browser and you should see the a text input to provide a room name.\\n\\nType in \\"lobby\\" as the room name and press Enter. You should be redirected to the room view at [http://localhost:8000/chat/room/lobby/](http://localhost:8000/chat/room/lobby/) but we haven\u2019t written the room view yet, so you\u2019ll get a \\"Page not found\\" error page.\\n\\nGo to the terminal where you ran the runserver command and press Control-C to stop the server.\\n\\n## Add the room view\\n\\nWe will now create the second view, a room view that lets you see messages posted in a particular chat room.\\n\\nCreate a new file `chat/templates/chat/room.html`. Your app directory should now look like this:\\n\\n```\\nchat\\n\u251c\u2500\u2500 __init__.py\\n\u251c\u2500\u2500 templates\\n\u2502   \u2514\u2500\u2500 chat\\n\u2502       \u251c\u2500\u2500 index.html\\n\u2502       \u2514\u2500\u2500 room.html\\n\u251c\u2500\u2500 urls.py\\n\u2514\u2500\u2500 views.py\\n```\\n\\nCreate the view template for the room view in `chat/templates/chat/room.html`:\\n\\n```html title=\\"chat/templates/chat/room.html\\"\\n<!DOCTYPE html>\\n<html>\\n\\n<head>\\n    <meta charset=\\"utf-8\\" />\\n    <title>Chat Room</title>\\n    <script src=\\"https://cdn.jsdelivr.net/gh/centrifugal/centrifuge-js@2.8.3/dist/centrifuge.min.js\\"><\/script>\\n</head>\\n\\n<body>\\n    <ul id=\\"chat-thread\\" class=\\"chat-thread\\"></ul>\\n    <div class=\\"chat-message\\">\\n        <input id=\\"chat-message-input\\" class=\\"chat-message-input\\" type=\\"text\\" autocomplete=\\"off\\" autofocus />\\n    </div>\\n    {{ room_name|json_script:\\"room-name\\" }}\\n    <script>\\n        const roomName = JSON.parse(document.getElementById(\'room-name\').textContent);\\n        const chatThread = document.querySelector(\'#chat-thread\');\\n        const messageInput = document.querySelector(\'#chat-message-input\');\\n\\n        const centrifuge = new Centrifuge(\\"ws://\\" + window.location.host + \\"/connection/websocket\\");\\n\\n        centrifuge.on(\'connect\', function (ctx) {\\n            console.log(\\"connected\\", ctx);\\n        });\\n\\n        centrifuge.on(\'disconnect\', function (ctx) {\\n            console.log(\\"disconnected\\", ctx);\\n        });\\n\\n        const sub = centrifuge.subscribe(\'rooms:\' + roomName, function (ctx) {\\n            const chatNewThread = document.createElement(\'li\');\\n            const chatNewMessage = document.createTextNode(ctx.data.message);\\n            chatNewThread.appendChild(chatNewMessage);\\n            chatThread.appendChild(chatNewThread);\\n            chatThread.scrollTop = chatThread.scrollHeight;\\n        });\\n\\n        centrifuge.connect();\\n\\n        messageInput.focus();\\n        messageInput.onkeyup = function (e) {\\n            if (e.keyCode === 13) {  // enter, return\\n                e.preventDefault();\\n                const message = messageInput.value;\\n                if (!message) {\\n                    return;\\n                }\\n                sub.publish({ \'message\': message });\\n                messageInput.value = \'\';\\n            }\\n        };\\n    <\/script>\\n</body>\\n\\n</html>\\n```\\n\\nCreate the view function for the room view in `chat/views.py`:\\n\\n```python title=\\"chat/views.py\\"\\nfrom django.shortcuts import render\\n\\n\\ndef index(request):\\n    return render(request, \'chat/index.html\')\\n\\n\\ndef room(request, room_name):\\n    return render(request, \'chat/room.html\', {\\n        \'room_name\': room_name\\n    })\\n```\\n\\nCreate the route for the room view in `chat/urls.py`:\\n\\n```python\\n# chat/urls.py\\nfrom django.urls import path, re_path\\n\\nfrom . import views\\n\\nurlpatterns = [\\n    path(\'\', views.index, name=\'index\'),\\n    re_path(\'room/(?P<room_name>[A-z0-9_-]+)/\', views.room, name=\'room\'),\\n]\\n```\\n\\nStart the development server:\\n\\n```\\npython3 manage.py runserver\\n```\\n\\nGo to [http://localhost:8000/chat/](http://localhost:8000/chat/) in your browser and to see the index page.\\n\\nType in \\"lobby\\" as the room name and press enter. You should be redirected to the room page at [http://localhost:8000/chat/lobby/](http://localhost:8000/chat/lobby/) which now displays an empty chat log.\\n\\nType the message \\"hello\\" and press Enter. Nothing happens! In particular, the message does not appear in the chat log. Why?\\n\\nThe room view is trying to open a WebSocket connection with Centrifugo using the URL `ws://localhost:8000/connection/websocket` but we haven\u2019t started Centrifugo to accept WebSocket connections yet. If you open your browser\u2019s JavaScript console, you should see an error that looks like this:\\n\\n```\\nWebSocket connection to \'ws://localhost:8000/connection/websocket\' failed\\n```\\n\\nAnd since port 8000 has already been allocated we will start Centrifugo at a different port actually.\\n\\n## Starting Centrifugo server\\n\\nAs promised we will use Centrifugo with Redis engine. So first thing to do before running Centrifugo is to start Redis:\\n\\n```bash\\ndocker run -it --rm -p 6379:6379 redis:6\\n```\\n\\nThen create a configuration file for Centrifugo:\\n\\n```json\\n{\\n    \\"port\\": 8001,\\n    \\"engine\\": \\"redis\\",\\n    \\"redis_address\\": \\"redis://localhost:6379\\",\\n    \\"allowed_origins\\": \\"http://localhost:9000\\",\\n    \\"proxy_connect_endpoint\\": \\"http://localhost:8000/chat/centrifugo/connect/\\",\\n    \\"proxy_publish_endpoint\\": \\"http://localhost:8000/chat/centrifugo/publish/\\",\\n    \\"proxy_subscribe_endpoint\\": \\"http://localhost:8000/chat/centrifugo/subscribe/\\",\\n    \\"proxy_http_headers\\": [\\"Cookie\\"],\\n    \\"namespaces\\": [\\n        {\\n            \\"name\\": \\"rooms\\",\\n            \\"publish\\": true,\\n            \\"proxy_publish\\": true,\\n            \\"proxy_subscribe\\": true\\n        }\\n    ]\\n}\\n```\\n\\nAnd run Centrifugo with it like this:\\n\\n```bash\\ncentrifugo -c config.json\\n```\\n\\nLet\'s describe some options we used here:\\n\\n* `port` - sets the port Centrifugo runs on since we are running everything on localhost we make it different (8001) from the port allocated for the Django server (8000).\\n* `engine` - as promised we are using Redis engine so we can easily scale Centrifigo nodes to handle lots of WebSocket connections\\n* `redis_address` allows setting Redis address\\n* `allowed_origins` - we will connect from `http://localhost:9000` so we need to allow it\\n* `namespaces` \u2013 we are using `rooms:` prefix when subscribing to a channel, i.e. using Centrifugo `rooms` namespace. Here we define this namespace and tell Centrifigo to proxy subscribe and publish events for channels in the namespace. \\n\\n:::tip\\n\\nIt\'s a good practice to use different namespaces in Centrifugo for different real-time features as this allows enabling only required options for a specific task. \\n\\n:::\\n\\nAlso, config has some options related to [Centrifugo proxy feature](/docs/server/proxy). This feature allows proxying WebSocket events to the configured endpoints. We will proxy three types of events:\\n\\n1. Connect (called when a user establishes WebSocket connection with Centrifugo)\\n1. Subscribe (called when a user wants to subscribe on a channel)\\n1. Publish (called when a user tries to publish data to a channel)\\n\\n## Adding Nginx\\n\\nIn Centrifugo config we set endpoints which we will soon implement inside our Django app. You may notice that the allowed origin has a URL with port `9000`. That\'s because we want to proxy Cookie headers from a persistent connection established with Centrifugo to the Django app and need Centrifugo and Django to share the same origin (so browsers can send Django session cookies to Centrifugo).\\n\\nWhile not used in this tutorial (we will use fake `tutorial-user` as user ID here) \u2013 this can be useful if you decide to authenticate connections using Django native sessions framework later. To achieve this we should also add Nginx with a configuration like this:\\n\\n```text title=\\"nginx.conf\\"\\nevents {\\n    worker_connections 1024;\\n}\\n\\nerror_log /dev/stdout info;\\n\\nhttp {\\n    access_log /dev/stdout;\\n\\n    server {\\n        listen 9000;\\n\\n        server_name localhost;\\n\\n        location / {\\n            proxy_pass http://host.docker.internal:8000;\\n            proxy_http_version 1.1;\\n            proxy_set_header Host $host;\\n            proxy_set_header X-Real-IP $remote_addr;\\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n            proxy_set_header X-Forwarded-Proto $scheme;\\n        }\\n\\n        location /connection/websocket {\\n            proxy_pass http://host.docker.internal:8001;\\n            proxy_http_version 1.1;\\n            proxy_buffering off;\\n            keepalive_timeout 65;\\n            proxy_read_timeout 60s;\\n            proxy_set_header Upgrade $http_upgrade;\\n            proxy_set_header Connection \'upgrade\';\\n            proxy_set_header Host $host;\\n            proxy_set_header X-Real-IP $remote_addr;\\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n            proxy_set_header X-Forwarded-Proto $scheme;\\n            proxy_cache_bypass $http_upgrade;\\n        }\\n    }\\n}\\n```\\n\\nStart Nginx (replace the path to `nginx.conf` to yours):\\n\\n```bash\\ndocker run -it --rm -v /path/to/nginx.conf:/etc/nginx/nginx.conf:ro -p 9000:9000 --add-host=host.docker.internal:host-gateway nginx\\n```\\n\\nNote that we are exposing port 9000 to localhost and use a possibility to use `host.docker.internal` host to communicate from inside Docker network with services which are running on localhost (on the host machine). See [this answer on SO](https://stackoverflow.com/questions/31324981/how-to-access-host-port-from-docker-container).\\n\\nOpen [http://localhost:9000](http://localhost:9000). Nginx should now properly proxy requests to Django server and to Centrifugo, but we still need to do some things.\\n\\n## Implementing proxy handlers\\n\\nWell, now if you try to open a chat page with Nginx, Centrifugo, Django, and Redis running you will notice some errors in Centrifugo logs. That\'s because Centrifugo tries to proxy WebSocket connect events to Django to authenticate them but we have not created event handlers in Django yet. Let\'s fix this.\\n\\nExtend chat/urls.py:\\n\\n```python title=\\"chat/urls.py\\"\\nfrom django.urls import path, re_path\\n\\nfrom . import views\\n\\nurlpatterns = [\\n    path(\'\', views.index, name=\'index\'),\\n    re_path(\'room/(?P<room_name>[A-z0-9_-]+)/\', views.room, name=\'room\'),\\n    path(\'centrifugo/connect/\', views.connect, name=\'connect\'),\\n    path(\'centrifugo/subscribe/\', views.subscribe, name=\'subscribe\'),\\n    path(\'centrifugo/publish/\', views.publish, name=\'publish\'),\\n]\\n```\\n\\nExtend chat/views.py:\\n\\n```python title=\\"chat/views.py\\"\\nfrom django.http import JsonResponse\\nfrom django.views.decorators.csrf import csrf_exempt\\n\\n@csrf_exempt\\ndef connect(request):\\n    # In connect handler we must authenticate connection.\\n    # Here we return a fake user ID to Centrifugo to keep tutorial short.\\n    # More details about connect result format can be found in proxy docs:\\n    # https://centrifugal.dev/docs/server/proxy#connect-proxy\\n    logger.debug(request.body)\\n    response = {\\n        \'result\': {\\n            \'user\': \'tutorial-user\'\\n        }\\n    }\\n    return JsonResponse(response)\\n\\n@csrf_exempt\\ndef publish(request):\\n    # In publish handler we can validate publication request initialted by a user.\\n    # Here we return an empty object \u2013 thus allowing publication.\\n    # More details about publish result format can be found in proxy docs:\\n    # https://centrifugal.dev/docs/server/proxy#publish-proxy\\n    response = {\\n        \'result\': {}\\n    }\\n    return JsonResponse(response)\\n\\n@csrf_exempt\\ndef subscribe(request):\\n    # In subscribe handler we can validate user subscription request to a channel.\\n    # Here we return an empty object \u2013 thus allowing subscription.\\n    # More details about subscribe result format can be found in proxy docs:\\n    # https://centrifugal.dev/docs/server/proxy#subscribe-proxy\\n    response = {\\n        \'result\': {}\\n    }\\n    return JsonResponse(response)        \\n```\\n\\n`connect` view will accept all connections and return user ID as `tutorial-user`. In real app you most probably want to use Django sessions and return real authenticated user ID instead of `tutorial-user`. Since we told Centrifugo to proxy connection `Cookie` headers native Django user authentication will work just fine. \\n\\nRestart Django and try the chat app again. You should now successfully connect. Open a browser tab to the room page at [http://localhost:9000/chat/room/lobby/](http://localhost:9000/chat/room/lobby/). Open a second browser tab to the same room page.\\n\\nIn the second browser tab, type the message \\"hello\\" and press Enter. You should now see \\"hello\\" echoed in the chat log in both the second browser tab and in the first browser tab.\\n\\nYou now have a basic fully-functional chat server!\\n\\n## What could be improved\\n\\nThe list is large, but it\'s fun to do. To name some possible improvements:\\n\\n* Replace `tutorial-user` used here with native Django session framework. We already proxying the `Cookie` header to Django from Centrifugo, so you can reuse native Django authentication. Only allow authenticated users to join rooms.\\n* Create `Room` model and add users to it \u2013 thus you will be able to check permissions inside subscribe and publish handlers.\\n* Create `Message` model to display chat history in `Room`.\\n* Replace Django devserver with something more suitable for production like [Gunicorn](https://gunicorn.org/).\\n* Check out Centrifugo possibilities like presence to display online users.\\n* Use [cent](https://github.com/centrifugal/cent) Centrifugo HTTP API library to publish something to a user on behalf of a server. In this case you can avoid using publish proxy, publish messages to Django over convinient AJAX call - and then call Centrifugo HTTP API to publish message into a channel.\\n* You can replace connect proxy (which is an HTTP call from Centrifugo to Django on each connect) with JWT authentication. JWT authentication may result in a better application performance (since no additional proxy requests will be issued on connect). It can allow your Django app to handle millions of users on a reasonably small hardware and survive mass reconnects from all those users. More details can be found in [Scaling WebSocket in Go and beyond](https://centrifugal.dev/blog/2020/11/12/scaling-websocket) blog post.\\n* Instead of using subscribe proxy you can put channel into connect proxy result or into JWT \u2013 thus using [server-side subscriptions](/docs/server/server_subs) and avoid subscribe proxy HTTP call.\\n\\nOne more thing I\'d like to note is that if you aim to build a chat application like WhatsApp or Telegram where you have a screen with list of chats (which can be pretty long!) you should not create a separate channel for each room. In this case using separate channel per room does not scale well and you better use personal channel for each user to receive all user-related messages. And as soon as message published to a chat you can send message to each participant\'s channel. In this case, take a look at Centrifugo [broadcast API](/docs/server/server_api#broadcast).\\n\\n## Tutorial source code with docker-compose\\n\\nThe full example which can run by issuing a single `docker compose up` [can be found on Github](https://github.com/centrifugal/examples/tree/master/v3/python_django_chat_tutorial). It also has some CSS styles so that the chat looks like shown in the beginning.\\n\\n## Conclusion\\n\\nHere we implemented a basic chat app with Django and Centrifugo.\\n\\nWhile a chat still requires work to be suitable for production this example can help understand core concepts of Centrifugo - specifically channel namespaces and proxy features.\\n\\nIt\'s possible to use unidirectional Centrifugo transports instead of bidirectional WebSocket used here \u2013 in this case, you can go without using `centrifuge-js` at all.\\n\\nCentrifugo scales perfectly if you need to handle more connections \u2013 thanks to Centrifugo built-in PUB/SUB engines.\\n\\nIt\'s also possible to use server-side subscriptions, keep channel history cache, use JWT authentication instead of connect proxy, enable channel presence, and more. All the power of Centrifugo is in your hands.\\n\\nHope you enjoyed this tutorial. And let the Centrifugal force be with you!\\n\\nJoin our [community channels](/docs/getting-started/introduction#join-community) in case of any questions left after reading this."},{"id":"/2021/10/18/integrating-with-nodejs","metadata":{"permalink":"/blog/2021/10/18/integrating-with-nodejs","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2021-10-18-integrating-with-nodejs.md","source":"@site/blog/2021-10-18-integrating-with-nodejs.md","title":"Centrifugo integration with NodeJS tutorial","description":"In this tutorial we are integrating Centrifugo with NodeJS. We are using Centrifugo connect proxy feature to authenticate connections over standard Express.js session middleware.","date":"2021-10-18T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"tutorial","permalink":"/blog/tags/tutorial"},{"label":"proxy","permalink":"/blog/tags/proxy"}],"readingTime":6.675,"hasTruncateMarker":true,"authors":[{"name":"Alexander Emelin","title":"Creator of Centrifugo","imageURL":"https://github.com/FZambia.png"}],"frontMatter":{"title":"Centrifugo integration with NodeJS tutorial","tags":["centrifugo","tutorial","proxy"],"description":"In this tutorial we are integrating Centrifugo with NodeJS. We are using Centrifugo connect proxy feature to authenticate connections over standard Express.js session middleware.","author":"Alexander Emelin","authorTitle":"Creator of Centrifugo","authorImageURL":"https://github.com/FZambia.png","image":"/img/keyboard_thumb.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Centrifugo integration with Django \u2013 building a basic chat application","permalink":"/blog/2021/11/04/integrating-with-django-building-chat-application"},"nextItem":{"title":"Centrifugo v3 released","permalink":"/blog/2021/08/31/hello-centrifugo-v3"}},"content":"![Centrifuge](/img/keyboard.png)\\n\\nCentrifugo is a scalable real-time messaging server in a language-agnostic way. In this tutorial we will integrate Centrifugo with NodeJS backend using a connect proxy feature of Centrifugo for user authentication and native session middleware of ExpressJS framework.\\n\\nWhy would NodeJS developers want to integrate a project with Centrifugo? This is a good question especially since there are lots of various tools for real-time messaging available in NodeJS ecosystem.\\n\\n\x3c!--truncate--\x3e\\n\\n:::caution\\n\\nThis tutorial was written for Centrifugo v3. We recently released [Centrifugo v4](/blog/2022/07/19/centrifugo-v4-released) which makes some parts of this tutorial obsolete. The core concepts are similar though \u2013 so this can still be used as a Centrifugo learning step.\\n\\n:::\\n\\nI found several points which could be a good motivation:\\n\\n* Centrifugo scales well \u2013 we have a very optimized Redis Engine with client-side sharding and Redis Cluster support. We can also scale with KeyDB, Nats, or Tarantool. Centrifugo can scale to millions connections distributed over different server nodes.\\n* Centrifugo is pretty fast (written in Go) and can handle thousands of clients per node. Client protocol is optimized for thousands of messages per second.\\n* Centrifugo provides a variety of features out-of-the-box \u2013 some of them are unique, especially for real-time servers that scale to many nodes.\\n* Centrifugo works as a separate service \u2013 so can be a universal tool in developer\'s pocket, can migrate from one project to another, no matter what programming language or framework is used for a business logic.\\n\\nHaving said this all \u2013 let\'s move to a tutorial itself.\\n\\n## What we are building\\n\\nNot a super-cool app to be honest. Our goal here is to give a reader an idea how integration with Centrifugo could look like. There are many possible apps which could be built on top of this knowledge.\\n\\nThe end result here will allow application user to authenticate and once authenticated \u2013 connect to Centrifugo. Centrifugo will proxy connection requests to NodeJS backend and native ExpressJS session middleware will be used for connection authentication. We will also send some periodical real-time messages to a user personal channel.\\n\\nThe [full source code of this tutorial](https://github.com/centrifugal/examples/tree/master/v3/nodejs_proxy) located on Github. You can clone examples repo and run this demo by simply writing:\\n\\n```bash\\ndocker compose up\\n```\\n\\n## Creating Express.js app\\n\\nStart new NodeJS app:\\n\\n```bash\\nnpm init\\n```\\n\\nInstall dependencies:\\n\\n```bash\\nnpm install express express-session cookie-parser axios morgan\\n```\\n\\nCreate `index.js` file.\\n\\n```javascript title=\\"index.js\\"\\nconst express = require(\'express\');\\nconst cookieParser = require(\\"cookie-parser\\");\\nconst sessions = require(\'express-session\');\\nconst morgan = require(\'morgan\');\\nconst axios = require(\'axios\');\\n\\nconst app = express();\\nconst port = 3000;\\napp.use(express.json());\\n\\nconst oneDay = 1000 * 60 * 60 * 24;\\n\\napp.use(sessions({\\n  secret: \\"this_is_my_secret_key\\",\\n  saveUninitialized: true,\\n  cookie: { maxAge: oneDay },\\n  resave: false\\n}));\\napp.use(cookieParser());\\napp.use(express.urlencoded({ extended: true }))\\napp.use(express.json())\\napp.use(express.static(\'static\'));\\napp.use(morgan(\'dev\'));\\n\\napp.get(\'/\', (req, res) => {\\n  if (req.session.userid) {\\n    res.sendFile(\'views/app.html\', { root: __dirname });\\n  } else\\n    res.sendFile(\'views/login.html\', { root: __dirname })\\n});\\n\\napp.listen(port, () => {\\n  console.log(`Example app listening at http://localhost:${port}`);\\n});\\n```\\n\\nCreate `login.html` file in `views` folder:\\n\\n```html title=\\"views/login.html\\"\\n<html>\\n\\n<body>\\n    <form action=\\"/login\\" method=\\"post\\">\\n        <h2>Login (username: demo-user, password: demo-pass)</h2>\\n        <div class=\\"input-field\\">\\n            <input type=\\"text\\" name=\\"username\\" id=\\"username\\" placeholder=\\"Enter Username\\">\\n        </div>\\n        <div class=\\"input-field\\">\\n            <input type=\\"password\\" name=\\"password\\" id=\\"password\\" placeholder=\\"Enter Password\\">\\n        </div>\\n        <input type=\\"submit\\" value=\\"Log in\\">\\n    </form>\\n</body>\\n\\n</html>\\n```\\n\\nAlso create `app.html` file in `views` folder:\\n\\n```html title=\\"views/app.html\\"\\n<html>\\n\\n<head>\\n  <link rel=\\"stylesheet\\" href=\\"app.css\\">\\n  <script src=\\"https://cdn.jsdelivr.net/gh/centrifugal/centrifuge-js@2.8.3/dist/centrifuge.min.js\\"><\/script>\\n</head>\\n\\n<body>\\n  <div>\\n    <a href=\'/logout\'>Click to logout</a>\\n  </div>\\n  <div id=\\"log\\"></div>\\n</body>\\n\\n</html>\\n```\\n\\nMake attention that we import `centrifuge-js` client here which abstracts away Centrifugo bidirectional WebSocket protocol.\\n\\nLet\'s write an HTTP handler for login form:\\n\\n```javascript title=\\"index.js\\"\\nconst myusername = \'demo-user\'\\nconst mypassword = \'demo-pass\'\\n\\napp.post(\'/login\', (req, res) => {\\n  if (req.body.username == myusername && req.body.password == mypassword) {\\n    req.session.userid = req.body.username;\\n    res.redirect(\'/\');\\n  } else {\\n    res.send(\'Invalid username or password\');\\n  }\\n});\\n```\\n\\nIn this example we use hardcoded username and password for out single user. Of course in real app you will have a database with user credentials. But since our goal is only show integration with Centrifugo \u2013 we are skipping these hard parts here.\\n\\nAlso create a handler for a logout request:\\n\\n```javascript title=\\"index.js\\"\\napp.get(\'/logout\', (req, res) => {\\n  req.session.destroy();\\n  res.redirect(\'/\');\\n});\\n```\\n\\nNow if you run an app with `node index.js` you will see a login form using which you can authenticate. At this point this is a mostly convenient NodeJS application, let\'s add Centrifugo integration. \\n\\n## Starting Centrifugo\\n\\nRun Centrifugo with `config.json` like this:\\n\\n```json title=\\"config.json\\"\\n{\\n  \\"token_hmac_secret_key\\": \\"secret\\",\\n  \\"admin\\": true,\\n  \\"admin_password\\": \\"password\\",\\n  \\"admin_secret\\": \\"my_admin_secret\\",\\n  \\"api_key\\": \\"my_api_key\\",\\n  \\"allowed_origins\\": [\\n    \\"http://localhost:9000\\"\\n  ],\\n  \\"user_subscribe_to_personal\\": true,\\n  \\"proxy_connect_endpoint\\": \\"http://localhost:3000/centrifugo/connect\\",\\n  \\"proxy_http_headers\\": [\\n    \\"Cookie\\"\\n  ]\\n}\\n```\\n\\nI.e.:\\n\\n```\\n./centrifugo -c config.json\\n```\\n\\nCreate `app.js` file in `static` folder:\\n\\n```javascript title=\\"static/app.js\\"\\nfunction drawText(text) {\\n    const div = document.createElement(\'div\');\\n    div.innerHTML = text;\\n    document.getElementById(\'log\').appendChild(div);\\n}\\n\\nconst centrifuge = new Centrifuge(\'ws://localhost:9000/connection/websocket\');\\n\\ncentrifuge.on(\'connect\', function () {\\n    drawText(\'Connected to Centrifugo\');\\n});\\n\\ncentrifuge.on(\'disconnect\', function () {\\n    drawText(\'Disconnected from Centrifugo\');\\n});\\n\\ncentrifuge.on(\'publish\', function (ctx) {\\n    drawText(\'Publication, time = \' + ctx.data.time);\\n});\\n\\ncentrifuge.connect();\\n```\\n\\n## Adding Nginx\\n\\nSince we are going to use native session auth of ExpressJS we can\'t just connect from localhost:3000 (where our NodeJS app is served) to Centrifugo running on localhost:8000 \u2013 browser won\'t send a `Cookie` header to Centrifugo in this case. Due to this reason we need a reverse proxy which will terminate a traffic from frontend and proxy requests to NodeJS process or to Centrifugo depending on URL path. In this case both browser and NodeJS app will share the same origin \u2013 so Cookie will be sent to Centrifugo in WebSocket Upgrade request.\\n\\n:::tip\\n\\nAlternatively, we could also use [JWT authentication](/docs/server/authentication) of Centrifugo but that\'s a topic for another tutorial. Here we are using [connect proxy feature](/docs/server/proxy#connect-proxy) for auth. \\n\\n:::\\n\\nNginx config will look like this:\\n\\n```\\nserver {\\n  listen 9000;\\n\\n  server_name localhost;\\n\\n  location / {\\n    proxy_pass http://localhost:3000;\\n    proxy_http_version 1.1;\\n    proxy_set_header Host $host;\\n    proxy_set_header X-Real-IP $remote_addr;\\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n    proxy_set_header X-Forwarded-Proto $scheme;\\n  }\\n\\n  location /connection {\\n    proxy_pass http://localhost:8000;\\n    proxy_http_version 1.1;\\n    proxy_set_header Upgrade $http_upgrade;\\n    proxy_set_header Connection \'upgrade\';\\n    proxy_set_header Host $host;\\n    proxy_set_header X-Real-IP $remote_addr;\\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n    proxy_set_header X-Forwarded-Proto $scheme;\\n    proxy_cache_bypass $http_upgrade;\\n  }\\n}\\n```\\n\\nRun Nginx and open [http://localhost:9000](http://localhost:9000). After authenticating in app you should see an attempt to connect to a WebSocket endpoint. But connection will fail since we need to implement connect proxy handler in NodeJS app.\\n\\n```javascript title=\\"index.js\\"\\napp.post(\'/centrifugo/connect\', (req, res) => {\\n  if (req.session.userid) {\\n    res.json({\\n      result: {\\n        user: req.session.userid\\n      }\\n    });\\n  } else\\n    res.json({\\n      disconnect: {\\n        code: 1000,\\n        reason: \\"unauthorized\\",\\n        reconnect: false\\n      }\\n    });\\n});\\n```\\n\\nRestart NodeJS process and try opening an app again. Application should now successfully connect to Centrifugo.\\n\\n## Send real-time messages\\n\\nLet\'s also periodically publish current server time to a client\'s personal channel. In Centrifugo configuration we set a `user_subscribe_to_personal` option which turns on [automatic subscription to a personal channel](/docs/server/server_subs#automatic-personal-channel-subscription) for each connected user. We can use `axios` library and send publish API requests to Centrifugo periodically (according to [API docs](/docs/server/server_api#http-api)): \\n\\n```javascript title=\\"index.js\\"\\nconst centrifugoApiClient = axios.create({\\n  baseURL: `http://centrifugo:8000/api`,\\n  headers: {\\n    Authorization: `apikey my_api_key`,\\n    \'Content-Type\': \'application/json\',\\n  },\\n});\\n\\nsetInterval(async () => {\\n  try {\\n    await centrifugoApiClient.post(\'\', {\\n      method: \'publish\',\\n      params: {\\n        channel: \'#\' + myusername, // construct personal channel name.\\n        data: {\\n          time: Math.floor(new Date().getTime() / 1000),\\n        },\\n      },\\n    });\\n  } catch (e) {\\n    console.error(e.message);\\n  }\\n}, 5000);\\n```\\n\\nAfter restarting NodeJS you should see periodical updates on application web page.\\n\\nYou can also log in into Centrifugo admin web UI [http://localhost:8000](http://localhost:8000) using password `password` - and play with other available server API from within web interface.\\n\\n## Conclusion\\n\\nWhile not being super useful this example can help understanding core concepts of Centrifugo - specifically connect proxy feature and server API.\\n\\nIt\'s possible to use unidirectional Centrifugo transports instead of bidrectional WebSocket used here \u2013 in this case you can go without using `centrifuge-js` at all.\\n\\nThis application scales perfectly if you need to handle more connections \u2013 thanks to Centrifugo builtin PUB/SUB engines.\\n\\nIt\'s also possible to use client-side subscriptions, keep channel history cache, enable channel presence and more. All the power of Centrifugo is in your hands."},{"id":"/2021/08/31/hello-centrifugo-v3","metadata":{"permalink":"/blog/2021/08/31/hello-centrifugo-v3","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2021-08-31-hello-centrifugo-v3.md","source":"@site/blog/2021-08-31-hello-centrifugo-v3.md","title":"Centrifugo v3 released","description":"Centrifugo v3 released with lots of exciting improvements","date":"2021-08-31T00:00:00.000Z","tags":[{"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"label":"release","permalink":"/blog/tags/release"}],"readingTime":14.055,"hasTruncateMarker":true,"authors":[{"name":"Centrifugal team","title":"Let the Centrifugal force be with you","imageURL":"/img/logo_animated.svg"}],"frontMatter":{"title":"Centrifugo v3 released","tags":["centrifugo","release"],"description":"Centrifugo v3 released with lots of exciting improvements","author":"Centrifugal team","authorTitle":"Let the Centrifugal force be with you","authorImageURL":"/img/logo_animated.svg","image":"/img/v3_blog.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Centrifugo integration with NodeJS tutorial","permalink":"/blog/2021/10/18/integrating-with-nodejs"},"nextItem":{"title":"Centrifuge \u2013 real-time messaging with Go","permalink":"/blog/2021/01/15/centrifuge-intro"}},"content":"![Centrifuge](/img/v3_blog.jpg)\\n\\nAfter almost three years of Centrifugo v2 life cycle we are happy to announce the next major release of Centrifugo. During the last several months deep in our Centrifugal laboratory we had been synthesizing an improved version of the server.\\n\\nNew Centrifugo v3 is targeting to improve Centrifugo adoption for basic real-time application cases, improves server performance and extends existing features with new functionality. It comes with unidirectional real-time transports, protocol speedups, super-fast engine implementation based on Tarantool, new documentation site, GRPC proxy, API extensions and PRO version which provides unique possibilities for business adopters.\\n\\n\x3c!--truncate--\x3e\\n\\n### Centrifugo v2 flashbacks\\n\\nCentrifugo v2 life cycle has come to an end. Before discussing v3 let\'s look back at what has been done during the last three years.\\n\\nCentrifugo v2 was a pretty huge refactoring of v1. Since the v2 release, Centrifugo is built on top of  new [Centrifuge library](https://github.com/centrifugal/centrifuge) for Go language. Centrifuge library evolved significantly since its initial release and now powers Grafana v8 real-time streaming among other things.\\n\\nHere is an awesome demo made by my colleague <a href=\\"https://github.com/alexanderzobnin\\">Alexander Zobnin</a> that demonstrates real-time telemetry of Assetto Corsa sports car streamed in real-time to Grafana dashboard: \\n\\n<div class=\\"vimeo-full-width\\">\\n   <iframe src=\\"https://player.vimeo.com/video/570333329?title=0&byline=0&portrait=0\\" frameBorder=\\"0\\" allow=\\"autoplay; fullscreen\\" allowFullScreen></iframe>\\n</div>\\n<p></p>\\n\\nCentrifugo integrated with Redis Streams, got Redis Cluster support, can now work with Nats server as a PUB/SUB broker. Notable additions of Centrifugo v2 were [server-side subscriptions](/docs/server/server_subs) with some interesting features on top \u2013 like maintaining a single global connection from one user and automatic personal channel subscription upon user connect.\\n\\nA very good addition which increased Centrifugo adoption a lot was introduction of [proxy to backend](/docs/server/proxy). This made Centrifugo fit many setups where JWT authentication and existing subscription permission model did not suit well before.\\n\\nClient ecosystem improved significantly. The fact that client protocol migrated to a strict Protobuf schema allowed to introduce binary protocol format (in addition to JSON) and simplify building client connectors. We now have much better and complete client libraries (compared to v1 situation).\\n\\nWe also have an [official Helm chart](https://github.com/centrifugal/helm-charts), [Grafana dashboard](https://grafana.com/grafana/dashboards/13039) for Prometheus datasource, and so on.\\n\\n![](https://grafana.com/api/dashboards/13039/images/8950/image)\\n\\nCentrifugo is becoming more noticeable in a wider real-time technology community. For example, it was included in a [periodic table of real-time](https://ably.com/periodic-table-of-realtime) created by Ably.com (one of the most powerful real-time messaging cloud services at the moment):\\n\\n![](https://ik.imagekit.io/ably/ghost/prod/2021/08/periodic-table-screenshots-combined-without-banner-no-legend.jpg?tr=w-1520)\\n\\nOf course, there are many aspects where Centrifugo can be improved. And v3 addresses some of them. Below we will look at the most notable features and changes of the new major Centrifugo version.\\n\\n### Backwards compatibility\\n\\nLet\'s start with the most important thing \u2013 backwards compatibility concerns.\\n\\nIn Centrifugo v3 client protocol mostly stayed the same. We expect that most applications will be able to update without any change on a client-side. This was an important concern for v3 given how painful the update cycle can be on mobile devices and lessons learned from v1 to v2 migration. There is one breaking change though which can affect users who use history API manually from a client-side (we provide a temporary workaround to give apps a chance to migrate smoothly).\\n\\nOn a server-side, much more changes happened, especially in the configuration: some options were renamed, some were removed. We provide a [v2 to v3 configuration converter](/docs/3/getting-started/migration_v3#v2-to-v3-config-converter) which can help dealing with changes. In most cases, all you should do is adapt Centrifugo configuration to match v3 changes and redeploy Centrifugo using v3 build instead of v2. All features are still there (or a replacement exists, like for `channels` API).\\n\\nFor more details, refer to the [v3 migration guide](/docs/3/getting-started/migration_v3).\\n\\n### License change\\n\\nAs some of you know we considered changing Centrifugo license to AGPL v3 for a new release. After thinking a lot about this we decided to not step into this area.\\n\\nBut the license has been changed: the license of OSS Centrifugo is now Apache 2.0 instead of MIT. Apache 2.0 is also a permissive OSS license, it\'s just a bit more concrete in some aspects.\\n\\n![](https://user-images.githubusercontent.com/2097922/91162089-8570e100-e6c3-11ea-8c41-cd8fcfe049d0.png)\\n\\n### Unidirectional real-time transports\\n\\nServer-side subscriptions introduced in Centrifugo v2 and recent improvements in the underlying Centrifuge library opened a road for a unidirectional approach.\\n\\nThis means that Centrifugo v3 provides a set of unidirectional real-time transports where messages flow only in one direction \u2013 from a server to a client. Why is this change important?\\n\\nCentrifugo originally concentrated on using bidirectional transports for client-server communication. Like WebSocket and SockJS. Bidirectional transports allow implementing some great protocol features since a client can communicate with a server in various ways after establishing a persistent connection. While this is a great opportunity this also leads to an increased complexity.\\n\\nCentrifugo users had to use special client connector libraries which abstracted underlying work into a simple public API. But internally connectors do many things: matching requests to responses, handling timeouts, handling an ordering, queuing operations, error handling. So the client connector is a pretty complex piece of software.\\n\\nBut what if a user just needs to receive real-time updates from a stable set of channels known in connection time? Can we simplify everything and avoid using custom software on a client-side?\\n\\nWith unidirectional transports, the answer is yes. Clients can now connect to Centrifugo using a bunch of unidirectional transports. And the greatest thing is that in this case, developers should not depend on Centrifugo client connectors at all \u2013 just use native browser APIs or GRPC-generated code. It\'s finally possible to consume events from Centrifugo using CURL (see [an example](/docs/transports/uni_http_stream#connecting-using-curl)).\\n\\nUsing unidirectional transports you can still benefit from Centrifugo built-in scalability with various engines, utilize built-in authentication over JWT or the connect proxy feature.\\n\\nWith subscribe server API (see below) it\'s even possible to subscribe unidirectional client to server-side channels dynamically. With refresh server API or the refresh proxy feature it\'s possible to manage a connection expiration.\\n\\nCentrifugo supports the following unidirectional transports:\\n\\n* [EventSource (SSE)](/docs/transports/uni_sse)\\n* [HTTP streaming](/docs/transports/uni_http_stream)\\n* [Unidirectional WebSocket](/docs/transports/uni_websocket)\\n* [Unidirectional GRPC stream](/docs/transports/uni_grpc)\\n\\nWe expect that introducing unidirectional transports will significantly increase Centrifugo adoption.\\n\\n### History iteration API\\n\\n<img src=\\"/img/centrifuge.svg\\" align=\\"right\\" width=\\"25%\\" />\\n\\nThere was a rather important limitation of Centrifugo history API \u2013 it was not very suitable for keeping large streams because a call to a history could only return the entire channel history.\\n\\nCentrifugo v3 introduces an API to iterate over a stream. It\'s possible to do from the current stream beginning or end, in both directions \u2013 forward and backward, with configured limit. Also with certain starting stream position if it\'s known.\\n\\nThis, among other things, can help to implement manual missed message recovery on a client-side to reduce the load on the application backend.\\n\\nHere is an example program in Go which endlessly iterates over stream both ends (using [gocent](https://github.com/centrifugal/gocent) API library), upon reaching the end of stream the iteration goes in reversed direction (not really useful in real world but fun): \\n\\n```go\\n// Iterate by 10.\\nlimit := 10\\n// Paginate in reversed order first, then invert it.\\nreverse := true\\n// Start with nil StreamPosition, then fill it with value while paginating.\\nvar sp *gocent.StreamPosition\\n\\nfor {\\n\\thistoryResult, err = c.History(\\n        ctx,\\n        channel,\\n\\t\\tgocent.WithLimit(limit),\\n\\t\\tgocent.WithReverse(reverse),\\n        gocent.WithSince(sp),\\n\\t)\\n\\tif err != nil {\\n\\t\\tlog.Fatalf(\\"Error calling history: %v\\", err)\\n\\t}\\n\\tfor _, pub := range historyResult.Publications {\\n\\t\\tlog.Println(pub.Offset, \\"=>\\", string(pub.Data))\\n\\t\\tsp = &gocent.StreamPosition{\\n\\t\\t\\tOffset: pub.Offset,\\n\\t\\t\\tEpoch:  historyResult.Epoch,\\n\\t\\t}\\n\\t}\\n\\tif len(historyResult.Publications) < limit {\\n\\t\\t// Got all pubs, invert pagination direction.\\n\\t\\treverse = !reverse\\n\\t\\tlog.Println(\\"end of stream reached, change iteration direction\\")\\n\\t}\\n}\\n```\\n\\n:::caution\\n\\nThis new API does not remove the need in having the main application database \u2013 that\'s still mandatory for idiomatic Centrifugo usage.\\n\\n:::\\n\\n### Redis Streams by default\\n\\nIn Centrifugo v3 Redis engine uses Redis Stream data structure by default for keeping channel history. Before v3 Redis Streams were supported by not enabled by default so almost nobody used them. This change is important in terms of introducing history iteration API described above \u2013 since Redis Streams allow doing iteration effectively. \\n\\n### Tarantool engine\\n\\nAs you may know, Centrifugo has several built-in engines that allow scaling Centrifugo nodes (using PUB/SUB) and keep shared history and presence state. Before v3 Centrifugo had in-memory and Redis (or KeyDB) engines available.\\n\\nIntroducing a new engine to Centrifugo is pretty hard since the engine should provide a very robust PUB/SUB performance, fast history and presence operations, possibility to publish a message to PUB/SUB and save to history atomically. It also should allow dealing with ephemeral frequently changing subscriptions. It\'s typical for Centrifugo use case to have millions of users each subscribed to a  unique channel and constantly connecting/disconnecting (thus subscribing/unsubscribing).\\n\\n![](https://www.tadviser.ru/images/thumb/1/1a/Tarantool_%D0%A1%D0%A3%D0%91%D0%94_logo_2020.png/840px-Tarantool_%D0%A1%D0%A3%D0%91%D0%94_logo_2020.png)\\n\\nIn v3 we added **experimental** support for the [Tarantool](https://www.tarantool.io/en/) engine. It fits nicely all the requirements above and provides a huge performance speedup for history and presence operations compared to Redis. According to our benchmarks, the speedup can be up to 4-10x depending on operation. The PUB/SUB performance of Tarantool is comparable with Redis (10-20% worse according to our internal benchmarks to be exact, but that\'s pretty much the same).\\n\\nFor example, let\'s look at Centrifugo benchmark where we recover zero messages (i.e. emulate a situations when many connections disconnected for a very short time interval due to load balancer reload).\\n\\nFor Redis engine:\\n\\n```bash title=\\"Redis engine, single Redis instance\\"\\nBenchmarkRedisRecover       26883 ns/op\\t    1204 B/op\\t   28 allocs/op\\n```\\n\\nCompare it with the same operation measured with Tarantool engine:\\n\\n```bash title=\\"Tarantool engine, single Tarantool instance\\"\\nBenchmarkTarantoolRecover    6292 ns/op\\t     563 B/op\\t   10 allocs/op\\n```\\n\\nTarantool can provide new storage properties (like synchronous replication), new adoption. We are pretty excited about adding it as an option.\\n\\nThe reason why Tarantool support is experimental is because Tarantool integration involves one more moving piece \u2013 the [Centrifuge Lua module](https://github.com/centrifugal/tarantool-centrifuge) which should be run by a Tarantool server.\\n\\nThis increases deployment complexity and given the fact that many users have their own best practices in Tarantool deployment we are still evaluating a sufficient way to distribute Lua part. For now, we are targeting standalone (see examples in [centrifugal/tarantool-centrifuge](https://github.com/centrifugal/tarantool-centrifuge)) and Cartridge Tarantool setups (with [centrifugal/rotor](https://github.com/centrifugal/rotor)).\\n\\nRefer to the [Tarantool Engine documentation](/docs/server/engines#tarantool-engine) for more details.\\n\\n### GRPC proxy\\n\\nCentrifugo can now transform events received over persistent connections from users into GRPC calls to the application backend (in addition to the HTTP proxy available in v2).\\n\\nGRPC support should make Centrifugo ready for today\'s microservice architecture where GRPC is a huge player for inter-service communication.\\n\\nSo we mostly just provide more choices for Centrifugo users here. GRPC has some good advantages \u2013 for example an application backend RPC layer which is responsible for communication with Centrifugo can now be generated from Protobuf definitions for all popular programming languages.\\n\\n### Server API improvements\\n\\n<img src=\\"/img/test-tube.svg\\" align=\\"right\\" width=\\"25%\\" />\\n\\nCentrifugo v3 has some valuable server API improvements.\\n\\nThe new `subscribe` API method allows subscribing connection to a channel at any point in time. This works by utilizing server-side subscriptions. So it\'s not only possible to subscribe connection to a list of server-side channels during the connection establishment phase \u2013 but also later during the connection lifetime. This may be very useful for the unidirectional approach - by emulating client-side subscribe call over request to application backend which in turn calls subscribe Centrifugo server API.\\n\\nPublish API now returns the current top stream position (offset and epoch) for channels with history enabled.\\n\\nServer history API inherited iteration possibilities described above.\\n\\nChannels command now returns a number of clients in a channel, also supports channel filtering by a pattern. Since we changed how channels call implemented internally there is no limitation anymore to call it when using Redis cluster.\\n\\nAdmin web UI has been updated too to support new API methods, so you can play with new API from its `actions` tab.\\n\\n### Better clustering\\n\\nCentrifugo behaves a bit better in cluster mode: as soon as a node leaves a cluster gracefully (upon graceful termination) it sends a shutdown signal to the control channel thus giving other nodes a chance to immediately delete that node from the local registry.\\n\\n### Client improvements\\n\\nWhile preparing the v3 release we improved client connectors too. All existing client connectors now actualized to the latest protocol, support server-side subscriptions, history API.\\n\\nOne important detail is that it\'s not required to set `?format=protobuf` URL param now when connecting to Centrifugo from mobile devices - this is now managed internally by using the WebSocket subprotocol mechanism (requires using the latest client connector version and Centrifugo v3).\\n\\n### New documentation site\\n\\nYou are reading this post on a new project site. It\'s built with amazing [Docusaurus](https://docusaurus.io/).\\n\\nA lot of documents were actualized, extended, and rewritten. We also now have new chapters like:\\n\\n* [Main highlights](/docs/getting-started/highlights)\\n* [Design overview](/docs/getting-started/design)\\n* [History and recovery](/docs/server/history_and_recovery)\\n* [Error and disconnect codes](/docs/server/codes).\\n\\nServer API and proxy documentation have been improved significantly.\\n\\n### Performance improvements\\n\\n<img src=\\"/img/stopwatch.svg\\" align=\\"right\\" width=\\"25%\\" />\\n\\nCentrifugo v3 has some notable performance improvements.\\n\\nJSON client protocol now utilizes a couple of libraries (`easyjson` for encoding and `segmentio/encoding` for unmarshaling). Actually we use a slightly customized version of `easyjson` library to achieve even faster performance than it provides out-of-the-box. Changes allowed to speed up JSON encoding and decoding up to 4-5x for small messages. For large payloads speed up can be even more noticeable \u2013 we observed up to 30x performance boost when serializing 5kb messages.\\n\\nFor example, let\'s look at a JSON serialization benchmark result for 256 byte payload. Here is what we had before:\\n\\n```bash title=\\"Centrifugo v2 JSON encoding/decoding\\"\\ncpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\\nBenchmarkMarshal-12              \\t 5883 ns/op\\t    1121 B/op\\t    6 allocs/op\\nBenchmarkMarshalParallel-12      \\t 1009 ns/op\\t    1121 B/op\\t    6 allocs/op\\nBenchmarkUnmarshal-12            \\t 1717 ns/op\\t    1328 B/op\\t   16 allocs/op\\nBenchmarkUnmarshalParallel-12    \\t492.2 ns/op\\t    1328 B/op\\t   16 allocs/op\\n```\\n\\nAnd what we have now with mentioned JSON optimizations:\\n\\n```bash title=\\"Centrifugo v3 JSON encoding/decoding\\"\\ncpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\\nBenchmarkMarshal-12              \\t 461.3 ns/op\\t 928 B/op\\t    3 allocs/op\\nBenchmarkMarshalParallel-12      \\t 250.6 ns/op\\t 928 B/op\\t    3 allocs/op\\nBenchmarkUnmarshal-12            \\t 476.5 ns/op\\t 136 B/op\\t    3 allocs/op\\nBenchmarkUnmarshalParallel-12    \\t 107.2 ns/op\\t 136 B/op\\t    3 allocs/op\\n```\\n\\n:::tip\\n\\nCentrifugo Protobuf protocol is still faster than JSON for encoding/decoding on a server-side.\\n\\n:::\\n\\nOf course, JSON encoding is only one part of Centrifugo \u2013 so you should not expect overall 4x performance improvement. But loaded setups should notice the difference and this should also be a good thing for reducing garbage collection pauses.\\n\\nCentrifugo inherited a couple of other improvements from the Centrifuge library.\\n\\nIn-memory connection hub is now sharded \u2013 this should reduce lock contention between operations in different channels. In [our artificial benchmarks](https://github.com/centrifugal/centrifuge/pull/184) we noticed a 3x better hub throughput, but in reality the benefit is heavily depends on the usage pattern.\\n\\nCentrifugo now allocates less during message broadcasting to a large number of subscribers.\\n\\nAlso, an upgrade to Go 1.17 for builds results in ~5% performance boost overall, thanks to a new way of passing function arguments and results using registers instead of the stack introduced in Go 1.17.\\n\\n### Centrifugo PRO\\n\\nThe final notable thing is an introduction of Centrifugo PRO. This is an extended version of Centrifugo built on top of the OSS version. It provides some unique features targeting business adopters.\\n\\nThose who followed Centrifugo for a long time know that there were some attempts to make project development sustainable. Buy me a coffee and Opencollective approaches were not successful, during a year we got ~300$ of total contributions. While we appreciate these contributions a lot - this does not fairly justify a time spent on Centrifugo maintenance these days and does not allow bringing it to the next level. So here is an another attempt to monetize Centrifugo.\\n\\nCentrifugo PRO details and features described [here in docs](/docs/pro/overview). Let\'s see how it goes. We believe that a set of additional functionality can provide great advantages for both small and large-scale Centrifugo setups. PRO features can give useful insights on a system, protect from client API misusing, reduce server resource usage, and more.\\n\\nPRO version will be released soon after Centrifugo v3 OSS.\\n\\n### Conclusion\\n\\nThere are some other changes introduced in v3 but not mentioned here. The full list can be found in the release notes and the migration guide.\\n\\nHope we stepped into an exciting time of the v3 life cycle and many improvements will follow. Join our communities in Telegram and Discord if you have questions or want to follow Centrifugo development:\\n\\n[![Join the chat at https://t.me/joinchat/ABFVWBE0AhkyyhREoaboXQ](https://img.shields.io/badge/Telegram-Group-orange?style=flat&logo=telegram)](https://t.me/joinchat/ABFVWBE0AhkyyhREoaboXQ) &nbsp;[![Join the chat at https://discord.gg/tYgADKx](https://img.shields.io/discord/719186998686122046?style=flat&label=Discord&logo=discord)](https://discord.gg/tYgADKx)\\n\\nEnjoy Centrifugo v3, and let the Centrifugal force be with you.\\n\\n:::note Special thanks\\n\\nSpecial thanks to [Anton Silischev](https://github.com/silischev) for the help with v3 tests, examples and CI. To [Leon Sorokin](https://github.com/leeoniya) for the spinning CSS Centrifugo logo. To [Michael Filonenko](https://github.com/filonenko-mikhail) for the help with Tarantool. To [German Saprykin](https://github.com/mogol) for Dart magic.\\n\\nThanks to the community members who tested out Centrifugo v3 beta, found bugs and sent improvements.\\n\\n<div>Icons used here made by <a href=\\"https://www.flaticon.com/authors/wanicon\\" title=\\"wanicon\\">wanicon</a> from <a href=\\"https://www.flaticon.com/\\" title=\\"Flaticon\\">www.flaticon.com</a></div>\\n\\n:::"},{"id":"/2021/01/15/centrifuge-intro","metadata":{"permalink":"/blog/2021/01/15/centrifuge-intro","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2021-01-15-centrifuge-intro.md","source":"@site/blog/2021-01-15-centrifuge-intro.md","title":"Centrifuge \u2013 real-time messaging with Go","description":"An introduction to Centrifuge \u2013 real-time messaging with Go","date":"2021-01-15T00:00:00.000Z","tags":[{"label":"centrifuge","permalink":"/blog/tags/centrifuge"},{"label":"go","permalink":"/blog/tags/go"}],"readingTime":22.93,"hasTruncateMarker":true,"authors":[{"name":"Alexander Emelin","title":"Creator of Centrifugo","imageURL":"https://github.com/FZambia.png"}],"frontMatter":{"title":"Centrifuge \u2013 real-time messaging with Go","tags":["centrifuge","go"],"author":"Alexander Emelin","authorTitle":"Creator of Centrifugo","authorImageURL":"https://github.com/FZambia.png","description":"An introduction to Centrifuge \u2013 real-time messaging with Go","image":"https://i.imgur.com/W1PeoJL.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Centrifugo v3 released","permalink":"/blog/2021/08/31/hello-centrifugo-v3"},"nextItem":{"title":"Scaling WebSocket in Go and beyond","permalink":"/blog/2020/11/12/scaling-websocket"}},"content":"![Centrifuge](https://i.imgur.com/W1PeoJL.jpg)\\n\\nIn this post I\'ll try to introduce [Centrifuge](https://github.com/centrifugal/centrifuge) - the heart of Centrifugo.\\n\\nCentrifuge is a real-time messaging library for the Go language.\\n\\nThis post is going to be pretty long (looks like I am a huge fan of long reads) \u2013 so make sure you also have a drink (probably two) and let\'s go!\\n\\n\x3c!--truncate--\x3e\\n\\n## How it\'s all started\\n\\nI wrote several blog posts before ([for example this one](https://medium.com/@fzambia/four-years-in-centrifuge-ce7a94e8b1a8) \u2013 yep, it\'s on Medium...) about an original motivation of [Centrifugo](https://github.com/centrifugal/centrifugo) server.\\n\\n:::danger\\n\\nCentrifugo server is not the same as Centrifuge library for Go. It\'s a full-featured project built on top of Centrifuge library. Naming can be confusing, but it\'s not too hard once you spend some time with ecosystem.\\n\\n:::\\n\\nIn short \u2013 Centrifugo was implemented to help traditional web frameworks dealing with many persistent connections (like WebSocket or SockJS HTTP transports). So frameworks like Django or Ruby on Rails, or frameworks from the PHP world could be used on a backend but still provide real-time messaging features like chats, multiplayer browser games, etc for users. With a little help from Centrifugo.\\n\\nNow there are cases when Centrifugo server used in conjunction even with a backend written in Go. While Go mostly has no problems dealing with many concurrent connections \u2013 Centrifugo provides some features beyond simple message passing between a client and a server. That makes it useful, especially since design is pretty non-obtrusive and fits well microservices world. Centrifugo is used in some well-known projects (like ManyChat, Yoola.io, Spot.im, Badoo etc).\\n\\nAt the end of 2018, I released Centrifugo v2 based on a real-time messaging library for Go language \u2013 Centrifuge \u2013 the subject of this post.\\n\\nIt was a pretty hard experience to decouple Centrifuge out of the monolithic Centrifugo server \u2013 I was unable to make all the things right immediately, so Centrifuge library API went through several iterations where I introduced backward-incompatible changes. All those changes targeted to make Centrifuge a more generic tool and remove opinionated or limiting parts.\\n\\n## So what is Centrifuge?\\n\\nThis is ... well, a framework to build real-time messaging applications with Go language. If you ever heard about [socket.io](https://socket.io) \u2013 then you can think about Centrifuge as an analogue. I think the most popular applications these days are chats of different forms, but I want to emphasize that Centrifuge is not a framework to build chats \u2013 it\'s a generic instrument that can be used to create different sorts of real-time applications \u2013 real-time charts, multiplayer games.\\n\\nThe obvious choice for real-time messaging transport to achieve fast and cross-platform bidirectional communication these days is WebSocket. Especially if you are targeting a browser environment. You mostly don\'t need to use WebSocket HTTP polyfills in 2021 (though there are still corner cases so Centrifuge supports [SockJS](https://github.com/sockjs/sockjs-client) polyfill).\\n\\nCentrifuge has its own custom protocol on top of plain WebSocket or SockJS frames. \\n\\nThe reason why Centrifuge has its own protocol on top of underlying transport is that it provides several useful primitives to build real-time applications. The protocol [described as strict Protobuf schema](https://github.com/centrifugal/protocol/blob/master/definitions/client.proto). It\'s possible to pass JSON or binary Protobuf-encoded data over the wire with Centrifuge.\\n\\n:::note\\n\\nGRPC is very handy these days too (and can be used in a browser with a help of additional proxies), some developers prefer using it for real-time messaging apps \u2013 especially when one-way communication needed. It can be a bit better from integration perspective but more resource-consuming on server side and a bit trickier to deploy.\\n\\n:::\\n\\n:::note\\n\\nTake a look at [WebTransport](https://w3c.github.io/webtransport/) \u2013 a brand-new spec for web browsers to allow fast communication between a client and a server on top of QUIC \u2013 it may be a good alternative to WebSocket in the future. This in a draft status at the moment, but it\'s [already possible to play with in Chrome](https://centrifugal.github.io/centrifugo/blog/quic_web_transport/).\\n\\n:::\\n\\nOwn protocol is one of the things that prove the framework status of Centrifuge. This dictates certain limits (for example, you can\'t just use an alternative message encoding) and makes developers use custom client connectors on a front-end side to communicate with a Centrifuge-based server (see more about connectors in ecosystem part).\\n\\nBut protocol solves many practical tasks \u2013 and here we are going to look at real-time features it provides for a developer.\\n\\n## Centrifuge Node\\n\\nTo start working with Centrifuge you need to start Centrifuge server Node. Node is a core of Centrifuge \u2013 it has many useful methods \u2013 set event handlers, publish messages to channels, etc. We will look at some events and channels concept very soon.\\n\\nAlso, Node abstracts away scalability aspects, so you don\'t need to think about how to scale WebSocket connections over different server instances and still have a way to deliver published messages to interested clients.\\n\\nFor now, let\'s start a single instance of Node that will serve connections for us:\\n\\n```go\\nnode, err := centrifuge.New(centrifuge.DefaultConfig)\\nif err != nil {\\n    log.Fatal(err)\\n}\\n\\nif err := node.Run(); err != nil {\\n    log.Fatal(err)\\n}\\n```\\n\\nIt\'s also required to serve a WebSocket handler \u2013 this is possible just by registering `centrifuge.WebsocketHandler` in HTTP mux:\\n\\n```go\\nwsHandler := centrifuge.NewWebsocketHandler(node, centrifuge.WebsocketConfig{})\\nhttp.Handle(\\"/connection/websocket\\", wsHandler)\\n```\\n\\nNow it\'s possible to connect to a server (using Centrifuge connector for a browser called `centrifuge-js`):\\n\\n```javascript\\nconst centrifuge = new Centrifuge(\'ws://localhost:8000/connection/websocket\');\\ncentrifuge.connect();\\n```\\n\\nThough connection will be rejected by the server since we also need to provide authentication details \u2013 Centrifuge expects explicitly provided connection `Credentials` to accept connection.\\n\\n## Authentication\\n\\nLet\'s look at how we can tell Centrifuge details about connected user identity, so it could accept an incoming connection.\\n\\nThere are two main ways to authenticate client connection in Centrifuge.\\n\\nThe first one is over the native middleware mechanism. It\'s possible to wrap `centrifuge.WebsocketHandler` or `centrifuge.SockjsHandler` with middleware that checks user authentication and tells Centrifuge current user ID over `context.Context`:\\n\\n```go\\nfunc auth(h http.Handler) http.Handler {\\n\\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\\n\\t\\tcred := &centrifuge.Credentials{\\n\\t\\t\\tUserID: \\"42\\",\\n\\t\\t}\\n\\t\\tnewCtx := centrifuge.SetCredentials(r.Context(), cred)\\n\\t\\tr = r.WithContext(newCtx)\\n\\t\\th.ServeHTTP(w, r)\\n\\t})\\n}\\n```\\n\\nSo WebsocketHandler can be registered this way (note that a handler now wrapped by auth middleware):\\n\\n```go\\nwsHandler := centrifuge.NewWebsocketHandler(node, centrifuge.WebsocketConfig{})\\nhttp.Handle(\\"/connection/websocket\\", auth(wsHandler))\\n```\\n\\nAnother authentication way is a bit more generic \u2013 developers can authenticate connection based on custom token sent from a client inside first WebSocket/SockJS frame. This is called `connect` frame in terms of Centrifuge protocol. Any string token can be set \u2013 this opens a way to use JWT, Paceto, and any other kind of authentication tokens. For example [see an authenticaton with JWT](https://github.com/centrifugal/centrifuge/tree/master/_examples/jwt_token).\\n\\n:::note\\n\\nBTW it\'s also possible to pass any information from client side with a first connect message from client to server and return custom information about server state to a client. This is out of post scope though.\\n\\n:::\\n\\nNothing prevents you to [integrate Centrifuge with OAuth2](https://github.com/centrifugal/centrifuge/tree/master/_examples/chat_oauth2) or another framework session mechanism \u2013 [like Gin for example](https://github.com/centrifugal/centrifuge/tree/master/_examples/chat_oauth2).\\n\\n## Channel subscriptions\\n\\nAs soon as a client connected and successfully authenticated it can subscribe to channels. Channel (room or topic in other systems) is a lightweight and ephemeral entity in Centrifuge. Channel can have different features (we will look at some channel features below). Channels created automatically as soon as the first subscriber joins and destroyed as soon as the last subscriber left.\\n\\nThe application can have many real-time features \u2013 even on one app screen. So sometimes client subscribes to several channels \u2013 each related to a specific real-time feature (for example one channel for chat updates, one channel likes notification stream, etc).\\n\\nChannel is just an ASCII string. A developer is responsible to find the best channel naming convention suitable for an application. Channel naming convention is an important aspect since in many cases developers want to authorize subscription to a channel on the server side \u2013 so only authorized users could listen to specific channel updates.\\n\\nLet\'s look at a basic subscription example on the client-side:\\n\\n```javascript\\ncentrifuge.subscribe(\'example\', function(msgCtx) {\\n    console.log(msgCtx)\\n})\\n```\\n\\nOn the server-side, you need to define subscribe event handler. If subscribe event handler not set then the connection won\'t be able to subscribe to channels at all. Subscribe event handler is where a developer may check permissions of the current connection to read channel updates. Here is a basic example of subscribe event handler that simply allows subscriptions to channel `example` for all authenticated connections and reject subscriptions to all other channels:\\n\\n```go\\nnode.OnConnect(func(client *centrifuge.Client) {\\n    client.OnSubscribe(func(e centrifuge.SubscribeEvent, cb centrifuge.SubscribeCallback) {\\n        if e.Channel != \\"example\\" {\\n            cb(centrifuge.SubscribeReply{}, centrifuge.ErrorPermissionDenied)\\n            return\\n        }\\n        cb(centrifuge.SubscribeReply{}, nil)\\n    })\\n})\\n```\\n\\nYou may notice a callback style of reacting to connection related things. While not being very idiomatic for Go it\'s very practical actually. The reason why we use callback style inside client event handlers is that it gives a developer possibility to control operation concurrency (i.e. process sth in separate goroutines or goroutine pool) and still control the order of events. See [an example](https://github.com/centrifugal/centrifuge/tree/master/_examples/concurrency) that demonstrates concurrency control in action.\\n\\nNow if some event published to a channel:\\n\\n```go\\n// Here is how we can publish data to a channel.\\nnode.Publish(\\"example\\", []byte(`{\\"input\\": \\"hello\\"}`))\\n```\\n\\n\u2013 data will be delivered to a subscribed client, and message will be printed to Javascript console. PUB/SUB in its usual form.\\n\\n:::note\\n\\nThough Centrifuge protocol based on Protobuf schema in example above we published a JSON message into a channel. By default, we can only send JSON to connections since default protocol format is JSON. But we can switch to Protobuf-based binary protocol by connecting to `ws://localhost:8000/connection/websocket?format=protobuf` endpoint \u2013 then it\'s possible to send binary data to clients.\\n\\n:::\\n\\n## Async message passing\\n\\nWhile Centrifuge mostly shines when you need channel semantics it\'s also possible to send any data to connection directly \u2013 to achieve bidirectional asynchronous communication, just what a native WebSocket provides.\\n\\nTo send a message to a server one can use the `send` method on the client-side:\\n\\n```javascript\\ncentrifuge.send({\\"input\\": \\"hello\\"});\\n```\\n\\nOn the server-side data will be available inside a message handler:\\n\\n```go\\nclient.OnMessage(func(e centrifuge.MessageEvent) {\\n    log.Printf(\\"message from client: %s\\", e.Data)\\n})\\n```\\n\\nAnd vice-versa, to send data to a client use `Send` method of `centrifuge.Client`:\\n\\n```go\\nclient.Send([]byte(`{\\"input\\": \\"hello\\"}`))\\n```\\n\\nTo listen to it on the client-side:\\n\\n```javascript\\ncentrifuge.on(\'message\', function(data) {\\n    console.log(data);\\n});\\n```\\n\\n## RPC\\n\\nRPC is a primitive for sending a request from a client to a server and waiting for a response (in this case all communication still happens via asynchronous message passing internally, but Centrifuge takes care of matching response data to request previously sent).\\n\\nOn client side it\'s as simple as:\\n\\n```javascript\\nconst resp = await centrifuge.rpc(\'my_method\', {});\\n```\\n\\nOn server side RPC event handler should be set to make calls available:\\n\\n```go\\nclient.OnRPC(func(e centrifuge.RPCEvent, cb centrifuge.RPCCallback) {\\n    if e.Method == \\"my_method\\" {\\n        cb(centrifuge.RPCReply{Data: []byte(`{\\"result\\": \\"42\\"}`)}, nil)\\n        return\\n    }\\n    cb(centrifuge.RPCReply{}, centrifuge.ErrorMethodNotFound)\\n})\\n```\\n\\nNote, that it\'s possible to pass the name of RPC and depending on it and custom request params return different results to a client \u2013 just like a regular HTTP request but over asynchronous WebSocket (or SockJS) connection.\\n\\n## Server-side subscriptions\\n\\nIn many cases, a client is a source of knowledge which channels it wants to subscribe to on a specific application screen. But sometimes you want to control subscriptions to channels on a server-side. This is also possible in Centrifuge.\\n\\nIt\'s possible to provide a slice of channels to subscribe connection to at the moment of connection establishment phase:\\n\\n```go\\nnode.OnConnecting(func(ctx context.Context, e centrifuge.ConnectEvent) (centrifuge.ConnectReply, error) {\\n    return centrifuge.ConnectReply{\\n        Subscriptions: map[string]centrifuge.SubscribeOptions{\\n            \\"example\\": {},\\n        },\\n    }, nil\\n})\\n```\\n\\nNote, that `OnConnecting` does not follow callback-style \u2013 this is because it can only happen once at the start of each connection \u2013 so there is no need to control operation concurrency.\\n\\nIn this case on the client-side you will have access to messages published to channels by listening to `on(\'publish\')` event:\\n\\n```javascript\\ncentrifuge.on(\'publish\', function(msgCtx) {\\n    console.log(msgCtx);\\n});\\n```\\n\\nAlso, `centrifuge.Client` has `Subscribe` and `Unsubscribe` methods so it\'s possible to subscribe/unsubscribe client to/from channel somewhere in the middle of its long WebSocket session.\\n\\n## Windowed history in channel\\n\\nEvery time a message published to a channel it\'s possible to provide custom history options. For example:\\n\\n```go\\nnode.Publish(\\n    \\"example\\",\\n    []byte(`{\\"input\\": \\"hello\\"}`),\\n    centrifuge.WithHistory(300, time.Minute),\\n)\\n```\\n\\nIn this case, Centrifuge will maintain a windowed Publication cache for a channel - or in other words, maintain a publication stream. This stream will have time retention (one minute in the example above) and the maximum size will be limited to the value provided during Publish (300 in the example above).\\n\\nEvery message inside a history stream has an incremental `offset` field. Also, a stream has a field called `epoch` \u2013 this is a unique identifier of stream generation - thus client will have a possibility to distinguish situations where a stream is completely removed and there is no guarantee that no messages have been lost in between even if offset looks fine.\\n\\nClient protocol provides a possibility to paginate over a stream from a certain position with a limit:\\n\\n```javascript\\nconst streamPosition = {\'offset\': 0, epoch: \'xyz\'} \\nresp = await sub.history({since: streamPosition, limit: 10});\\n```\\n\\nIteration over history stream is a new feature which is just merged into Centrifuge master branch and can only be used from Javascript client at the moment.\\n\\nAlso, Centrifuge has an automatic message recovery feature. Automatic recovery is very useful in scenarios when tons of persistent connections start reconnecting at once. I already described why this is useful in one of my previous posts about Websocket scalability. In short \u2013 since WebSocket connections are stateful then at the moment of mass reconnect they can create a very big spike in load on your main application database. Such mass reconnects are a usual thing in practice - for example when you reload your load balancers or re-deploying the Websocket server (new code version).\\n\\nOf course, recovery can also be useful for regular short network disconnects - when a user travels in the subway for example. But you always need a way to load an actual state from the main application database in case of an unsuccessful recovery.\\n\\nTo enable automatic recovery you can provide the `Recover` flag in subscribe options:\\n\\n```go\\nclient.OnSubscribe(func(e centrifuge.SubscribeEvent, cb centrifuge.SubscribeCallback) {\\n    cb(centrifuge.SubscribeReply{\\n        Options: centrifuge.SubscribeOptions{\\n            Recover:   true,\\n        },\\n    }, nil)\\n})\\n```\\n\\nObviously, recovery will work only for channels where history stream maintained. The limitation in recovery is that all missed publications sent to client in one protocol frame \u2013 pagination is not supported during recovery process. This means that recovery is mostly effective for not too long offline time without tons of missed messages.\\n\\n## Online presence and presence stats\\n\\nAnother cool thing Centrifuge exposes to developers is online presence information for channels. Presence information contains a list of active channel subscribers. This is useful to show the online status of players in a game for example.\\n\\nAlso, it\'s possible to turn on Join/Leave message feature inside channels: so each time connection subscribes to a channel all channel subscribers receive a Join message with client information (client ID, user ID). As soon as the client unsubscribes Leave message is sent to remaining channel subscribers with information who left a channel.\\n\\nHere is how to enable both online presence and join/leave features for a subscription to channel:\\n\\n```go\\nclient.OnSubscribe(func(e centrifuge.SubscribeEvent, cb centrifuge.SubscribeCallback) {\\n    cb(centrifuge.SubscribeReply{\\n        Options: centrifuge.SubscribeOptions{\\n            Presence:   true,\\n            JoinLeave:  true,\\n        },\\n    }, nil)\\n})\\n```\\n\\nOn a client-side then it\'s possible to call for the presence and setting event handler for join/leave messages. \\n\\nThe important thing to be aware of when using Join/Leave messages is that this feature can dramatically increase CPU utilization and overall traffic in channels with a big number of active subscribers \u2013 since on every client connect/disconnect event such Join or Leave message must be sent to all subscribers. The advice here \u2013 avoid using Join/Leave messages or be ready to scale (Join/Leave messages scale well when adding more Centrifuge Nodes \u2013 more about scalability below).\\n\\nOne more thing to remember is that online presence information can also be pretty expensive to request in channels with many active subscribers \u2013 since it returns information about all connections \u2013 thus payload in response can be large. To help a bit with this situation Centrifuge has a presence stats client API method. Presence stats only contain two counters: the number of active connections in the channel and amount of unique users in the channel.\\n\\nIf you still need to somehow process online presence in rooms with a massive number of active subscribers \u2013 then I think you better do it in near real-time - for example with fast OLAP like [ClickHouse](https://clickhouse.tech/).\\n\\n## Scalability aspects\\n\\nTo be fair it\'s not too hard to implement most of the features above inside one in-memory process. Yes, it takes time, but the code is mostly straightforward. When it comes to scalability things tend to be a bit harder.\\n\\nCentrifuge designed with the idea in mind that one machine is not enough to handle all application WebSocket connections. Connections should scale over application backend instances, and it should be simple to add more application nodes when the amount of users (connections) grows.\\n\\nCentrifuge abstracts scalability over the `Node` instance and two interfaces: `Broker` interface and `PresenceManager` interface.\\n\\nA broker is responsible for PUB/SUB and streaming semantics:\\n\\n```go\\ntype Broker interface {\\n\\tRun(BrokerEventHandler) error\\n\\tSubscribe(ch string) error\\n\\tUnsubscribe(ch string) error\\n\\tPublish(ch string, data []byte, opts PublishOptions) (StreamPosition, error)\\n\\tPublishJoin(ch string, info *ClientInfo) error\\n\\tPublishLeave(ch string, info *ClientInfo) error\\n\\tPublishControl(data []byte, nodeID string) error\\n\\tHistory(ch string, filter HistoryFilter) ([]*Publication, StreamPosition, error)\\n\\tRemoveHistory(ch string) error\\n}\\n```\\n\\nSee [full version with comments](https://github.com/centrifugal/centrifuge/blob/v0.14.2/engine.go#L98) in source code.\\n\\nEvery Centrifuge Node subscribes to channels via a broker. This provides a possibility to scale connections over many node instances \u2013 published messages will flow only to nodes with active channel subscribers.\\n\\nIt\'s and important thing to combine PUB/SUB with history inside a Broker implementation to achieve an atomicity of saving message into history stream and publishing it to PUB/SUB with generated offset.\\n\\nPresenceManager is responsible for online presence information management:\\n\\n```go\\ntype PresenceManager interface {\\n\\tPresence(ch string) (map[string]*ClientInfo, error)\\n\\tPresenceStats(ch string) (PresenceStats, error)\\n\\tAddPresence(ch string, clientID string, info *ClientInfo, expire time.Duration) error\\n\\tRemovePresence(ch string, clientID string) error\\n}\\n```\\n\\n[Full code with comments](https://github.com/centrifugal/centrifuge/blob/v0.14.2/engine.go#L150).\\n\\n`Broker` and `PresenceManager` together form an `Engine` interface:\\n\\n```go\\ntype Engine interface {\\n\\tBroker\\n\\tPresenceManager\\n}\\n```\\n\\nBy default, Centrifuge uses `MemoryEngine` that does not use any external services but limits developers to using only one Centrifuge Node (i.e. one server instance). Memory Engine is fast and can be suitable for some scenarios - even in production (with configured backup instance) \u2013 but as soon as the number of connections grows \u2013 you may need to load balance connections to different server instances. Here comes the Redis Engine.\\n\\nRedis Engine utilizes Redis for Broker and PresenceManager parts.\\n\\nHistory cache saved to Redis STREAM or Redis LIST data structures. For presence, Centrifuge uses a combination of HASH and ZSET structures.\\n\\nCentrifuge tries to fully utilize the connection between Node and Redis by using pipelining where possible and smart batching technique. All operations done in a single RTT with the help of Lua scripts loaded automatically to Redis on engine start.\\n\\nRedis is pretty fast and will allow your app to scale to some limits. When Redis starts being a bottleneck it\'s possible to shard data over different Redis instances. Client-side consistent sharding is built-in in Centrifuge and allows scaling further.\\n\\nIt\'s also possible to achieve Redis\'s high availability with built-in Sentinel support. Redis Cluster supported too. So Redis Engine covers many options to communicate with Redis deployed in different ways.\\n\\nAt Avito we served about 800k active connections in the messenger app with ease using a slightly adapted Centrifuge Redis Engine, so an approach proved to be working for rather big applications. We will look at some more concrete numbers below in the performance section.\\n\\nBoth `Broker` and `PresenceManager` are pluggable, so it\'s possible to replace them with alternative implementations. Examples show [how to use Nats server](https://github.com/centrifugal/centrifuge/tree/master/_examples/custom_broker_nats) for at most once only PUB/SUB together with Centrifuge. Also, we have [an example of full-featured Engine for Tarantool database](https://github.com/centrifugal/centrifuge/tree/master/_examples/custom_engine_tarantool) \u2013 Tarantool Engine shows even better throughput for history and presence operations than Redis-based Engine (up to 10x for some ops).\\n\\n## Order and delivery properties\\n\\nSince Centrifuge is a messaging system I also want to describe its order and message delivery guarantees.\\n\\nMessage ordering in channels supported. As soon as you publish messages into channels one after another of course.\\n\\nMessage delivery model is at most once by default. This is mostly comes from PUB/SUB model \u2013 message can be dropped on Centrifuge level if subscriber is offline or simply on broker level \u2013 since Redis PUB/SUB also works with at most once guarantee.\\n\\nThough if you maintain history stream inside a channel then things become a bit different. In this case you can tell Centrifuge to check client position inside stream. Since every publication has a unique incremental offset Centrifuge can track that client has correct offset inside a channel stream. If Centrifuge detects any missed messages it disconnects a client with special code \u2013 thus make it reconnect and recover messages from history stream. Since a message first saved to history stream and then published to PUB/SUB inside broker these mechanisms allow achieving at least once message delivery guarantee.\\n\\n![What happens on publish](https://i.imgur.com/PLb9xS5.jpg)\\n\\nEven if stream completely expired or dropped from broker memory Centrifuge will give a client a tip that messages could be lost \u2013 so client has a chance to restore state from a main application database.\\n\\n## Ecosystem\\n\\nHere I want to be fair with my readers \u2013 Centrifuge is not ideal. This is a project maintained mostly by one person at the moment with all consequences. This hits an ecosystem a lot, can make some design choices opinionated or non-optimal.\\n\\nI mentioned in the first post that Centrifuge built on top of the custom protocol. The protocol is based on a strict Protobuf schema, works with JSON and binary data transfer, supports many features. But \u2013 this means that to connect to the Centrifuge-based server developers have to use custom connectors that can speak with Centrifuge over its custom protocol.\\n\\nThe difficulty here is that protocol is asynchronous. Asynchronous protocols are harder to implement than synchronous ones. Multiplexing frames allows achieving good performance and fully utilize a single connection \u2013 but it hurts simplicity.\\n\\nAt this moment Centrifuge has client connectors for:\\n\\n* [centrifuge-js](https://github.com/centrifugal/centrifuge-js) - Javascript client for a browser, NodeJS and React Native\\n* [centrifuge-go](https://github.com/centrifugal/centrifuge-go) - for Go language\\n* [centrifuge-mobile](https://github.com/centrifugal/centrifuge-mobile) - for mobile development based on centrifuge-go and [gomobile](https://github.com/golang/mobile) project\\n* [centrifuge-swift](https://github.com/centrifugal/centrifuge-swift) - for iOS native development\\n* [centrifuge-java](https://github.com/centrifugal/centrifuge-java) - for Android native development and general Java\\n* [centrifuge-dart](https://github.com/centrifugal/centrifuge-dart) - for Dart and Flutter\\n\\nNot all clients support all protocol features. Another drawback is that all clients do not have a persistent maintainer \u2013 I mostly maintain everything myself. Connectors can have non-idiomatic and pretty dumb code since I had no previous experience with mobile development, they lack proper tests and documentation. This is unfortunate.\\n\\nThe good thing is that all connectors feel very similar, I am quickly releasing new versions when someone sends a pull request with improvements or bug fixes. So all connectors are alive.\\n\\nI maintain a feature matrix in connectors to let users understand what\'s supported. Actually feature support is pretty nice throughout all these connectors - there are only several things missing and not so much work required to make all connectors full-featured. But I really need help here.\\n\\nIt will be a big mistake to not mention Centrifugo as a big plus for Centrifuge library ecosystem. Centrifugo is a server deployed in many projects throughout the world. Many features of Centrifuge library and its connectors have already been tested by Centrifugo users.\\n\\nOne more thing to mention is that Centrifuge does not have v1 release. It still evolves \u2013 I believe that the most dramatic changes have already been made and backward compatibility issues will be minimal in the next releases \u2013 but can\'t say for sure.\\n\\n## Performance\\n\\nI made a test stand in Kubernetes with one million connections.\\n\\nI can\'t call this a proper benchmark \u2013 since in a benchmark your main goal is to destroy a system, in my test I just achieved some reasonable numbers on limited hardware. These numbers should give a good insight into a possible throughput, latency, and estimate hardware requirements (at least approximately).\\n\\nConnections landed on different server pods, 5 Redis instances have been used to scale connections between pods.\\n\\nThe detailed test stand description [can be found in Centrifugo documentation](https://centrifugal.github.io/centrifugo/misc/benchmark/).\\n\\n![Benchmark](/img/benchmark.gif)\\n\\nSome quick conclusions are:\\n\\n* One connection costs about 30kb of RAM\\n* Redis broker CPU utilization increases linearly with more messages traveling around\\n* 1 million connections with 500k **delivered** messages per second with 200ms delivery latency in 99 percentile can be served with hardware amount equal to one modern physical server machine. The possible amount of messages can vary a lot depending on the number of channel subscribers though.\\n\\n## Limitations\\n\\nCentrifuge does not allow subscribing on the same channel twice inside a single connection. It\'s not simple to add due to design decisions made \u2013 though there was no single user report about this in seven years of Centrifugo/Centrifuge history.\\n\\nCentrifuge does not support wildcard subscriptions. Not only because I never needed this myself but also due to some design choices made \u2013 so be aware of this.\\n\\nSockJS fallback does not support binary data - only JSON. If you want to use binary in your application then you can only use WebSocket with Centrifuge - there is no built-in fallback transport in this case.\\n\\nSockJS also requires sticky session support from your load balancer to emulate a stateful bidirectional connection with its HTTP fallback transports. Ideally, Centrifuge will go away from SockJS at some point, maybe when WebTransport becomes mature so users will have a choice between WebTransport or WebSocket.\\n\\nWebsocket `permessage-deflate` compression supported (thanks to Gorilla WebSocket), but it can be pretty expensive in terms of CPU utilization and memory usage \u2013 the overhead depends on usage pattern, it\'s pretty hard to estimate in numbers.\\n\\nAs said above you cannot only rely on Centrifuge for state recovery \u2013 it\'s still required to have a way to fully load application state from the main database.\\n\\nAlso, I am not very happy with current error and disconnect handling throughout the connector ecosystem \u2013 this can be improved though, and I have some ideas for the future.\\n\\n## Examples\\n\\nI am adding examples to [_examples](https://github.com/centrifugal/centrifuge/tree/master/_examples) folder of Centrifuge repo. These examples completely cover Centrifuge API - including things not mentioned here.\\n\\nCheck out the [tips & tricks](https://github.com/centrifugal/centrifuge#tips-and-tricks) section of README \u2013 it contains some additional insights about an implementation.\\n\\n## Conclusion\\n\\nI think [Centrifuge](https://github.com/centrifugal/centrifuge) could be a nice alternative to [socket.io](https://socket.io) - with a better performance, main server implementation in Go language, and even more builtin features to build real-time apps.\\n\\nCentrifuge ecosystem definitely needs more work, especially in client connectors area, tutorials, community, stabilizing API, etc.\\n\\nCentrifuge fits pretty well proprietary application development where time matters and deadlines are close, so developers tend to choose a ready solution instead of writing their own. I believe Centrifuge can be a great time saver here.\\n\\nFor Centrifugo server users Centrifuge package provides a way to write a more flexible server code adapted for business requirements but still use the same real-time core and have the same protocol features."},{"id":"/2020/11/12/scaling-websocket","metadata":{"permalink":"/blog/2020/11/12/scaling-websocket","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2020-11-12-scaling-websocket.md","source":"@site/blog/2020-11-12-scaling-websocket.md","title":"Scaling WebSocket in Go and beyond","description":"The post describes techniques to write scalable WebSocket servers within Go ecosystem and beyond it","date":"2020-11-12T00:00:00.000Z","tags":[{"label":"websocket","permalink":"/blog/tags/websocket"},{"label":"go","permalink":"/blog/tags/go"}],"readingTime":18.705,"hasTruncateMarker":true,"authors":[{"name":"Alexander Emelin","title":"Creator of Centrifugo","imageURL":"https://github.com/FZambia.png"}],"frontMatter":{"title":"Scaling WebSocket in Go and beyond","tags":["websocket","go"],"description":"The post describes techniques to write scalable WebSocket servers within Go ecosystem and beyond it","author":"Alexander Emelin","authorTitle":"Creator of Centrifugo","authorImageURL":"https://github.com/FZambia.png","image":"https://i.imgur.com/QOJ1M9a.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Centrifuge \u2013 real-time messaging with Go","permalink":"/blog/2021/01/15/centrifuge-intro"},"nextItem":{"title":"Experimenting with QUIC and WebTransport","permalink":"/blog/2020/10/16/experimenting-with-quic-transport"}},"content":"![gopher-broker](https://i.imgur.com/QOJ1M9a.png)\\n\\nI believe that in 2020 WebSocket is still an entertaining technology which is not so well-known and understood like HTTP. In this blog post I\'d like to tell about state of WebSocket in Go language ecosystem, and a way we could write scalable WebSocket servers with Go and beyond Go.\\n\\n\x3c!--truncate--\x3e\\n\\nWe won\'t talk a lot about WebSocket transport pros and cons \u2013 I\'ll provide links to other resources on this topic. Most advices here are generic enough and can be easily approximated to other programming languages. Also in this post we won\'t talk about ready to use solutions (if you are looking for it \u2013 check out [Real-time Web Technologies guide](https://www.leggetter.co.uk/real-time-web-technologies-guide/) by Phil Leggetter), just general considerations. There is not so much information about scaling WebSocket on the internet so if you are interested in WebSocket and real-time messaging technologies - keep on reading.\\n\\nIf you don\'t know what WebSocket is \u2013 check out the following curious links:\\n\\n* https://hpbn.co/websocket/ \u2013 a wonderful chapter of great book by Ilya Grigorik\\n* https://lucumr.pocoo.org/2012/9/24/websockets-101/ \u2013 valuable thoughts about WebSocket from Armin Ronacher\\n\\nAs soon as you know WebSocket basics \u2013 we can proceed.\\n\\n## WebSocket server tasks\\n\\nSpeaking about scalable servers that work with many persistent WebSocket connections \u2013 I found several important tasks such a server should be able to do:\\n\\n* Maintain many active connections\\n* Send many messages to clients\\n* Support WebSocket fallback to scale to every client\\n* Authenticate incoming connections and invalidate connections\\n* Survive massive reconnect of all clients without loosing messages\\n\\n:::note\\n\\nOf course not all of these points equally important in various situations.\\n\\n:::\\n\\nBelow we will look at some tips which relate to these points.\\n\\n![one_hour_scale](https://i.imgur.com/4lYjJSP.png)\\n\\n## WebSocket libraries\\n\\nIn Go language ecosystem we have several libraries which can be used as a building block for a WebSocket server.\\n\\nPackage [golang.org/x/net/websocket](https://godoc.org/golang.org/x/net/websocket) is considered **deprecated**.\\n\\nThe default choice in the community is [gorilla/websocket](https://github.com/gorilla/websocket) library. Made by Gary Burd (who also gifted us an awesome [Redigo](https://github.com/gomodule/redigo) package to communicate with Redis) \u2013 it\'s widely used, performs well, has a very good API \u2013 so in most cases you should go with it. Some people think that library not actively maintained at moment \u2013 but this is not quite true, it implements full WebSocket RFC, so actually it can be considered done.\\n\\nIn 2018 my ex-colleague Sergey Kamardin open-sourced [gobwas/ws](https://github.com/gobwas/ws) library. It provides a bit lower-level API than `gorilla/websocket` thus allows reducing RAM usage per connection and has nice optimizations for WebSocket upgrade process. It does not support WebSocket `permessage-deflate` compression but otherwise a good alternative you can consider using. If you have not read Sergey\'s famous post [A Million WebSockets and Go](https://www.freecodecamp.org/news/million-websockets-and-go-cc58418460bb/) \u2013 make a bookmark!\\n\\nOne more library is [nhooyr/websocket](https://github.com/nhooyr/websocket). It\'s the youngest one and actively maintained. It compiles to WASM which can be a cool thing for someone. The API is a bit different from what `gorilla/websocket` offers, and one of the big advantages I see is that it solves a problem with a proper WebSocket closing handshake which is [a bit hard to do right with Gorilla WebSocket](https://github.com/gorilla/websocket/issues/448).\\n\\nYou can consider all listed libraries except one from `x/net` for your project. Take a library, follow its examples (make attention to goroutine-safety of various API operations). Personally I prefer Gorilla WebSocket at moment since it\'s feature-complete and battle tested by tons of projects around Go world.\\n\\n## OS tuning\\n\\nOK, so you have chosen a library and built a server on top of it. As soon as you put it in production the interesting things start happening.\\n\\nLet\'s start with several OS specific key things you should do to prepare for many connections from WebSocket clients.\\n\\nEvery connection will cost you an open file descriptor, so you should tune a maximum number of open file descriptors your process can use. An errors like `too many open files` raise due to OS limit on file descriptors which is usually 256-1024 by default (see with `ulimit -n` on Unix). A nice overview on how to do this on different systems can be found [in Riak docs](https://docs.riak.com/riak/kv/2.2.3/using/performance/open-files-limit.1.html). Wanna more connections? Make this limit higher.\\n\\nNice tip here is to limit a maximum number of connections your process can serve \u2013 making it less than known file descriptor limit:\\n\\n```go\\n//\xa0ulimit\xa0-n\xa0==\xa065535\\nif conns.Len() >= 65500 {\\n    return errors.New(\\"connection\xa0limit\xa0reached\\")\\n}\\nconns.Add(conn)\\n```\\n\\n\u2013 otherwise you have a risk to not even able to look at `pprof` when things go bad. And you always need monitoring of open file descriptors.\\n\\nYou can also consider using [netutil.LimitListener](https://godoc.org/golang.org/x/net/netutil#LimitListener) for this task, but don\'t forget to put pprof on another port with another HTTP server instance in this case.\\n\\nKeep attention on *Ephemeral ports* problem which is often happens between your load balancer and your WebSocket server. The problem arises due to the fact that each TCP connection uniquely identified in the OS by the 4-part-tuple:\\n\\n```\\nsource ip | source port | destination ip | destination port\\n```\\n\\nOn balancer/server boundary you are limited in 65536 possible variants by default. But actually due to some OS limits and sockets in TIME_WAIT state the number is even less. A very good explanation and how to deal with it can be found [in Pusher blog](https://making.pusher.com/ephemeral-port-exhaustion-and-how-to-avoid-it/).\\n\\nYour possible number of connections also limited by conntrack table. Netfilter framework which is part of iptables keeps information about all connections and has limited size for this information. See how to see its limits and instructions to increase [in this article](https://morganwu277.github.io/2018/05/26/Solve-production-issue-of-nf-conntrack-table-full-dropping-packet/).\\n\\nOne more thing you can do is tune your network stack for performance. Do this only if you understand that you need it. Maybe start [with this gist](https://gist.github.com/mustafaturan/47268d8ad6d56cadda357e4c438f51ca), but don\'t optimize without full understanding why you are doing this. \\n\\n## Sending many messages\\n\\nNow let\'s speak about sending many messages. The general tips follows.\\n\\n**Make payload smaller**. This is obvious \u2013 fewer data means more effective work on all layers. BTW WebSocket framing overhead is minimal and adds only 2-8 bytes to your payload. You can read detailed dedicated research in [Dissecting WebSocket\'s Overhead](https://crossbario.com/blog/Dissecting-Websocket-Overhead/) article. You can reduce an amount of data traveling over network with `permessage-deflate` WebSocket extension, so your data will be compressed. Though using `permessage-deflate` is not always a good thing for server due to [poor performance of flate](https://github.com/gorilla/websocket/issues/203), so you should be prepared for a CPU and RAM resource usage on server side. While Gorilla WebSocket has a lot of optimizations internally by reusing flate writers, overhead is still noticeable. The increase value heavily depends on your load profile.\\n\\n**Make less system calls**. Every syscall will have a constant overhead, and actually in WebSocket server under load you will mostly see read and write system calls in your CPU profiles. An advice here \u2013 try to use client-server protocol that supports message batching, so you can join individual messages together.\\n\\n**Use effective message serialization protocol**. Maybe use code generation for JSON to avoid extensive usage of reflect package done by Go std lib. Maybe use sth like [gogo/protobuf](https://github.com/gogo/protobuf) package which allows to speedup Protobuf marshalling and unmarshalling. Unfortunately Gogo Protobuf [is going through hard times\\n](https://github.com/gogo/protobuf/issues/691) at this moment. Try to serialize a message only once when sending to many subscribers.\\n\\n**Have a way to scale to several machines** - more power, more possible messages. We will talk about this very soon.\\n\\n## WebSocket fallback transport\\n\\n![ie](https://i.imgur.com/IAOyvmg.png)\\n\\nEven in 2020 there are still users which cannot establish connection with WebSocket server. Actually the problem mostly appears with browsers. Some users still use old browsers. But they have a choice \u2013 install a newer browser. Still, there could also be users behind corporate proxies. Employees can have a trusted certificate installed on their machine so company proxy can re-encrypt even TLS traffic. Also, some browser extensions can block WebSocket traffic.\\n\\nOne ready solution to this is [Sockjs-Go](https://github.com/igm/sockjs-go/) library. This is a mature library that provides fallback transport for WebSocket. If client does not succeed with WebSocket connection establishment then client can use some of HTTP transports for client-server communication: [EventSource aka Server-Sent Events](https://hpbn.co/server-sent-events-sse/), XHR-streaming, Long-Polling etc. The downside with those transports is that to achieve bidirectional communication you should use sticky sessions on your load balancer since SockJS keeps connection session state in process memory. We will talk about many instances of your WebSocket server very soon.\\n\\nYou can implement WebSocket fallback yourself, this should be simple if you have a sliding window message stream on your backend which we will discuss very soon.\\n\\nMaybe look at [GRPC](https://grpc.io/docs/what-is-grpc/introduction/), depending on application it could be better or worse than WebSocket \u2013 in general you can expect a better performance and less resource consumption from WebSocket for bidirectional communication case. My measurements for a **bidirectional** scenario showed 3x win for WebSocket (binary + GOGO protobuf) in terms of server CPU consumption and 4 times less RAM per connection. Though if you only need RPC then GRPC can be a better choice. But you need additional proxy to work with GRPC from a browser. \\n\\n## Performance is not scalability\\n\\nYou can optimize client-server protocol, tune your OS, but at some point you won\'t be able to use only one process on one server machine. You need to scale connections and work your server does over different server machines. Horizontal scaling is also good for a server high availability. Actually there are some sort of real-time applications where a single isolated process makes sense - for example multiplayer games where limited number of players play independent game rounds.\\n\\n![many_instances](https://i.imgur.com/8ElqpjI.png)\\n\\nAs soon as you distribute connections over several machines you have to find a way to deliver a message to a certain user. The basic approach here is to publish messages to all server instances. This can work but this does not scale well. You need a sort of instance discovery to make this less painful.\\n\\nHere comes PUB/SUB, where you can connect WebSocket server instances over central PUB/SUB broker. Clients that establish connections with your WebSocket server subscribe to topics (channels) in a broker, and as soon as you publish a message to that topic it will be delivered to all active subscribers on WebSocket server instances. If server node does not have interested subscriber then it won\'t get a message from a broker thus you are getting effective network communication.\\n\\nActually the main picture of this post illustrates exactly this architecture:\\n\\n![gopher-broker](https://i.imgur.com/QOJ1M9a.png)\\n\\nLet\'s think about requirements for a broker for real-time messaging application. We want a broker:\\n\\n* with reasonable performance and possibility to scale\\n* which maintains message order in topics\\n* can support millions of topics, where each topic should be ephemeral and lightweight \u2013 topics can be created when user comes to application and removed after user goes away\\n* possibility to keep a sliding window of messages inside channel to help us survive massive reconnect scenario (will talk about this later below, can be a separate part from broker actually)\\n\\nPersonally when we talk about such brokers here are some options that come into my mind:\\n\\n* [RabbitMQ](https://www.rabbitmq.com/)\\n* [Kafka](https://kafka.apache.org/) or [Pulsar](https://pulsar.apache.org/)\\n* [Nats or Nats-Streaming](https://nats.io/)\\n* [Tarantool](https://www.tarantool.io/en/)\\n* [Redis](https://redis.io/)\\n\\n**Sure there are more exist** including libraries like [ZeroMQ](https://zeromq.org/) or [nanomsg](https://nanomsg.org/).\\n\\nBelow I\'ll try to consider these solutions for the task of making scalable WebSocket server facing many user connections from Internet.\\n\\nIf you are looking for unreliable at most once PUB/SUB then any of solutions mentioned above should be sufficient. Many real-time messaging apps are ok with at most once guarantee delivery.\\n\\nIf you don\'t want to miss messages then things are a bit harder. Let\'s try to evaluate these options for a task where application has lots of different topics from which it wants to receive messages with at least once guarantee (having a personal topic per client is common thing in applications). A short analysis below can be a bit biased, but I believe thoughts are reasonable enough. I did not found enough information on the internet about scaling WebSocket beyond a single server process, so I\'ll try to fill the gap a little based on my personal knowledge without pretending to be absolutely objective in these considerations.\\n\\nIn some posts on the internet about scaling WebSocket I saw advices to use RabbitMQ for PUB/SUB stuff in real-time messaging server. While this is a great messaging server, it does not like a high rate of queue bind and unbind type of load. It will work, but you will need to use a lot of server resources for not so big number of clients (imagine having millions of queues inside RabbitMQ). I have an example from my practice where RabbitMQ consumed about 70 CPU cores to serve real-time messages for 100k online connections. After replacing it with Redis keeping the same message delivery semantics we got only 0.3 CPU consumption on broker side.\\n\\nKafka and Pulsar are great solutions, but not for this task I believe. The problem is again in dynamic ephemeral nature of our topics. Kafka also likes a more stable configuration of its topics. Keeping messages on disk can be an overkill for real-time messaging task. Also your consumers on Kafka server should pull from millions of different topics, not sure how well it performs, but my thoughts at moment - this should not perform very well. Kafka itself scales perfectly, you will definitely be able to achieve a goal but resource usage will be significant. Here is [a post from Trello](https://tech.trello.com/why-we-chose-kafka/) where they moved from RabbitMQ to Kafka for similar real-time messaging task and got about 5x resource usage improvements. Note also that the more partitions you have the more heavy failover process you get.\\n\\nNats and Nats-Streaming. Raw Nats can only provide at most once guarantee. BTW recently Nats developers [released native WebSocket support](https://github.com/nats-io/nats-server/issues/315), so you can consider it for your application. Nats-Streaming server as broker will allow you to not lose messages. To be fair I don\'t have enough information about how well Nats-Streaming scales to millions of topics. An upcoming [Jetstream](https://github.com/nats-io/jetstream) which will be a part of Nats server can also be an interesting option \u2013 like Kafka it provides a persistent stream of messages for at least once delivery semantics. But again, it involves disk storage, a nice thing for backend microservices communication but can be an overkill for real-time messaging task.\\n\\nSure Tarantool can fit to this task well too. It\'s fast, im-memory and flexible. Some possible problems with Tarantool are not so healthy state of its client libraries, complexity and the fact that it\'s heavily enterprise-oriented. You should invest enough time to benefit from it, but this can worth it actually. See [an article](https://hackernoon.com/tarantool-when-it-takes-500-lines-of-code-to-notify-a-million-users-11d340523493) on how to do a performant broker for WebSocket applications with Tarantool.\\n\\nBuilding PUB/SUB system on top of ZeroMQ will require you to build separate broker yourself. This could be an unnecessary complexity for your system. It\'s possible to implement PUB/SUB pattern with ZeroMQ and nanomsg without a central broker, but in this case messages without active subscribers on a server will be dropped on a consumer side thus all publications will travel to all server nodes. \\n\\nMy personal choice at moment is Redis. While **Redis PUB/SUB itself provides at most once guarantee**, you can build at least once delivery on top of PUB/SUB and Redis data structures (though this can be challenging enough). Redis is very fast (especially when using pipelining protocol feature), and what is more important \u2013 **very predictable**. It gives you a good understanding of operation time complexity. You can shard topics over different Redis instances running in HA setup - with Sentinel or with Redis Cluster. It allows writing LUA procedures with some advanced logic which can be uploaded over client protocol thus feels like ordinary commands. You can use Redis to keep sliding window event stream which gives you access to missed messages from a certain position. We will talk about this later.\\n\\nOK, the end of opinionated thoughts here :)\\n\\nDepending on your choice the implementation of your system will vary and will have different properties \u2013 so try to evaluate possible solutions based on your application requirements. Anyway, whatever broker will be your choice, try to follow this rules to build effective PUB/SUB system:\\n\\n* take into account message delivery guarantees of your system: at most once or at least once, ideally you should have an option to have both for different real-time features in your app\\n* make sure to use one or pool of connections between your server and a broker, don\'t create new connection per each client or topic that comes to your WebSocket server\\n* use effective serialization format between your WebSocket server and broker\\n\\n## Massive reconnect\\n\\n![mass_reconnect](https://i.imgur.com/S9koKYg.png)\\n\\nLet\'s talk about one more problem that is unique for Websocket servers compared to HTTP. Your app can have thousands or millions of active WebSocket connections. In contract to stateless HTTP APIs your application is stateful. It uses push model. As soon as you deploying your WebSocket server or reload your load balancer (Nginx maybe) \u2013 connections got dropped and all that army of users start reconnecting. And this can be like an avalanche actually. How to survive?\\n\\nFirst of all - use exponential backoff strategies on client side. I.e. reconnect with intervals like 1, 2, 4, 8, 16 seconds with some random jitter.\\n\\nTurn on various rate limiting strategies on your WebSocket server, some of them should be turned on your backend load balancer level (like controlling TCP connection establishment rate), some are application specific (maybe limit an amount of requests from certain user).\\n\\nOne more interesting technique to survive massive reconnect is using JWT (JSON Web Token) for authentication. I\'ll try to explain why this can be useful.\\n\\n![jwt](https://i.imgur.com/aaTEhXo.png)\\n\\nAs soon as your client start reconnecting you will have to authenticate each connection. In massive setups with many persistent connection this can be a very significant load on your Session backend. Since you need an extra request to your session storage for every client coming back. This can be a no problem for some infrastructures but can be really disastrous for others. JWT allows to reduce this spike in load on session storage since it can have all required authentication information inside its payload. When using JWT make sure you have chosen a reasonable JWT expiration time \u2013 expiration interval depends on your application nature and just one of trade-offs you should deal with as developer.\\n\\nDon\'t forget about making an effective connection between your WebSocket server and broker \u2013 as soon as all clients start reconnecting you should resubscribe your server nodes to all topics as fast as possible. Use techniques like smart batching at this moment.\\n\\nLet\'s look at a small piece of code that demonstrates this technique. Imagine we have a source channel from which we get items to process. We don\u2019t want to process items individually but in batch. For this we wait for first item coming from channel, then try to collect as many items from channel buffer as we want without blocking and timeouts involved. And then process slice of items we collected at once. For example build Redis pipeline from them and send to Redis in one connection write call.\\n\\n```go\\nmaxBatchSize := 50\\n\\nfor {\\n    select {\\n    case item := <-sourceCh:\\n        batch := []string{item}\\n    loop:\\n        for len(batch) < maxBatchSize {\\n            select {\\n            case item := <-sourceCh:\\n                batch = append(batch, item)\\n            default:\\n                break loop\\n            }\\n        }\\n        // Do sth with collected batch of items.\\n        println(len(batch))\\n    }\\n}\\n```\\n\\nLook at a complete example in a Go playground: https://play.golang.org/p/u7SAGOLmDke.\\n\\nI also made a repo where I demonstrate how this technique together with Redis pipelining feature allows to fully utilize connection for a good performance https://github.com/FZambia/redigo-smart-batching.\\n\\nAnother advice for those who run WebSocket services in Kubernetes. Learn how your ingress behaves \u2013 for example Nginx ingress can reload its configuration on every change inside Kubernetes services map resulting into closing all active WebSocket connections. Proxies like Envoy don\'t have this behaviour, so you can reduce number of mass disconnections in your system. You can also proxy WebSocket without using ingress at all over configured WebSocket service NodePort.\\n\\n## Message event stream benefits\\n\\nHere comes a final part of this post. Maybe the most important one.\\n\\nNot only mass client re-connections could create a significant load on a session backend but also a huge load on your main application database. Why? Because WebSocket applications are stateful. Clients rely on a stream of messages coming from a backend to maintain its state actual. As soon as connection dropped client tries to reconnect. In some scenarios it also wants to restore its actual state. What if client reconnected after 3 seconds? How many state updates it could miss? Nobody knows. So to make sure state is actual client tries to get it from application database. This is again **a significant spike in load on your main database** in massive reconnect scenario. In can be really painful with many active connections.\\n\\nSo what I think is nice to have for scenarios where we can\'t afford to miss messages (like in chat-like apps for example) is having effective and performant stream of messages inside each channel. Keep this stream in fast in-memory storage. This stream can have time retention and be limited in size (think about it as a sliding window of messages). I already mentioned that Redis can do this \u2013 it\'s possible to keep messages in Redis List or Redis Stream data structures. Other broker solutions could give you access to such a stream inside each channel out of the box.\\n\\nSo as soon as client reconnects it can restore its state from fast in-memory event stream without even querying your database. Actually to survive mass reconnect scenario you don\'t need to keep such a stream for a long time \u2013 several minutes should be enough. You can **even create your own Websocket fallback implementation (like Long-Polling) utilizing event stream with limited retention**.\\n\\n## Conclusion\\n\\nHope advices given here will be useful for a reader and will help writing a more robust and more scalable real-time application backends.\\n\\n[Centrifugo server](https://github.com/centrifugal/centrifugo/) and [Centrifuge library for Go language](https://github.com/centrifugal/centrifuge) have most of the mechanics described here including the last one \u2013 message stream for topics limited by size and retention period. Both also have techniques to prevent message loss due to at most once nature of Redis PUB/SUB giving at least once delivery guarantee inside message history window size and retention period."},{"id":"/2020/10/16/experimenting-with-quic-transport","metadata":{"permalink":"/blog/2020/10/16/experimenting-with-quic-transport","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2020-10-16-experimenting-with-quic-transport.md","source":"@site/blog/2020-10-16-experimenting-with-quic-transport.md","title":"Experimenting with QUIC and WebTransport","description":"Here we experiment with QUIC and WebTransport in Chrome","date":"2020-10-16T00:00:00.000Z","tags":[{"label":"quic","permalink":"/blog/tags/quic"},{"label":"webtransport","permalink":"/blog/tags/webtransport"},{"label":"go","permalink":"/blog/tags/go"}],"readingTime":14.165,"hasTruncateMarker":true,"authors":[{"name":"Alexander Emelin","title":"Creator of Centrifugo","imageURL":"https://github.com/FZambia.png"}],"frontMatter":{"title":"Experimenting with QUIC and WebTransport","tags":["quic","webtransport","go"],"description":"Here we experiment with QUIC and WebTransport in Chrome","author":"Alexander Emelin","authorTitle":"Creator of Centrifugo","authorImageURL":"https://github.com/FZambia.png","image":"https://i.imgur.com/sH9zfhe.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Scaling WebSocket in Go and beyond","permalink":"/blog/2020/11/12/scaling-websocket"},"nextItem":{"title":"Million connections with Centrifugo","permalink":"/blog/2020/02/10/million-connections-with-centrifugo"}},"content":"![post-cover](https://i.imgur.com/sH9zfhe.jpg)\\n\\n**UPDATE: WebTransport spec is still evolving. Most information here is not actual anymore. For example the working group has no plan to implement both QuicTransport and HTTP3-based transports \u2013 only HTTP3 based WebTransport is going to be implemented. Maybe we will publish a follow-up of this post at some point.**\\n\\n\x3c!--truncate--\x3e\\n\\n## Overview\\n\\nWebTransport is a new browser API offering low-latency, bidirectional, client-server messaging. If you have not heard about it before I suggest to first read a post called [Experimenting with QuicTransport](https://web.dev/quictransport/) published recently on web.dev \u2013 it gives a nice overview to WebTransport and shows client-side code examples. Here we will concentrate on implementing server side.\\n\\nSome key points about WebTransport spec:\\n\\n* WebTransport standard will provide a possibility to use streaming client-server communication using modern transports such as [QUIC](https://en.wikipedia.org/wiki/QUIC) and [HTTP/3](https://en.wikipedia.org/wiki/HTTP/3)\\n* It can be a good alternative to [WebSocket](https://en.wikipedia.org/wiki/WebSocket) messaging, standard provides some capabilities that are not possible with current WebSocket spec: possibility to get rid of head-of-line blocking problems using individual streams for different data, the possibility to reuse a single connection to a server in different browser tabs\\n* WebTransport also defines an unreliable stream API using UDP datagrams (which is possible since QUIC is UDP-based) \u2013 which is what browsers did not have before without a rather complex [WebRTC](https://en.wikipedia.org/wiki/WebRTC) setup involving ICE, STUN, etc. This is sweet for in-browser real-time games.\\n\\nTo help you figure out things here are links to current WebTransport specs:\\n\\n* [WebTransport overview](https://tools.ietf.org/html/draft-vvv-webtransport-overview-01) \u2013 this spec gives an overview of WebTransport and provides requirements to transport layer\\n* [WebTransport over QUIC](https://tools.ietf.org/html/draft-vvv-webtransport-quic) \u2013 this spec describes QUIC-based transport for WebTransport\\n* [WebTransport over HTTP/3](https://tools.ietf.org/html/draft-vvv-webtransport-http3) \u2013 this spec describes HTTP/3-based transport for WebTransport (actually HTTP/3 is a protocol defined on top of QUIC)\\n\\nAt moment Chrome only implements [trial possibility](https://web.dev/quictransport/#register-for-ot) to try out WebTransport standard and only implements WebTransport over QUIC. Developers can initialize transport with code like this:\\n\\n```javascript\\nconst transport = new QuicTransport(\'quic-transport://localhost:4433/path\');\\n```\\n\\nIn case of HTTP/3 transport one will use URL like `\'https://localhost:4433/path\'` in transport constructor. All WebTransport underlying transports should support instantiation over URL \u2013 that\'s one of the spec requirements. \\n\\nI decided that this is a cool possibility to finally play with QUIC protocol and its Go implementation [github.com/lucas-clemente/quic-go](https://github.com/lucas-clemente/quic-go).\\n\\n:::danger\\n\\nPlease keep in mind that all things described in this post are work in progress. WebTransport drafts, Quic-Go library, even QUIC protocol itself are subjects to change. You should not use it in production yet.\\n\\n:::\\n\\n[Experimenting with QuicTransport](https://web.dev/quictransport/) post contains links to a [client example](https://googlechrome.github.io/samples/quictransport/client.html) and companion [Python server implementation](https://github.com/GoogleChrome/samples/blob/gh-pages/quictransport/quic_transport_server.py).\\n\\n![client example](https://i.imgur.com/Hty00aG.png)\\n\\nWe will use a linked client example to connect to a server that runs on localhost and uses [github.com/lucas-clemente/quic-go](https://github.com/lucas-clemente/quic-go) library. To make our example work we need to open client example in Chrome, and actually, at this moment we need to install Chrome Canary. The reason behind this is that the  `quic-go` library supports QUIC draft-29 while Chrome < 85 implements QuicTransport over draft-27. If you read this post at a time when Chrome stable 85 already released then most probably you don\'t need to install Canary release and just use your stable Chrome.\\n\\nWe also need to generate self-signed certificates since WebTransport only works with a TLS layer, and we should make Chrome trust our certificates. Let\'s prepare our client environment before writing a server and first install Chrome Canary.\\n\\n## Install Chrome Canary\\n\\nGo to https://www.google.com/intl/en/chrome/canary/, download and install Chrome Canary. We will use it to open [client example](https://googlechrome.github.io/samples/quictransport/client.html).\\n\\n:::note\\n\\nIf you have Chrome >= 85 then most probably you can skip this step.\\n\\n:::\\n\\n## Generate self-signed TLS certificates\\n\\nSince WebTransport based on modern network transports like QUIC and HTTP/3 security is a keystone. For our experiment we will create a self-signed TLS certificate using `openssl`. \\n\\nMake sure you have `openssl` installed:\\n\\n```bash\\n$ which openssl\\n/usr/bin/openssl\\n```\\n\\nThen run:\\n\\n```bash\\nopenssl genrsa -des3 -passout pass:x -out server.pass.key 2048\\nopenssl rsa -passin pass:x -in server.pass.key -out server.key\\nrm server.pass.key\\nopenssl req -new -key server.key -out server.csr\\n```\\n\\nSet `localhost` for Common Name when asked.\\n\\nThe self-signed TLS certificate generated from the `server.key` private key and `server.csr` files:\\n\\n```bash\\nopenssl x509 -req -sha256 -days 365 -in server.csr -signkey server.key -out server.crt\\n```\\n\\nAfter these manipulations you should have `server.crt` and `server.key` files in your working directory.\\n\\nTo help you with process here is my console output during these steps (click to open):\\n\\n??? example \\"My console output generating self-signed certificates\\"\\n    ```bash\\n    $ openssl genrsa -des3 -passout pass:x -out server.pass.key 2048\\n    Generating RSA private key, 2048 bit long modulus\\n    ...........................................................................................+++\\n    .....................+++\\n    e is 65537 (0x10001)\\n    \\n    $ ls\\n    server.pass.key\\n    \\n    $ openssl rsa -passin pass:x -in server.pass.key -out server.key\\n    writing RSA key\\n    \\n    $ ls\\n    server.key      server.pass.key\\n    \\n    $ rm server.pass.key\\n    \\n    $ openssl req -new -key server.key -out server.csr\\n    You are about to be asked to enter information that will be incorporated\\n    into your certificate request.\\n    What you are about to enter is what is called a Distinguished Name or a DN.\\n    There are quite a few fields but you can leave some blank\\n    For some fields there will be a default value,\\n    If you enter \'.\', the field will be left blank.\\n    -----\\n    Country Name (2 letter code) []:RU\\n    State or Province Name (full name) []:\\n    Locality Name (eg, city) []:\\n    Organization Name (eg, company) []:\\n    Organizational Unit Name (eg, section) []:\\n    Common Name (eg, fully qualified host name) []:localhost\\n    Email Address []:\\n    \\n    Please enter the following \'extra\' attributes\\n    to be sent with your certificate request\\n    A challenge password []:\\n    \\n    $ openssl x509 -req -sha256 -days 365 -in server.csr -signkey server.key -out server.crt\\n    Signature ok\\n    subject=/C=RU/CN=localhost\\n    Getting Private key\\n    \\n    $ ls\\n    server.crt server.csr server.key\\n    ``` \\n\\n## Run client example\\n\\nNow the last step. What we need to do is run Chrome Canary with some flags that will allow it to trust our self-signed certificates. I suppose there is an alternative way making Chrome trust your certificates, but I have not tried it.\\n\\nFirst let\'s find out a fingerprint of our cert:\\n\\n```bash\\nopenssl x509 -in server.crt -pubkey -noout | openssl pkey -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64\\n```\\n\\nIn my case base64 fingerprint was `pe2P0fQwecKFMc6kz3+Y5MuVwVwEtGXyST5vJeaOO/M=`, yours will be different.\\n\\nThen run Chrome Canary with some additional flags that will make it trust out certs (close other Chrome Canary instances before running it):\\n\\n```bash\\n$ /Applications/Google\\\\ Chrome\\\\ Canary.app/Contents/MacOS/Google\\\\ Chrome\\\\ Canary \\\\\\n    --origin-to-force-quic-on=localhost:4433 \\\\\\n    --ignore-certificate-errors-spki-list=pe2P0fQwecKFMc6kz3+Y5MuVwVwEtGXyST5vJeaOO/M=\\n```\\n\\nThis example is for MacOS, for your system see [docs on how to run Chrome/Chromium with custom flags](https://www.chromium.org/developers/how-tos/run-chromium-with-flags).\\n\\nNow you can open https://googlechrome.github.io/samples/quictransport/client.html URL in started browser and click `Connect` button. What? Connection not established? OK, this is fine since we need to run our server :)\\n\\n## Writing a QUIC server\\n\\nMaybe in future we will have libraries that are specified to work with WebTransport over QUIC or HTTP/3, but for now we should implement server manually. As said above we will use [github.com/lucas-clemente/quic-go](https://github.com/lucas-clemente/quic-go) library to do this.\\n\\n### Server skeleton\\n\\nFirst, let\'s define a simple skeleton for our server:\\n\\n```go\\npackage main\\n\\nimport (\\n\\t\\"errors\\"\\n\\t\\"log\\"\\n\\n    \\"github.com/lucas-clemente/quic-go\\"\\n)\\n\\n// Config for WebTransportServerQuic.\\ntype Config struct {\\n\\t// ListenAddr sets an address to bind server to.\\n\\tListenAddr string\\n\\t// TLSCertPath defines a path to .crt cert file.\\n\\tTLSCertPath string\\n\\t// TLSKeyPath defines a path to .key cert file\\n\\tTLSKeyPath string\\n\\t// AllowedOrigins represents list of allowed origins to connect from.\\n\\tAllowedOrigins []string\\n}\\n\\n// WebTransportServerQuic can handle WebTransport QUIC connections according\\n// to https://tools.ietf.org/html/draft-vvv-webtransport-quic-02.\\ntype WebTransportServerQuic struct {\\n\\tconfig Config\\n}\\n\\n// NewWebTransportServerQuic creates new WebTransportServerQuic.\\nfunc NewWebTransportServerQuic(config Config) *WebTransportServerQuic {\\n\\treturn &WebTransportServerQuic{\\n\\t\\tconfig: config,\\n\\t}\\n}\\n\\n// Run server.\\nfunc (s *WebTransportServerQuic) Run() error {\\n\\treturn errors.New(\\"not implemented\\")\\n}\\n\\nfunc main() {\\n\\tserver := NewWebTransportServerQuic(Config{\\n\\t\\tListenAddr:     \\"0.0.0.0:4433\\",\\n\\t\\tTLSCertPath:    \\"server.crt\\",\\n\\t\\tTLSKeyPath:     \\"server.key\\",\\n\\t\\tAllowedOrigins: []string{\\"localhost\\", \\"googlechrome.github.io\\"},\\n\\t})\\n\\tif err := server.Run(); err != nil {\\n\\t\\tlog.Fatal(err)\\n\\t}\\n}\\n```\\n\\n### Accept QUIC connections\\n\\nLet\'s concentrate on implementing `Run` method. We need to accept QUIC client connections. This can be done by creating `quic.Listener` instance and using its `.Accept` method to accept incoming client sessions.\\n\\n```go\\n// Run server.\\nfunc (s *WebTransportServerQuic) Run() error {\\n\\tlistener, err := quic.ListenAddr(s.config.ListenAddr, s.generateTLSConfig(), nil)\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tfor {\\n\\t\\tsess, err := listener.Accept(context.Background())\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\t\\tlog.Printf(\\"session accepted: %s\\", sess.RemoteAddr().String())\\n\\t\\tgo func() {\\n\\t\\t\\tdefer func() {\\n\\t\\t\\t\\t_ = sess.CloseWithError(0, \\"bye\\")\\n\\t\\t\\t\\tlog.Println(\\"close session\\")\\n\\t\\t\\t}()\\n\\t\\t\\ts.handleSession(sess)\\n\\t\\t}()\\n\\t}\\n}\\n\\nfunc (s *WebTransportServerQuic) handleSession(sess quic.Session) {\\n    // Not implemented yet.    \\n}\\n```\\n\\nAn interesting thing to note is that QUIC allows closing connection with specific application-level integer code and custom string reason. Just like WebSocket if you worked with it.\\n\\nAlso note, that we are starting our `Listener` with TLS configuration returned by `s.generateTLSConfig()` method. Let\'s take a closer look at how this method can be implemented.\\n\\n```go\\n// https://tools.ietf.org/html/draft-vvv-webtransport-quic-02#section-3.1\\nconst alpnQuicTransport = \\"wq-vvv-01\\"\\n\\nfunc (s *WebTransportServerQuic) generateTLSConfig() *tls.Config {\\n\\tcert, err := tls.LoadX509KeyPair(s.config.TLSCertPath, s.config.TLSKeyPath)\\n\\tif err != nil {\\n\\t\\tlog.Fatal(err)\\n\\t}\\n\\treturn &tls.Config{\\n\\t\\tCertificates: []tls.Certificate{cert},\\n\\t\\tNextProtos:   []string{alpnQuicTransport},\\n\\t}\\n}\\n```\\n\\nInside `generateTLSConfig` we load x509 certs from cert files generated above. WebTransport uses ALPN ([Application-Layer Protocol Negotiation](https://en.wikipedia.org/wiki/Application-Layer_Protocol_Negotiation) to prevent handshakes with a server that does not support WebTransport spec. This is just a string `wq-vvv-01` inside `NextProtos` slice of our `*tls.Config`.\\n\\n### Connection Session handling\\n\\nAt this moment if you run a server and open a client example in Chrome then click `Connect` button \u2013 you should see that connection successfully established in event log area:\\n\\n![client example](https://i.imgur.com/PyEr9W9.png)\\n\\nNow if you try to send data to a server nothing will happen. That\'s because we have not implemented reading data from session streams. \\n\\nStreams in QUIC provide a lightweight, ordered byte-stream abstraction to an application. Streams can be unidirectional or bidirectional.\\n\\nStreams can be short-lived, streams can also be long-lived and can last the entire duration of a connection.\\n\\nClient example provides three possible ways to communicate with a server:\\n\\n* Send a datagram\\n* Open a unidirectional stream\\n* Open a bidirectional stream\\n\\nUnfortunately, `quic-go` library does not support sending UDP datagrams at this moment. To do this `quic-go` should implement one more draft called [An Unreliable Datagram Extension to QUIC](https://tools.ietf.org/html/draft-pauly-quic-datagram-05). There is already [an ongoing pull request](https://github.com/lucas-clemente/quic-go/pull/2162) that implements it. This means that it\'s too early for us to experiment with unreliable UDP WebTransport client-server communication in Go. By the way, the interesting facts about UDP over QUIC are that QUIC congestion control mechanism will [still apply](https://tools.ietf.org/html/draft-ietf-quic-datagram-00#section-5.3) and QUIC datagrams [can support acknowledgements](https://tools.ietf.org/html/draft-ietf-quic-datagram-00#section-5.1).\\n\\nImplementing a unidirectional stream is possible with `quic-go` since the library supports creating and accepting unidirectional streams, but I\'ll leave this for a reader (though we will need accepting one unidirectional stream for parsing client indication anyway \u2013 see below).\\n\\nHere we will only concentrate on implementing a server for a bidirectional case. We are in the Centrifugo blog, and this is the most interesting type of stream for me personally.\\n\\n### Parsing client indication\\n\\nAccording to [section-3.2](https://tools.ietf.org/html/draft-vvv-webtransport-quic-02#section-3.2) of Quic WebTransport spec in order to verify that the client\'s origin allowed connecting to the server, the user agent has to communicate the origin to the server. This is accomplished by sending a special message, called client indication, on stream 2, which is the first client-initiated unidirectional stream.\\n\\nHere we will implement this. In the beginning of our session handler we will accept a unidirectional stream initiated by a client.\\n\\nAt moment spec defines two client indication keys: `Origin` and `Path`. In our case an origin value will be `https://googlechrome.github.io` and path will be `/counter`.\\n\\nLet\'s define some constants and structures:\\n\\n```go\\n// client indication stream can not exceed 65535 bytes in length.\\n// https://tools.ietf.org/html/draft-vvv-webtransport-quic-02#section-3.2\\nconst maxClientIndicationLength = 65535\\n\\n// define known client indication keys.\\ntype clientIndicationKey int16\\n\\nconst (\\n\\tclientIndicationKeyOrigin clientIndicationKey = 0\\n\\tclientIndicationKeyPath                       = 1\\n)\\n\\n// ClientIndication container.\\ntype ClientIndication struct {\\n\\t// Origin client indication value.\\n\\tOrigin string\\n\\t// Path client indication value.\\n\\tPath string\\n}\\n```\\n\\nNow what we should do is accept unidirectional stream inside session handler:\\n\\n```go\\nfunc (s *WebTransportServerQuic) handleSession(sess quic.Session) {\\n    stream, err := sess.AcceptUniStream(context.Background())\\n    if err != nil {\\n        log.Println(err)\\n        return\\n    }\\n    log.Printf(\\"uni stream accepted, id: %d\\", stream.StreamID())\\n\\n    indication, err := receiveClientIndication(stream)\\n    if err != nil {\\n        log.Println(err)\\n        return\\n    }\\n    log.Printf(\\"client indication: %+v\\", indication)\\n\\n    if err := s.validateClientIndication(indication); err != nil {\\n        log.Println(err)\\n        return\\n    }\\n\\n    // this method blocks.\\n    if err := s.communicate(sess); err != nil {\\n        log.Println(err)\\n    }\\n}\\n\\nfunc receiveClientIndication(stream quic.ReceiveStream) (ClientIndication, error) {\\n    return ClientIndication{}, errors.New(\\"not implemented yet\\")\\n}\\n\\nfunc (s *WebTransportServerQuic) validateClientIndication(indication ClientIndication) error {\\n\\treturn errors.New(\\"not implemented yet\\")\\n}\\n\\nfunc (s *WebTransportServerQuic) communicate(sess quic.Session) error {\\n    return errors.New(\\"not implemented yet\\")\\n}\\n```\\n\\nAs you can see to accept a unidirectional stream with data we can use `.AcceptUniStream` method of `quic.Session`. After accepting a stream we should read client indication data from it. \\n\\nAccording to spec it will contain a client indication in the following format:\\n\\n```\\n0                   1                   2                   3\\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\\n|           Key (16)            |          Length (16)          |\\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\\n|                           Value (*)                         ...\\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\\n```\\n\\nThe code below parses client indication out of a stream data, we decode key-value pairs from uni stream until an end of stream (indicated by EOF):\\n\\n```go\\nfunc receiveClientIndication(stream quic.ReceiveStream) (ClientIndication, error) {\\n\\tvar clientIndication ClientIndication\\n\\n    // read no more than maxClientIndicationLength bytes.\\n\\treader := io.LimitReader(stream, maxClientIndicationLength)\\n\\n\\tdone := false\\n\\n\\tfor {\\n\\t\\tif done {\\n\\t\\t\\tbreak\\n\\t\\t}\\n\\t\\tvar key int16\\n\\t\\terr := binary.Read(reader, binary.BigEndian, &key)\\n\\t\\tif err != nil {\\n\\t\\t\\tif err == io.EOF {\\n\\t\\t\\t\\tdone = true\\n\\t\\t\\t} else {\\n\\t\\t\\t\\treturn clientIndication, err\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tvar valueLength int16\\n\\t\\terr = binary.Read(reader, binary.BigEndian, &valueLength)\\n\\t\\tif err != nil {\\n\\t\\t\\treturn clientIndication, err\\n\\t\\t}\\n\\t\\tbuf := make([]byte, valueLength)\\n\\t\\tn, err := reader.Read(buf)\\n\\t\\tif err != nil {\\n\\t\\t\\tif err == io.EOF {\\n                // still need to process indication value.\\n\\t\\t\\t\\tdone = true\\n\\t\\t\\t} else {\\n\\t\\t\\t\\treturn clientIndication, err\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tif int16(n) != valueLength {\\n\\t\\t\\treturn clientIndication, errors.New(\\"read less than expected\\")\\n\\t\\t}\\n\\t\\tvalue := string(buf)\\n\\n\\t\\tswitch clientIndicationKey(key) {\\n\\t\\tcase clientIndicationKeyOrigin:\\n\\t\\t\\tclientIndication.Origin = value\\n\\t\\tcase clientIndicationKeyPath:\\n\\t\\t\\tclientIndication.Path = value\\n\\t\\tdefault:\\n\\t\\t\\tlog.Printf(\\"skip unknown client indication key: %d: %s\\", key, value)\\n\\t\\t}\\n\\t}\\n\\treturn clientIndication, nil\\n}\\n```\\n\\nWe also validate Origin inside `validateClientIndication` method of our server:\\n\\n```go\\nvar errBadOrigin = errors.New(\\"bad origin\\")\\n\\nfunc (s *WebTransportServerQuic) validateClientIndication(indication ClientIndication) error {\\n\\tu, err := url.Parse(indication.Origin)\\n\\tif err != nil {\\n\\t\\treturn errBadOrigin\\n\\t}\\n\\tif !stringInSlice(u.Host, s.config.AllowedOrigins) {\\n\\t\\treturn errBadOrigin\\n\\t}\\n\\treturn nil\\n}\\n\\nfunc stringInSlice(a string, list []string) bool {\\n\\tfor _, b := range list {\\n\\t\\tif b == a {\\n\\t\\t\\treturn true\\n\\t\\t}\\n\\t}\\n\\treturn false\\n}\\n```\\n\\nDo you have `stringInSlice` function in every Go project? I do :)\\n\\n### Communicating over bidirectional streams\\n\\nThe final part here is accepting a bidirectional stream from a client, reading it, and sending responses back. Here we will just echo everything a client sends to a server back to a client. You can implement whatever bidirectional communication you want actually.\\n\\nVery similar to unidirectional case we can call `.AcceptStream` method of session to accept a bidirectional stream.\\n\\n```go\\nfunc (s *WebTransportServerQuic) communicate(sess quic.Session) error {\\n\\tfor {\\n\\t\\tstream, err := sess.AcceptStream(context.Background())\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\t\\tlog.Printf(\\"stream accepted: %d\\", stream.StreamID())\\n\\t\\tif _, err := io.Copy(stream, stream); err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\t}\\n}\\n```\\n\\nWhen you press `Send` button in client example it creates a bidirectional stream, sends data to it, then closes stream. Thus our code is sufficient. For a more complex communication that involves many concurrent streams you will have to write a more complex code that allows working with streams concurrently on server side.\\n\\n![client example](https://i.imgur.com/5299Vr4.png)\\n\\n### Full server example\\n\\nFull server code can be found [in a Gist](https://gist.github.com/FZambia/07dca3a7a75a264746101cd5657f1150). Again \u2013 this is a toy example based on things that all work in progress.\\n\\n## Conclusion\\n\\nWebTransport is an interesting technology that can open new possibilities in modern Web development. At this moment it\'s possible to play with it using QUIC transport \u2013 here we looked at how one can do that. Though we still have to wait a bit until all these things will be suitable for production usage.\\n\\nAlso, even when ready we will still have to think about WebTransport fallback options \u2013 since wide adoption of browsers that support some new technology and infrastructure takes time. Actually WebTransport spec authors consider fallback options in design. This was mentioned in IETF slides ([PDF, 2.6MB](https://www.ietf.org/proceedings/106/slides/slides-106-webtrans-webtrans-bof-slides-03)), but I have not found any additional information beyond that.\\n\\nPersonally, I think the most exciting thing about WebTransport is the possibility to exchange UDP datagrams, which can help a lot to in-browser gaming. Unfortunately, we can\'t test it at this moment with Go (but it\'s already possible using Python as server as shown [in the example](https://github.com/GoogleChrome/samples/blob/gh-pages/quictransport/quic_transport_server.py)).\\n\\nWebTransport could be a nice candidate for a new Centrifugo transport next to WebSocket and SockJS \u2013 time will show."},{"id":"/2020/02/10/million-connections-with-centrifugo","metadata":{"permalink":"/blog/2020/02/10/million-connections-with-centrifugo","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2020-02-10-million-connections-with-centrifugo.md","source":"@site/blog/2020-02-10-million-connections-with-centrifugo.md","title":"Million connections with Centrifugo","description":"Describing a test stand in Kubernetes where we connect one million websocket connections to a server, using Redis to scale nodes, and providing insights about hardware resources required to achieve 500k messages per second","date":"2020-02-10T00:00:00.000Z","tags":[{"label":"centrifuge","permalink":"/blog/tags/centrifuge"},{"label":"go","permalink":"/blog/tags/go"}],"readingTime":3.045,"hasTruncateMarker":true,"authors":[{"name":"Centrifugal team","title":"Let the Centrifugal force be with you","imageURL":"/img/logo_animated.svg"}],"frontMatter":{"title":"Million connections with Centrifugo","tags":["centrifuge","go"],"description":"Describing a test stand in Kubernetes where we connect one million websocket connections to a server, using Redis to scale nodes, and providing insights about hardware resources required to achieve 500k messages per second","author":"Centrifugal team","authorTitle":"Let the Centrifugal force be with you","authorImageURL":"/img/logo_animated.svg","image":"/img/million_conns.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Experimenting with QUIC and WebTransport","permalink":"/blog/2020/10/16/experimenting-with-quic-transport"}},"content":"<img src=\\"/img/million_conns.jpg\\" />\\n\\nIn order to get an understanding about possible hardware requirements for reasonably massive Centrifugo setup we made a test stand inside Kubernetes.\\n\\nOur goal was to run server based on Centrifuge library (the core of Centrifugo server) with one million WebSocket connections and send many messages to connected clients. While sending many messages we have been looking at delivery time latency. In fact we will see that about 30 million messages per minute (500k messages per second) will be delivered to connected clients and latency won\'t be larger than 200ms in 99 percentile.\\n\\n\x3c!--truncate--\x3e\\n\\nServer nodes have been run on machines with the following configuration:\\n\\n* CPU Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz\\n* Linux Debian 4.9.65-3+deb9u1 (2017-12-23) x86_64 GNU/Linux \\n\\nSome `sysctl` values:\\n\\n```\\nfs.file-max = 3276750\\nfs.nr_open = 1048576\\nnet.ipv4.tcp_mem = 3086496\\t4115330\\t6172992\\nnet.ipv4.tcp_rmem = 8192\\t8388608\\t16777216\\nnet.ipv4.tcp_wmem = 4096\\t4194394\\t16777216\\nnet.core.rmem_max = 33554432\\nnet.core.wmem_max = 33554432\\n```\\n\\nKubernetes used these machines as its nodes. \\n\\nWe started 20 Centrifuge-based server pods. Our clients connected to server pods using Centrifuge Protobuf protocol. To scale horizontally we used Redis Engine and sharded it to 5 different Redis instances (each Redis instance consumes 1 CPU max).\\n\\nTo achieve many client connections we used 100 Kubernetes pods each generating about 10k client connections to server.\\n\\nHere are some numbers we achieved:\\n\\n* 1 million WebSocket connections\\n* Each connection subscribed to 2 channels: one personal channel and one group channel (with 10 subscribers in it), i.e. we had about 1.1 million active channels at each moment.\\n* 28 million messages per minute (about 500k per second) **delivered** to clients\\n* 200k per minute constant connect/disconnect rate to simulate real-life situation where clients connect/disconnect from server\\n* 200ms delivery latency in 99 percentile\\n* The size of each published message was about 100 bytes\\n\\nAnd here are some numbers about final resource usage on server side (we don\'t actually interested in client side resource usage here):\\n\\n* 40 CPU total for server nodes when load achieved values claimed above (20 pods, ~2 CPU each)\\n* 27 GB of RAM used mostly to handle 1 mln WebSocket connections, i.e. about 30kb RAM per connection\\n* 0.32 CPU usage on every Redis instance\\n* 100 mbit/sec rx \u0438 150 mbit/sec tx of network used on each server pod\\n\\nThe picture that demonstrates experiment (better to open image in new tab):\\n\\n![Benchmark](/img/benchmark.gif)\\n\\nThis also demonstrates that to handle one million of WebSocket connections without many messages sent to clients you need about 10 CPU total for server nodes and about 5% of CPU on each of Redis instances. In this case CPU mostly spent on connect/disconnect flow, ping/pong frames, subscriptions to channels.\\n\\nIf we enable history and history message recovery features we see an increased Redis CPU usage: 64% instead of 32% on the same workload. Other resources usage is pretty the same.\\n\\nThe results mean that one can theoretically achieve the comparable numbers on single modern server machine. But numbers can vary a lot in case of different load scenarios. In this benchmark we looked at basic use case where we only connect many clients and send Publications to them. There are many features in Centrifuge library and in Centrifugo not covered by this artificial experiment. Also note that though benchmark was made for Centrifuge library for Centrifugo you can expect similar results.\\n\\nRead and write buffer sizes of websocket connections were set to 512 kb on server side (sizes of buffers affect memory usage), with Centrifugo this means that to reproduce the same configuration you need to set:\\n\\n```json\\n{\\n    ...\\n    \\"websocket_read_buffer_size\\": 512,\\n    \\"websocket_write_buffer_size\\": 512\\n}\\n```"}]}}')}}]);