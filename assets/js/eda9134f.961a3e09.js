"use strict";(self.webpackChunkcentrifugal_dev=self.webpackChunkcentrifugal_dev||[]).push([[1648],{28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var s=t(96540);const i={},r=s.createContext(i);function a(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(r.Provider,{value:n},e.children)}},28518:e=>{e.exports=JSON.parse('{"permalink":"/blog/2026/03/01/scaling-ai-token-streams-with-centrifugo","editUrl":"https://github.com/centrifugal/centrifugal.dev/edit/main/blog/2026-03-01-scaling-ai-token-streams-with-centrifugo.md","source":"@site/blog/2026-03-01-scaling-ai-token-streams-with-centrifugo.md","title":"Scaling AI token streams with Centrifugo","description":"An interactive playground demonstrating what Centrifugo brings to AI token streaming \u2014 aggregation, recovery, multi-tab sync, transport fallbacks, and horizontal scaling with Redis.","date":"2026-03-01T00:00:00.000Z","tags":[{"inline":true,"label":"centrifugo","permalink":"/blog/tags/centrifugo"},{"inline":true,"label":"ai","permalink":"/blog/tags/ai"},{"inline":true,"label":"streaming","permalink":"/blog/tags/streaming"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"websocket","permalink":"/blog/tags/websocket"},{"inline":true,"label":"sse","permalink":"/blog/tags/sse"}],"readingTime":10.52,"hasTruncateMarker":true,"authors":[{"name":"Alexander Emelin","title":"Founder of Centrifugal Labs","imageURL":"/img/alexander_emelin.jpeg","key":null,"page":null}],"frontMatter":{"title":"Scaling AI token streams with Centrifugo","tags":["centrifugo","ai","streaming","tutorial","websocket","sse"],"description":"An interactive playground demonstrating what Centrifugo brings to AI token streaming \u2014 aggregation, recovery, multi-tab sync, transport fallbacks, and horizontal scaling with Redis.","author":"Alexander Emelin","authorTitle":"Founder of Centrifugal Labs","authorImageURL":"/img/alexander_emelin.jpeg","image":"/img/scale_ai.jpg","hide_table_of_contents":false},"unlisted":false,"nextItem":{"title":"Publication filtering by tags - reducing bandwidth with server-side stream filtering","permalink":"/blog/2025/10/14/server-side-publication-filtering-by-tags"}}')},86957:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var s=t(28518),i=t(74848),r=t(28453);const a={title:"Scaling AI token streams with Centrifugo",tags:["centrifugo","ai","streaming","tutorial","websocket","sse"],description:"An interactive playground demonstrating what Centrifugo brings to AI token streaming \u2014 aggregation, recovery, multi-tab sync, transport fallbacks, and horizontal scaling with Redis.",author:"Alexander Emelin",authorTitle:"Founder of Centrifugal Labs",authorImageURL:"/img/alexander_emelin.jpeg",image:"/img/scale_ai.jpg",hide_table_of_contents:!1},o=void 0,c={authorsImageUrls:[void 0]},l=[{value:"The playground",id:"the-playground",level:2},{value:"1. Token streaming and aggregation",id:"1-token-streaming-and-aggregation",level:2},{value:"2. Stream recovery",id:"2-stream-recovery",level:2},{value:"3. Redis for scaling and persistence",id:"3-redis-for-scaling-and-persistence",level:2},{value:"4. Transport fallbacks",id:"4-transport-fallbacks",level:2},{value:"5. Multi-tab sync with PostgreSQL",id:"5-multi-tab-sync-with-postgresql",level:2},{value:"How it works",id:"how-it-works",level:3},{value:"The pattern",id:"the-pattern",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("img",{src:"/img/scale_ai.jpg"}),"\n",(0,i.jsxs)(n.p,{children:["In a ",(0,i.jsx)(n.a,{href:"/blog/2025/06/17/streaming-ai-gpt-responses-with-centrifugo",children:"previous post"})," we showed how to stream LLM responses through Centrifugo \u2014 backend receives tokens from an LLM API, publishes them to a channel, browser subscribes and renders text as it arrives."]}),"\n",(0,i.jsx)(n.p,{children:"That post covered the basics. This one looks at what Centrifugo adds beyond just delivering tokens from point A to point B \u2014 automatic recovery after disconnects, horizontal scaling, transport flexibility, and multi-tab synchronization backed by a database. We built an interactive playground demo that demonstrates the concepts \u2013 you can run it locally and see every feature in action."}),"\n",(0,i.jsx)(n.h2,{id:"the-playground",children:"The playground"}),"\n",(0,i.jsxs)(n.p,{children:["The source code is on ",(0,i.jsx)(n.a,{href:"https://github.com/centrifugal/examples/tree/master/v6/scale-ai",children:"GitHub"}),". Run it:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/centrifugal/examples.git\ncd examples/v6/scale-ai\ndocker compose up --build\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Open ",(0,i.jsx)(n.a,{href:"http://localhost:9000",children:"http://localhost:9000"}),"."]}),"\n",(0,i.jsx)(n.p,{children:'The playground simulates an AI token stream without requiring an actual LLM API. You control the token rate, total token count, and other parameters. The backend picks a random AI-related question, generates random words as the "answer", and publishes them to Centrifugo \u2014 the delivery path is identical to a real LLM integration.'}),"\n",(0,i.jsx)(n.p,{children:"The architecture:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                 \u2502    nginx  :9000     \u2502\n                 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2518\n                    \u2502               \u2502\n                    \u25bc               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 postgres \u2502\u25c0\u2500\u2500\u2502 backend \u2502\u2500\u2500\u25b6\u2502 centrifugo \u2502\u2500\u2500\u25b6\u2502 redis \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  :5000  \u2502   \u2502   :8000    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Nginx serves the frontend and proxies ",(0,i.jsx)(n.code,{children:"/api"})," to the backend, ",(0,i.jsx)(n.code,{children:"/connection"})," and ",(0,i.jsx)(n.code,{children:"/emulation"})," to Centrifugo. The backend publishes tokens via Centrifugo HTTP API and persists stream state in PostgreSQL. The frontend subscribes to channels using centrifuge-js."]}),"\n",(0,i.jsx)(n.p,{children:"Let's walk through each feature."}),"\n",(0,i.jsx)(n.h2,{id:"1-token-streaming-and-aggregation",children:"1. Token streaming and aggregation"}),"\n",(0,i.jsx)(n.p,{children:"Start a stream with default settings: 30 tokens/sec, 100 tokens total. Tokens appear in real-time in the output area. The stats bar shows messages and tokens incrementing."}),"\n",(0,i.jsx)("video",{width:"100%",loop:!0,autoPlay:"autoplay",muted:!0,controls:"",src:"/img/scale_ai_baseline.mp4"}),"\n",(0,i.jsx)(n.p,{children:"This is the simplest case \u2014 one token per message, one message per publish call to Centrifugo HTTP API:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async def publish_to_centrifugo(channel: str, data: dict):\n    await http_client.post(\n        CENTRIFUGO_API_URL,\n        json={"channel": channel, "data": data},\n        headers={"X-API-Key": CENTRIFUGO_API_KEY},\n    )\n'})}),"\n",(0,i.jsx)(n.p,{children:"On the client side, the subscription receives each token:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"subscription.on('publication', (ctx) => {\n    const msg = ctx.data;\n    if (msg.text) {\n        appendText(msg.text + ' ');\n    }\n});\n"})}),"\n",(0,i.jsx)(n.p,{children:"With default settings, 100 tokens = 100 messages. You could achieve this with plain SSE \u2014 one endpoint, one event stream. But production AI streaming hits problems that SSE alone doesn't solve: what happens when the connection drops mid-stream? When the user switches networks? When you need to scale beyond one server? When multiple tabs need the same stream? The rest of this post walks through how Centrifugo handles each of these at the infrastructure layer."}),"\n",(0,i.jsx)(n.p,{children:'Before moving on \u2014 try toggling "Publisher-side aggregation" in the controls, set it to 5, and start a new stream.'}),"\n",(0,i.jsx)("video",{width:"100%",loop:!0,autoPlay:"autoplay",muted:!0,controls:"",src:"/img/scale_ai_aggregation.mp4"}),"\n",(0,i.jsx)(n.p,{children:"The token count still reaches 100, but the message count drops to ~20. The backend buffers N tokens and publishes them as a batch. At 80 tokens/sec with aggregation of 5, you go from 80 to 16 publish calls per second per stream \u2014 fewer syscalls at every layer, and the text still flows naturally. This is a general technique, not Centrifugo-specific, but it composes well with Centrifugo's delivery."}),"\n",(0,i.jsx)(n.h2,{id:"2-stream-recovery",children:"2. Stream recovery"}),"\n",(0,i.jsx)(n.p,{children:'Start a stream and click "Simulate Disconnect" while tokens are flowing. You\'ll see:'}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"[DISCONNECTED]"})," marker \u2014 the client called ",(0,i.jsx)(n.code,{children:"centrifuge.disconnect()"})]}),"\n",(0,i.jsx)(n.li,{children:"Then 2.5 second gap \u2014 tokens keep being published by the backend, but the client isn't connected"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"[RECONNECTING...]"})," marker \u2014 the client calls ",(0,i.jsx)(n.code,{children:"centrifuge.connect()"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"[RECOVERED]"})," marker \u2014 all missed tokens arrive at once, the stream continues"]}),"\n"]}),"\n",(0,i.jsx)("video",{width:"100%",loop:!0,autoPlay:"autoplay",muted:!0,controls:"",src:"/img/scale_ai_recovery.mp4"}),"\n",(0,i.jsx)(n.p,{children:"The recovery detection on the client:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"subscription.on('subscribed', (ctx) => {\n    if (ctx.wasRecovering && ctx.recovered) {\n        appendMarker(' [RECOVERED] ', 'text-green-400');\n    }\n});\n"})}),"\n",(0,i.jsxs)(n.p,{children:["This is Centrifugo's ",(0,i.jsx)(n.a,{href:"/docs/server/history_and_recovery",children:"history-based recovery"}),". The channel is configured with ",(0,i.jsx)(n.code,{children:"history_size: 500"})," and ",(0,i.jsx)(n.code,{children:"force_recovery: true"}),". When the client reconnects, Centrifugo sends all publications that were missed during the disconnect. The client SDK handles this transparently \u2014 the ",(0,i.jsx)(n.code,{children:"publication"})," event fires for each missed message in order."]}),"\n",(0,i.jsxs)(n.p,{children:["In the playground we call ",(0,i.jsx)(n.code,{children:"disconnect()"})," and ",(0,i.jsx)(n.code,{children:"connect()"})," manually to make the gap visible. In a real application you wouldn't do this \u2014 Centrifugo SDKs have built-in reconnect with exponential backoff. When the network drops, the client reconnects automatically and recovery happens without any application code involved. The SDK handles the full cycle: detect disconnect, reconnect, re-subscribe, recover missed messages."]}),"\n",(0,i.jsx)(n.p,{children:"Building this from scratch is a real project: reconnect logic, offset tracking on the client, message buffering on the server, a catch-up protocol. With Centrifugo it's a config option and SDK behavior you get out of the box."}),"\n",(0,i.jsx)(n.p,{children:"Mobile networks drop, laptops sleep, WiFi switches. Users won't notice a brief gap if the response continues from where it left off."}),"\n",(0,i.jsx)(n.h2,{id:"3-redis-for-scaling-and-persistence",children:"3. Redis for scaling and persistence"}),"\n",(0,i.jsx)(n.p,{children:"The playground already runs Centrifugo with Redis as the engine:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "engine": {\n    "type": "redis",\n    "redis": {\n      "address": "redis:6379"\n    }\n  }\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"This is what makes recovery work \u2014 channel history is stored in Redis, not in Centrifugo's process memory. If you restart Centrifugo while a stream is active, recovery still works because the history survives in Redis."}),"\n",(0,i.jsx)(n.p,{children:"Redis also decouples the real-time layer from the backend. The backend publishes tokens to Centrifugo via HTTP API, then moves on \u2014 it doesn't hold WebSocket connections, doesn't track who's listening, doesn't buffer messages. Centrifugo and Redis handle all of that. This means you can scale each layer independently: add more backend instances to handle more concurrent LLM calls, add more Centrifugo nodes to handle more subscribers. They share state through Redis \u2014 the backend publishes to any Centrifugo node, Redis routes messages to the node that holds the subscriber's connection."}),"\n",(0,i.jsx)(n.p,{children:"The playground uses a single Redis and single Centrifugo instance, but the architecture is ready for horizontal scaling \u2014 just add more Centrifugo nodes pointing to the same Redis. In production, Centrifugo supports Redis Cluster and Redis Sentinel for high availability."}),"\n",(0,i.jsx)(n.h2,{id:"4-transport-fallbacks",children:"4. Transport fallbacks"}),"\n",(0,i.jsx)(n.p,{children:"The playground has a transport selector: WebSocket, HTTP Streaming, SSE. Try starting a stream with each \u2014 the token delivery works identically regardless of the transport underneath. If you switch transport mid-stream, recovery kicks in and delivers missed tokens, but the real point is that the application code doesn't change at all."}),"\n",(0,i.jsx)("video",{width:"100%",loop:!0,autoPlay:"autoplay",muted:!0,controls:"",src:"/img/scale_ai_transport.mp4"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"function buildTransports(type) {\n    switch (type) {\n        case 'websocket':\n            return [{ transport: 'websocket', endpoint: `${proto}//${host}/connection/websocket` }];\n        case 'http_stream':\n            return [{ transport: 'http_stream', endpoint: `${httpProto}//${host}/connection/http_stream` }];\n        case 'sse':\n            return [{ transport: 'sse', endpoint: `${httpProto}//${host}/connection/sse` }];\n    }\n}\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Why this matters: corporate networks often run TLS-intercepting proxies that terminate HTTPS, inspect traffic, and re-encrypt it. These proxies may not forward the HTTP ",(0,i.jsx)(n.code,{children:"Upgrade"})," handshake that WebSocket requires \u2014 even when the original connection is over TLS. If your only transport is WebSocket, those users silently fail to connect. With Centrifugo, you configure SSE and HTTP-streaming as fallbacks. The client SDK can try transports in order and use the first one that connects."]}),"\n",(0,i.jsx)(n.p,{children:"From the application's perspective, nothing changes \u2014 the subscription API, recovery, channel multiplexing all work identically regardless of the underlying transport. The Centrifugo config to enable these:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "sse": { "enabled": true },\n  "http_stream": { "enabled": true }\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:["For SSE and HTTP-streaming, Centrifugo uses an emulation layer for the bidirectional part (subscribe/unsubscribe commands). Nginx needs to proxy the ",(0,i.jsx)(n.code,{children:"/emulation"})," endpoint in addition to ",(0,i.jsx)(n.code,{children:"/connection"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["A related point worth mentioning: services like OpenAI use SSE for streaming, partly because SSE over HTTP/2 lets multiple streams share a single TCP connection. Centrifugo supports ",(0,i.jsx)(n.a,{href:"/docs/transports/websocket#websocket-over-http2-rfc-8441",children:"WebSocket over HTTP/2 (RFC 8441)"}),", where each WebSocket connection becomes an HTTP/2 stream inside a shared HTTP/2 connection. You get the multiplexing benefits of SSE/HTTP/2 while keeping bidirectional communication and Centrifugo features like recovery."]}),"\n",(0,i.jsx)(n.h2,{id:"5-multi-tab-sync-with-postgresql",children:"5. Multi-tab sync with PostgreSQL"}),"\n",(0,i.jsx)(n.p,{children:"The features above work within a single tab's lifecycle. But what happens when a user opens a second tab? Or navigates back to a page while a stream is still running? The tab needs to discover the active stream, catch up on tokens it missed, and continue receiving live updates."}),"\n",(0,i.jsx)(n.p,{children:"The playground demonstrates this. Start a stream in one tab, then open a fresh tab \u2014 it automatically picks up the same stream, shows accumulated tokens, and continues with live ones."}),"\n",(0,i.jsx)("video",{width:"100%",loop:!0,autoPlay:"autoplay",muted:!0,controls:"",src:"/img/scale_ai_multi_tabs.mp4"}),"\n",(0,i.jsx)(n.h3,{id:"how-it-works",children:"How it works"}),"\n",(0,i.jsxs)(n.p,{children:["The backend persists each stream in PostgreSQL \u2014 the question, accumulated answer, and status. Only two writes happen: an ",(0,i.jsx)(n.code,{children:"INSERT"})," when the stream starts and an ",(0,i.jsx)(n.code,{children:"UPDATE"})," when it finishes (with the complete answer and ",(0,i.jsx)(n.code,{children:"status='done'"}),"). No database writes during streaming \u2014 Centrifugo handles real-time delivery."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Discovering streams on page load."})," When a tab opens, it calls ",(0,i.jsx)(n.code,{children:"GET /api/stream/active"})," to find the most recent stream. If the stream is still active, the tab joins it. If it's already done, the tab shows the full question and answer from the database."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Discovering streams in real-time."})," What about a tab that's already open when someone starts a new stream? Every tab subscribes to an ",(0,i.jsx)(n.code,{children:"ai:notifications"})," channel. The backend publishes a notification there when a stream starts:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'await publish_to_centrifugo("ai:notifications", {\n    "type": "stream_started",\n    "id": stream_id,\n    "channel": channel,\n    "question": question,\n})\n'})}),"\n",(0,i.jsx)(n.p,{children:"When a tab receives this notification, it joins the new stream immediately \u2014 no polling, no page reload."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Subscribing with catch-up."})," Every tab \u2014 including the one that started the stream \u2014 subscribes to the stream's channel with ",(0,i.jsx)(n.code,{children:"since: {offset: 0, epoch: ''}"}),". This tells Centrifugo to deliver all existing channel history through the normal recovery flow, then continue with live publications \u2014 all through the same ",(0,i.jsx)(n.code,{children:"publication"})," handler, in order:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"const opts = { since: { offset: 0, epoch: '' } };\nsubscription = centrifuge.newSubscription(channel, opts);\n\nsubscription.on('publication', (ctx) => {\n    // History and live publications arrive through the same handler, in order.\n    processPublication(ctx);\n});\n"})}),"\n",(0,i.jsx)(n.p,{children:"No client-side buffering, no deduplication logic. Centrifugo's recovery mechanism handles the sequencing. This also eliminates the race condition between starting the stream and subscribing \u2014 even if the backend publishes tokens before the client subscribes, they arrive through history recovery."}),"\n",(0,i.jsx)(n.p,{children:"Building this from scratch means implementing ordered pub/sub with offset tracking, fan-out to multiple subscribers, and a protocol to distinguish history replay from live delivery. Centrifugo provides all of these as built-in primitives."}),"\n",(0,i.jsx)(n.h3,{id:"the-pattern",children:"The pattern"}),"\n",(0,i.jsx)(n.p,{children:"This is a realistic pattern for combining a database with real-time delivery:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"PostgreSQL"})," is the source of truth for completed conversations \u2014 query it to show past streams, build dashboards, or feed analytics."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Centrifugo"})," handles live token delivery and short-term history for mid-stream catch-up."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"The backend stays stateless"})," \u2014 it writes to PG and publishes to Centrifugo, it doesn't hold client connections or track who is listening."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"A new tab can appear at any point during a stream, catch up seamlessly, and continue with live tokens. This works because Centrifugo's core operation is fan-out: publish once, deliver to all subscribers on the channel. No extra work on the backend, no tracking of which tabs or devices are connected."}),"\n",(0,i.jsx)(n.p,{children:"Practical cases: a user starts a conversation on desktop, picks up the phone, the response keeps streaming. Or a customer support scenario where an AI suggests responses and multiple agents see the same stream. These are all just N subscribers on one channel. And since Centrifugo supports channel multiplexing, the AI stream, a notifications feed, and presence indicators can all share a single WebSocket connection \u2014 each additional subscription is virtually free."}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"Each of these features solves a real problem in AI streaming:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recovery"})," handles the inevitable network disruptions without application-level complexity"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Redis"})," decouples the real-time layer, enables horizontal scaling, and persists history"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Transport fallbacks"})," ensure the service works in restricted network environments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-tab sync with PostgreSQL"})," shows how Centrifugo pairs with a database \u2014 PG for durable state, Centrifugo for real-time delivery and catch-up"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"None of these are AI-specific features \u2014 they're general real-time infrastructure capabilities that happen to be exactly what AI token streaming needs. Implementing them from scratch is significant engineering work: retry logic, offset tracking, message buffering, transport negotiation, fan-out routing. Centrifugo provides them as a single infrastructure layer."})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);